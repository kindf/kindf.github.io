[{"categories":["游戏服务器"],"content":"\r示意图\r各进程及作用 gate 进程： 负责和客户端直连和消息的分发 login 进程： 负责玩家的登录验证并为玩家分配 gameworld 进程 gameworld 进程： 游戏逻辑服务器 global 进程： 与多个 gameworld 进程连接，负责游戏的全局逻辑 fight 进程： 战斗进程，为 gameworld 提供战斗服务 database 进程： db 进程，与数据库连接 centor 进程： 与多个 global 连接，负责跨服逻辑 登录流程 客户端拉起 SDK 登录渠道并获取 token 客户端请求 http 服务器获取各个服的 gate 地址 客户端连接上 gate 并通过 login 进行验证 login 进程为验证完的用户分配 gameworld gameworld 通过 gate 下发玩家登录协议 ","date":"2024-09-17","objectID":"/posts/server/server_frame2/:0:0","series":null,"tags":null,"title":"滚服游戏服务器框架","uri":"/posts/server/server_frame2/#"},{"categories":["游戏服务器"],"content":"\r示意图\r各进程及作用 gate 进程： 负责和客户端直连和消息的分发 login 进程： 负责玩家的登录验证并为玩家分配 gameworld 进程 gameworld 进程： 游戏逻辑服务器 global 进程： 与多个 gameworld 进程连接，负责游戏的全局逻辑 fight 进程： 战斗进程，为 gameworld 提供战斗服务 database 进程： db 进程，与数据库连接 centor 进程： 与多个 global 连接，负责跨服逻辑 登录流程 客户端拉起 SDK 登录渠道并获取 token 客户端请求 http 服务器获取各个服的 gate 地址 客户端连接上 gate 并通过 login 进行验证 login 进程为验证完的用户分配 gameworld gameworld 通过 gate 下发玩家登录协议 ","date":"2024-09-17","objectID":"/posts/server/server_frame2/:0:0","series":null,"tags":null,"title":"滚服游戏服务器框架","uri":"/posts/server/server_frame2/#示意图"},{"categories":["游戏服务器"],"content":"\r示意图\r各进程及作用 gate 进程： 负责和客户端直连和消息的分发 login 进程： 负责玩家的登录验证并为玩家分配 gameworld 进程 gameworld 进程： 游戏逻辑服务器 global 进程： 与多个 gameworld 进程连接，负责游戏的全局逻辑 fight 进程： 战斗进程，为 gameworld 提供战斗服务 database 进程： db 进程，与数据库连接 centor 进程： 与多个 global 连接，负责跨服逻辑 登录流程 客户端拉起 SDK 登录渠道并获取 token 客户端请求 http 服务器获取各个服的 gate 地址 客户端连接上 gate 并通过 login 进行验证 login 进程为验证完的用户分配 gameworld gameworld 通过 gate 下发玩家登录协议 ","date":"2024-09-17","objectID":"/posts/server/server_frame2/:0:0","series":null,"tags":null,"title":"滚服游戏服务器框架","uri":"/posts/server/server_frame2/#各进程及作用"},{"categories":["游戏服务器"],"content":"\r示意图\r各进程及作用 gate 进程： 负责和客户端直连和消息的分发 login 进程： 负责玩家的登录验证并为玩家分配 gameworld 进程 gameworld 进程： 游戏逻辑服务器 global 进程： 与多个 gameworld 进程连接，负责游戏的全局逻辑 fight 进程： 战斗进程，为 gameworld 提供战斗服务 database 进程： db 进程，与数据库连接 centor 进程： 与多个 global 连接，负责跨服逻辑 登录流程 客户端拉起 SDK 登录渠道并获取 token 客户端请求 http 服务器获取各个服的 gate 地址 客户端连接上 gate 并通过 login 进行验证 login 进程为验证完的用户分配 gameworld gameworld 通过 gate 下发玩家登录协议 ","date":"2024-09-17","objectID":"/posts/server/server_frame2/:0:0","series":null,"tags":null,"title":"滚服游戏服务器框架","uri":"/posts/server/server_frame2/#登录流程"},{"categories":["skynet相关"],"content":"\r主要数据结构struct skynet_monitor { ATOM_INT version; int check_version; uint32_t source; uint32_t destination; }; skynet 的每个工作线程对应一个 skynet_monitor 结构体 version：当前的版本号 check_version：检查版本号 source：发送消息的 actor 地址 destination：接收消息的 actor 地址 monitor 线程void skynet_monitor_check(struct skynet_monitor *sm) { if (sm-\u003eversion == sm-\u003echeck_version) { if (sm-\u003edestination) { skynet_context_endless(sm-\u003edestination); skynet_error(NULL, \"A message from [ :%08x ] to [ :%08x ] maybe in an endless loop (version = %d)\", sm-\u003esource , sm-\u003edestination, sm-\u003eversion); } } else { sm-\u003echeck_version = sm-\u003eversion; } } monitor 线程每 5s 执行 skynet_monitor_check 函数，如果发现 destination ，则表示该 actor 可能陷入死循环中，这里只把该 actor 标记为死循环状态 状态设置struct message_queue *skynet_context_message_dispatch(struct skynet_monitor *sm, struct message_queue *q, int weight) { ... skynet_monitor_trigger(sm, msg.source , handle); if (ctx-\u003ecb == NULL) { skynet_free(msg.data); } else { dispatch_message(ctx, \u0026msg); } skynet_monitor_trigger(sm, 0,0); ... } skynet_monitor_trigger(struct skynet_monitor *sm, uint32_t source, uint32_t destination) { sm-\u003esource = source; sm-\u003edestination = destination; ATOM_FINC(\u0026sm-\u003eversion); } 在调用 dispatch_message 处理 actor 的消息的前会调用 skynet_monitor_trigger 把该线程对应 skynet_monitor 的 destination 字段设置为目标 actor 的地址 在调用 dispatch_message 处理 actor 的消息的后会调用 skynet_monitor_trigger 把该线程对应 skynet_monitor 的 destination 字段设置为无效 总结 每个线程都有一个 skynet_monitor 结构体用于记录监控信息 工作线程执行消息处理前调用 skynet_monitor_trigger 把 skynet_monitor 的 destination 字段设置为处理函数所在 actor 的地址 工作线程执行消息处理后调用 skynet_monitor_trigger 把 skynet_monitor 的 destination 字段设置为无效 monitor 线程每 5s 都会检查每个线程的 skynet_monitor 的 destination 字段是否有效，有效表示该 actor 对象的处理函数可能陷入死循环 monitor 线程检测到可能陷入死循环的 actor 只是把该 actor 标记为 endless 并打印错误 ","date":"2024-09-10","objectID":"/posts/skynet/skynet_moniter/:0:0","series":null,"tags":null,"title":"skynet 的 monitor 线程","uri":"/posts/skynet/skynet_moniter/#"},{"categories":["skynet相关"],"content":"\r主要数据结构struct skynet_monitor { ATOM_INT version; int check_version; uint32_t source; uint32_t destination; }; skynet 的每个工作线程对应一个 skynet_monitor 结构体 version：当前的版本号 check_version：检查版本号 source：发送消息的 actor 地址 destination：接收消息的 actor 地址 monitor 线程void skynet_monitor_check(struct skynet_monitor *sm) { if (sm-\u003eversion == sm-\u003echeck_version) { if (sm-\u003edestination) { skynet_context_endless(sm-\u003edestination); skynet_error(NULL, \"A message from [ :%08x ] to [ :%08x ] maybe in an endless loop (version = %d)\", sm-\u003esource , sm-\u003edestination, sm-\u003eversion); } } else { sm-\u003echeck_version = sm-\u003eversion; } } monitor 线程每 5s 执行 skynet_monitor_check 函数，如果发现 destination ，则表示该 actor 可能陷入死循环中，这里只把该 actor 标记为死循环状态 状态设置struct message_queue *skynet_context_message_dispatch(struct skynet_monitor *sm, struct message_queue *q, int weight) { ... skynet_monitor_trigger(sm, msg.source , handle); if (ctx-\u003ecb == NULL) { skynet_free(msg.data); } else { dispatch_message(ctx, \u0026msg); } skynet_monitor_trigger(sm, 0,0); ... } skynet_monitor_trigger(struct skynet_monitor *sm, uint32_t source, uint32_t destination) { sm-\u003esource = source; sm-\u003edestination = destination; ATOM_FINC(\u0026sm-\u003eversion); } 在调用 dispatch_message 处理 actor 的消息的前会调用 skynet_monitor_trigger 把该线程对应 skynet_monitor 的 destination 字段设置为目标 actor 的地址 在调用 dispatch_message 处理 actor 的消息的后会调用 skynet_monitor_trigger 把该线程对应 skynet_monitor 的 destination 字段设置为无效 总结 每个线程都有一个 skynet_monitor 结构体用于记录监控信息 工作线程执行消息处理前调用 skynet_monitor_trigger 把 skynet_monitor 的 destination 字段设置为处理函数所在 actor 的地址 工作线程执行消息处理后调用 skynet_monitor_trigger 把 skynet_monitor 的 destination 字段设置为无效 monitor 线程每 5s 都会检查每个线程的 skynet_monitor 的 destination 字段是否有效，有效表示该 actor 对象的处理函数可能陷入死循环 monitor 线程检测到可能陷入死循环的 actor 只是把该 actor 标记为 endless 并打印错误 ","date":"2024-09-10","objectID":"/posts/skynet/skynet_moniter/:0:0","series":null,"tags":null,"title":"skynet 的 monitor 线程","uri":"/posts/skynet/skynet_moniter/#主要数据结构"},{"categories":["skynet相关"],"content":"\r主要数据结构struct skynet_monitor { ATOM_INT version; int check_version; uint32_t source; uint32_t destination; }; skynet 的每个工作线程对应一个 skynet_monitor 结构体 version：当前的版本号 check_version：检查版本号 source：发送消息的 actor 地址 destination：接收消息的 actor 地址 monitor 线程void skynet_monitor_check(struct skynet_monitor *sm) { if (sm-\u003eversion == sm-\u003echeck_version) { if (sm-\u003edestination) { skynet_context_endless(sm-\u003edestination); skynet_error(NULL, \"A message from [ :%08x ] to [ :%08x ] maybe in an endless loop (version = %d)\", sm-\u003esource , sm-\u003edestination, sm-\u003eversion); } } else { sm-\u003echeck_version = sm-\u003eversion; } } monitor 线程每 5s 执行 skynet_monitor_check 函数，如果发现 destination ，则表示该 actor 可能陷入死循环中，这里只把该 actor 标记为死循环状态 状态设置struct message_queue *skynet_context_message_dispatch(struct skynet_monitor *sm, struct message_queue *q, int weight) { ... skynet_monitor_trigger(sm, msg.source , handle); if (ctx-\u003ecb == NULL) { skynet_free(msg.data); } else { dispatch_message(ctx, \u0026msg); } skynet_monitor_trigger(sm, 0,0); ... } skynet_monitor_trigger(struct skynet_monitor *sm, uint32_t source, uint32_t destination) { sm-\u003esource = source; sm-\u003edestination = destination; ATOM_FINC(\u0026sm-\u003eversion); } 在调用 dispatch_message 处理 actor 的消息的前会调用 skynet_monitor_trigger 把该线程对应 skynet_monitor 的 destination 字段设置为目标 actor 的地址 在调用 dispatch_message 处理 actor 的消息的后会调用 skynet_monitor_trigger 把该线程对应 skynet_monitor 的 destination 字段设置为无效 总结 每个线程都有一个 skynet_monitor 结构体用于记录监控信息 工作线程执行消息处理前调用 skynet_monitor_trigger 把 skynet_monitor 的 destination 字段设置为处理函数所在 actor 的地址 工作线程执行消息处理后调用 skynet_monitor_trigger 把 skynet_monitor 的 destination 字段设置为无效 monitor 线程每 5s 都会检查每个线程的 skynet_monitor 的 destination 字段是否有效，有效表示该 actor 对象的处理函数可能陷入死循环 monitor 线程检测到可能陷入死循环的 actor 只是把该 actor 标记为 endless 并打印错误 ","date":"2024-09-10","objectID":"/posts/skynet/skynet_moniter/:0:0","series":null,"tags":null,"title":"skynet 的 monitor 线程","uri":"/posts/skynet/skynet_moniter/#monitor-线程"},{"categories":["skynet相关"],"content":"\r主要数据结构struct skynet_monitor { ATOM_INT version; int check_version; uint32_t source; uint32_t destination; }; skynet 的每个工作线程对应一个 skynet_monitor 结构体 version：当前的版本号 check_version：检查版本号 source：发送消息的 actor 地址 destination：接收消息的 actor 地址 monitor 线程void skynet_monitor_check(struct skynet_monitor *sm) { if (sm-\u003eversion == sm-\u003echeck_version) { if (sm-\u003edestination) { skynet_context_endless(sm-\u003edestination); skynet_error(NULL, \"A message from [ :%08x ] to [ :%08x ] maybe in an endless loop (version = %d)\", sm-\u003esource , sm-\u003edestination, sm-\u003eversion); } } else { sm-\u003echeck_version = sm-\u003eversion; } } monitor 线程每 5s 执行 skynet_monitor_check 函数，如果发现 destination ，则表示该 actor 可能陷入死循环中，这里只把该 actor 标记为死循环状态 状态设置struct message_queue *skynet_context_message_dispatch(struct skynet_monitor *sm, struct message_queue *q, int weight) { ... skynet_monitor_trigger(sm, msg.source , handle); if (ctx-\u003ecb == NULL) { skynet_free(msg.data); } else { dispatch_message(ctx, \u0026msg); } skynet_monitor_trigger(sm, 0,0); ... } skynet_monitor_trigger(struct skynet_monitor *sm, uint32_t source, uint32_t destination) { sm-\u003esource = source; sm-\u003edestination = destination; ATOM_FINC(\u0026sm-\u003eversion); } 在调用 dispatch_message 处理 actor 的消息的前会调用 skynet_monitor_trigger 把该线程对应 skynet_monitor 的 destination 字段设置为目标 actor 的地址 在调用 dispatch_message 处理 actor 的消息的后会调用 skynet_monitor_trigger 把该线程对应 skynet_monitor 的 destination 字段设置为无效 总结 每个线程都有一个 skynet_monitor 结构体用于记录监控信息 工作线程执行消息处理前调用 skynet_monitor_trigger 把 skynet_monitor 的 destination 字段设置为处理函数所在 actor 的地址 工作线程执行消息处理后调用 skynet_monitor_trigger 把 skynet_monitor 的 destination 字段设置为无效 monitor 线程每 5s 都会检查每个线程的 skynet_monitor 的 destination 字段是否有效，有效表示该 actor 对象的处理函数可能陷入死循环 monitor 线程检测到可能陷入死循环的 actor 只是把该 actor 标记为 endless 并打印错误 ","date":"2024-09-10","objectID":"/posts/skynet/skynet_moniter/:0:0","series":null,"tags":null,"title":"skynet 的 monitor 线程","uri":"/posts/skynet/skynet_moniter/#状态设置"},{"categories":["skynet相关"],"content":"\r主要数据结构struct skynet_monitor { ATOM_INT version; int check_version; uint32_t source; uint32_t destination; }; skynet 的每个工作线程对应一个 skynet_monitor 结构体 version：当前的版本号 check_version：检查版本号 source：发送消息的 actor 地址 destination：接收消息的 actor 地址 monitor 线程void skynet_monitor_check(struct skynet_monitor *sm) { if (sm-\u003eversion == sm-\u003echeck_version) { if (sm-\u003edestination) { skynet_context_endless(sm-\u003edestination); skynet_error(NULL, \"A message from [ :%08x ] to [ :%08x ] maybe in an endless loop (version = %d)\", sm-\u003esource , sm-\u003edestination, sm-\u003eversion); } } else { sm-\u003echeck_version = sm-\u003eversion; } } monitor 线程每 5s 执行 skynet_monitor_check 函数，如果发现 destination ，则表示该 actor 可能陷入死循环中，这里只把该 actor 标记为死循环状态 状态设置struct message_queue *skynet_context_message_dispatch(struct skynet_monitor *sm, struct message_queue *q, int weight) { ... skynet_monitor_trigger(sm, msg.source , handle); if (ctx-\u003ecb == NULL) { skynet_free(msg.data); } else { dispatch_message(ctx, \u0026msg); } skynet_monitor_trigger(sm, 0,0); ... } skynet_monitor_trigger(struct skynet_monitor *sm, uint32_t source, uint32_t destination) { sm-\u003esource = source; sm-\u003edestination = destination; ATOM_FINC(\u0026sm-\u003eversion); } 在调用 dispatch_message 处理 actor 的消息的前会调用 skynet_monitor_trigger 把该线程对应 skynet_monitor 的 destination 字段设置为目标 actor 的地址 在调用 dispatch_message 处理 actor 的消息的后会调用 skynet_monitor_trigger 把该线程对应 skynet_monitor 的 destination 字段设置为无效 总结 每个线程都有一个 skynet_monitor 结构体用于记录监控信息 工作线程执行消息处理前调用 skynet_monitor_trigger 把 skynet_monitor 的 destination 字段设置为处理函数所在 actor 的地址 工作线程执行消息处理后调用 skynet_monitor_trigger 把 skynet_monitor 的 destination 字段设置为无效 monitor 线程每 5s 都会检查每个线程的 skynet_monitor 的 destination 字段是否有效，有效表示该 actor 对象的处理函数可能陷入死循环 monitor 线程检测到可能陷入死循环的 actor 只是把该 actor 标记为 endless 并打印错误 ","date":"2024-09-10","objectID":"/posts/skynet/skynet_moniter/:0:0","series":null,"tags":null,"title":"skynet 的 monitor 线程","uri":"/posts/skynet/skynet_moniter/#总结"},{"categories":["游戏服务器"],"content":"\r功能需求 单人邮件：对特定玩家发送邮件（如后台补偿邮件，背包空间不足道具由邮件发送等） 多人邮件：对一批玩家发送邮件（如排行榜奖励邮件，公会奖励邮件等） 全服邮件：对所有玩家发送邮件（如开服福利邮件，全服补偿邮件等） 基本设计思路 简单版本： 数据储存：用一个全局数据结构 g_mail_list 存放所有邮件，每发送一封邮件（包括个人，多人和全服邮件）就往 g_mail_list 添加 id 自增的邮件，玩家个人信息储存已接收到的邮件 发邮件： 往 g_mail_list 插入邮件信息，遍历该邮件的接收玩家列表，如果玩家在线，把邮件信息添加至玩家的邮件字段中并通知客户端，否则不处理 玩家上线时会拉取 g_mail_list，根据本身的邮件列表和全局邮件列表拉取新添加的邮件并通知客户端 缺陷： 发送全服邮件时会向所有在线玩家发送邮件，频繁发送多人/全服邮件可能会导致服务器短时间压力过大 由于单人邮件也存放在 g_mail_list 中，导致 g_mail_list 中的邮件数量过多，玩家登录时遍历该列表耗时会增加 改进方案： 对于缺陷 1：要发送到玩家的邮件存放在队列中，定时批量处理邮件（削峰平谷） 对于缺陷 2：可以把单人邮件从 g_mail_list 中剥离出来存放在对应玩家的 cache 中，玩家上线/定时检查缓存中的个人邮件信息 基于 skynet 实现的邮件系统\r单人邮件： 所有单人邮件都会发送到 cache 服务中 cache 服务收到邮件信息会记录到对应的玩家 cache 中，如果玩家在线，则通知到客户端 玩家登录会请求 cache 服务拉取所有缓存邮件 多人/全服邮件： 邮件会先发送到 group_mail 服务记录起来 group_mail 定时批量将邮件发送到在线玩家的 cache 中并通知客户端 玩家上线会从 cache 请求所有邮件列表 ","date":"2024-09-08","objectID":"/posts/server/mail/:0:0","series":null,"tags":null,"title":"游戏邮件系统","uri":"/posts/server/mail/#"},{"categories":["游戏服务器"],"content":"\r功能需求 单人邮件：对特定玩家发送邮件（如后台补偿邮件，背包空间不足道具由邮件发送等） 多人邮件：对一批玩家发送邮件（如排行榜奖励邮件，公会奖励邮件等） 全服邮件：对所有玩家发送邮件（如开服福利邮件，全服补偿邮件等） 基本设计思路 简单版本： 数据储存：用一个全局数据结构 g_mail_list 存放所有邮件，每发送一封邮件（包括个人，多人和全服邮件）就往 g_mail_list 添加 id 自增的邮件，玩家个人信息储存已接收到的邮件 发邮件： 往 g_mail_list 插入邮件信息，遍历该邮件的接收玩家列表，如果玩家在线，把邮件信息添加至玩家的邮件字段中并通知客户端，否则不处理 玩家上线时会拉取 g_mail_list，根据本身的邮件列表和全局邮件列表拉取新添加的邮件并通知客户端 缺陷： 发送全服邮件时会向所有在线玩家发送邮件，频繁发送多人/全服邮件可能会导致服务器短时间压力过大 由于单人邮件也存放在 g_mail_list 中，导致 g_mail_list 中的邮件数量过多，玩家登录时遍历该列表耗时会增加 改进方案： 对于缺陷 1：要发送到玩家的邮件存放在队列中，定时批量处理邮件（削峰平谷） 对于缺陷 2：可以把单人邮件从 g_mail_list 中剥离出来存放在对应玩家的 cache 中，玩家上线/定时检查缓存中的个人邮件信息 基于 skynet 实现的邮件系统\r单人邮件： 所有单人邮件都会发送到 cache 服务中 cache 服务收到邮件信息会记录到对应的玩家 cache 中，如果玩家在线，则通知到客户端 玩家登录会请求 cache 服务拉取所有缓存邮件 多人/全服邮件： 邮件会先发送到 group_mail 服务记录起来 group_mail 定时批量将邮件发送到在线玩家的 cache 中并通知客户端 玩家上线会从 cache 请求所有邮件列表 ","date":"2024-09-08","objectID":"/posts/server/mail/:0:0","series":null,"tags":null,"title":"游戏邮件系统","uri":"/posts/server/mail/#功能需求"},{"categories":["游戏服务器"],"content":"\r功能需求 单人邮件：对特定玩家发送邮件（如后台补偿邮件，背包空间不足道具由邮件发送等） 多人邮件：对一批玩家发送邮件（如排行榜奖励邮件，公会奖励邮件等） 全服邮件：对所有玩家发送邮件（如开服福利邮件，全服补偿邮件等） 基本设计思路 简单版本： 数据储存：用一个全局数据结构 g_mail_list 存放所有邮件，每发送一封邮件（包括个人，多人和全服邮件）就往 g_mail_list 添加 id 自增的邮件，玩家个人信息储存已接收到的邮件 发邮件： 往 g_mail_list 插入邮件信息，遍历该邮件的接收玩家列表，如果玩家在线，把邮件信息添加至玩家的邮件字段中并通知客户端，否则不处理 玩家上线时会拉取 g_mail_list，根据本身的邮件列表和全局邮件列表拉取新添加的邮件并通知客户端 缺陷： 发送全服邮件时会向所有在线玩家发送邮件，频繁发送多人/全服邮件可能会导致服务器短时间压力过大 由于单人邮件也存放在 g_mail_list 中，导致 g_mail_list 中的邮件数量过多，玩家登录时遍历该列表耗时会增加 改进方案： 对于缺陷 1：要发送到玩家的邮件存放在队列中，定时批量处理邮件（削峰平谷） 对于缺陷 2：可以把单人邮件从 g_mail_list 中剥离出来存放在对应玩家的 cache 中，玩家上线/定时检查缓存中的个人邮件信息 基于 skynet 实现的邮件系统\r单人邮件： 所有单人邮件都会发送到 cache 服务中 cache 服务收到邮件信息会记录到对应的玩家 cache 中，如果玩家在线，则通知到客户端 玩家登录会请求 cache 服务拉取所有缓存邮件 多人/全服邮件： 邮件会先发送到 group_mail 服务记录起来 group_mail 定时批量将邮件发送到在线玩家的 cache 中并通知客户端 玩家上线会从 cache 请求所有邮件列表 ","date":"2024-09-08","objectID":"/posts/server/mail/:0:0","series":null,"tags":null,"title":"游戏邮件系统","uri":"/posts/server/mail/#基本设计思路"},{"categories":["游戏服务器"],"content":"\r功能需求 单人邮件：对特定玩家发送邮件（如后台补偿邮件，背包空间不足道具由邮件发送等） 多人邮件：对一批玩家发送邮件（如排行榜奖励邮件，公会奖励邮件等） 全服邮件：对所有玩家发送邮件（如开服福利邮件，全服补偿邮件等） 基本设计思路 简单版本： 数据储存：用一个全局数据结构 g_mail_list 存放所有邮件，每发送一封邮件（包括个人，多人和全服邮件）就往 g_mail_list 添加 id 自增的邮件，玩家个人信息储存已接收到的邮件 发邮件： 往 g_mail_list 插入邮件信息，遍历该邮件的接收玩家列表，如果玩家在线，把邮件信息添加至玩家的邮件字段中并通知客户端，否则不处理 玩家上线时会拉取 g_mail_list，根据本身的邮件列表和全局邮件列表拉取新添加的邮件并通知客户端 缺陷： 发送全服邮件时会向所有在线玩家发送邮件，频繁发送多人/全服邮件可能会导致服务器短时间压力过大 由于单人邮件也存放在 g_mail_list 中，导致 g_mail_list 中的邮件数量过多，玩家登录时遍历该列表耗时会增加 改进方案： 对于缺陷 1：要发送到玩家的邮件存放在队列中，定时批量处理邮件（削峰平谷） 对于缺陷 2：可以把单人邮件从 g_mail_list 中剥离出来存放在对应玩家的 cache 中，玩家上线/定时检查缓存中的个人邮件信息 基于 skynet 实现的邮件系统\r单人邮件： 所有单人邮件都会发送到 cache 服务中 cache 服务收到邮件信息会记录到对应的玩家 cache 中，如果玩家在线，则通知到客户端 玩家登录会请求 cache 服务拉取所有缓存邮件 多人/全服邮件： 邮件会先发送到 group_mail 服务记录起来 group_mail 定时批量将邮件发送到在线玩家的 cache 中并通知客户端 玩家上线会从 cache 请求所有邮件列表 ","date":"2024-09-08","objectID":"/posts/server/mail/:0:0","series":null,"tags":null,"title":"游戏邮件系统","uri":"/posts/server/mail/#基于-skynet-实现的邮件系统"},{"categories":["游戏服务器"],"content":"\r游戏聊天实现 聊天功能基本需求 支持玩家与玩家私聊，保存私聊数据 支持多个频道聊天（如队伍频道，公会频道，世界频道，系统频道） 支持敏感词过滤 游戏聊天 游戏聊天模块通常是一个单独的进程（也有做法是把聊天模块放到游戏逻辑进程中） 功能相对独立，移植性强 减小逻辑服务器的压力 玩家登录 chat 服务器 玩家登录 gameworld 进程时，会把登录消息通知到 chat 进程 chat 进程生成玩家登录 token 通过 gameworld 进程（此时客户端和 chat 没有建立起连接）返回给客户端 玩家客户端使用 token 连接到 chat 进程完成登录 chat 服务器逻辑设计 channel：频道对象，其中主要成员是关注该频道的玩家 id 列表和该频道的历史消息 chat 服务器中有多个 channel，如 世界频道，系统频道，帮派频道（不同帮派对应不同 channel）等 服务器启动时会创建固定频道（如世界频道和系统频道），帮派创建时会创建新的帮派频道（惰性创建） 玩家登录会把玩家加入到其关注频道的玩家列表中，玩家下线会从频道的玩家列表中移除 玩家发送信息会根据类型投递到对应的 channel 中，channel 会广播给关注该频道的用户并缓冲历史信息 玩家聊天 玩家私聊的信息一般会保存起来，而且能给离线玩家发送信息，这个在玩家逻辑进程中实现更加方便 逻辑服务器中会维护一份玩家的缓存数据，这份数据会一直存在于服务器内存中，定时存盘 玩家的私聊信息存放在玩家的缓存数据中，这样即使时玩家离线也能收到其他玩家的信息 敏感词过滤 敏感词过滤算法（trie 树） ","date":"2024-09-06","objectID":"/posts/server/chat_server/:0:1","series":null,"tags":null,"title":"游戏聊天","uri":"/posts/server/chat_server/#游戏聊天实现"},{"categories":["游戏服务器"],"content":"\r游戏聊天实现 聊天功能基本需求 支持玩家与玩家私聊，保存私聊数据 支持多个频道聊天（如队伍频道，公会频道，世界频道，系统频道） 支持敏感词过滤 游戏聊天 游戏聊天模块通常是一个单独的进程（也有做法是把聊天模块放到游戏逻辑进程中） 功能相对独立，移植性强 减小逻辑服务器的压力 玩家登录 chat 服务器 玩家登录 gameworld 进程时，会把登录消息通知到 chat 进程 chat 进程生成玩家登录 token 通过 gameworld 进程（此时客户端和 chat 没有建立起连接）返回给客户端 玩家客户端使用 token 连接到 chat 进程完成登录 chat 服务器逻辑设计 channel：频道对象，其中主要成员是关注该频道的玩家 id 列表和该频道的历史消息 chat 服务器中有多个 channel，如 世界频道，系统频道，帮派频道（不同帮派对应不同 channel）等 服务器启动时会创建固定频道（如世界频道和系统频道），帮派创建时会创建新的帮派频道（惰性创建） 玩家登录会把玩家加入到其关注频道的玩家列表中，玩家下线会从频道的玩家列表中移除 玩家发送信息会根据类型投递到对应的 channel 中，channel 会广播给关注该频道的用户并缓冲历史信息 玩家聊天 玩家私聊的信息一般会保存起来，而且能给离线玩家发送信息，这个在玩家逻辑进程中实现更加方便 逻辑服务器中会维护一份玩家的缓存数据，这份数据会一直存在于服务器内存中，定时存盘 玩家的私聊信息存放在玩家的缓存数据中，这样即使时玩家离线也能收到其他玩家的信息 敏感词过滤 敏感词过滤算法（trie 树） ","date":"2024-09-06","objectID":"/posts/server/chat_server/:0:1","series":null,"tags":null,"title":"游戏聊天","uri":"/posts/server/chat_server/#聊天功能基本需求"},{"categories":["游戏服务器"],"content":"\r游戏聊天实现 聊天功能基本需求 支持玩家与玩家私聊，保存私聊数据 支持多个频道聊天（如队伍频道，公会频道，世界频道，系统频道） 支持敏感词过滤 游戏聊天 游戏聊天模块通常是一个单独的进程（也有做法是把聊天模块放到游戏逻辑进程中） 功能相对独立，移植性强 减小逻辑服务器的压力 玩家登录 chat 服务器 玩家登录 gameworld 进程时，会把登录消息通知到 chat 进程 chat 进程生成玩家登录 token 通过 gameworld 进程（此时客户端和 chat 没有建立起连接）返回给客户端 玩家客户端使用 token 连接到 chat 进程完成登录 chat 服务器逻辑设计 channel：频道对象，其中主要成员是关注该频道的玩家 id 列表和该频道的历史消息 chat 服务器中有多个 channel，如 世界频道，系统频道，帮派频道（不同帮派对应不同 channel）等 服务器启动时会创建固定频道（如世界频道和系统频道），帮派创建时会创建新的帮派频道（惰性创建） 玩家登录会把玩家加入到其关注频道的玩家列表中，玩家下线会从频道的玩家列表中移除 玩家发送信息会根据类型投递到对应的 channel 中，channel 会广播给关注该频道的用户并缓冲历史信息 玩家聊天 玩家私聊的信息一般会保存起来，而且能给离线玩家发送信息，这个在玩家逻辑进程中实现更加方便 逻辑服务器中会维护一份玩家的缓存数据，这份数据会一直存在于服务器内存中，定时存盘 玩家的私聊信息存放在玩家的缓存数据中，这样即使时玩家离线也能收到其他玩家的信息 敏感词过滤 敏感词过滤算法（trie 树） ","date":"2024-09-06","objectID":"/posts/server/chat_server/:0:1","series":null,"tags":null,"title":"游戏聊天","uri":"/posts/server/chat_server/#游戏聊天"},{"categories":["游戏服务器"],"content":"\r游戏聊天实现 聊天功能基本需求 支持玩家与玩家私聊，保存私聊数据 支持多个频道聊天（如队伍频道，公会频道，世界频道，系统频道） 支持敏感词过滤 游戏聊天 游戏聊天模块通常是一个单独的进程（也有做法是把聊天模块放到游戏逻辑进程中） 功能相对独立，移植性强 减小逻辑服务器的压力 玩家登录 chat 服务器 玩家登录 gameworld 进程时，会把登录消息通知到 chat 进程 chat 进程生成玩家登录 token 通过 gameworld 进程（此时客户端和 chat 没有建立起连接）返回给客户端 玩家客户端使用 token 连接到 chat 进程完成登录 chat 服务器逻辑设计 channel：频道对象，其中主要成员是关注该频道的玩家 id 列表和该频道的历史消息 chat 服务器中有多个 channel，如 世界频道，系统频道，帮派频道（不同帮派对应不同 channel）等 服务器启动时会创建固定频道（如世界频道和系统频道），帮派创建时会创建新的帮派频道（惰性创建） 玩家登录会把玩家加入到其关注频道的玩家列表中，玩家下线会从频道的玩家列表中移除 玩家发送信息会根据类型投递到对应的 channel 中，channel 会广播给关注该频道的用户并缓冲历史信息 玩家聊天 玩家私聊的信息一般会保存起来，而且能给离线玩家发送信息，这个在玩家逻辑进程中实现更加方便 逻辑服务器中会维护一份玩家的缓存数据，这份数据会一直存在于服务器内存中，定时存盘 玩家的私聊信息存放在玩家的缓存数据中，这样即使时玩家离线也能收到其他玩家的信息 敏感词过滤 敏感词过滤算法（trie 树） ","date":"2024-09-06","objectID":"/posts/server/chat_server/:0:1","series":null,"tags":null,"title":"游戏聊天","uri":"/posts/server/chat_server/#玩家登录-chat-服务器"},{"categories":["游戏服务器"],"content":"\r游戏聊天实现 聊天功能基本需求 支持玩家与玩家私聊，保存私聊数据 支持多个频道聊天（如队伍频道，公会频道，世界频道，系统频道） 支持敏感词过滤 游戏聊天 游戏聊天模块通常是一个单独的进程（也有做法是把聊天模块放到游戏逻辑进程中） 功能相对独立，移植性强 减小逻辑服务器的压力 玩家登录 chat 服务器 玩家登录 gameworld 进程时，会把登录消息通知到 chat 进程 chat 进程生成玩家登录 token 通过 gameworld 进程（此时客户端和 chat 没有建立起连接）返回给客户端 玩家客户端使用 token 连接到 chat 进程完成登录 chat 服务器逻辑设计 channel：频道对象，其中主要成员是关注该频道的玩家 id 列表和该频道的历史消息 chat 服务器中有多个 channel，如 世界频道，系统频道，帮派频道（不同帮派对应不同 channel）等 服务器启动时会创建固定频道（如世界频道和系统频道），帮派创建时会创建新的帮派频道（惰性创建） 玩家登录会把玩家加入到其关注频道的玩家列表中，玩家下线会从频道的玩家列表中移除 玩家发送信息会根据类型投递到对应的 channel 中，channel 会广播给关注该频道的用户并缓冲历史信息 玩家聊天 玩家私聊的信息一般会保存起来，而且能给离线玩家发送信息，这个在玩家逻辑进程中实现更加方便 逻辑服务器中会维护一份玩家的缓存数据，这份数据会一直存在于服务器内存中，定时存盘 玩家的私聊信息存放在玩家的缓存数据中，这样即使时玩家离线也能收到其他玩家的信息 敏感词过滤 敏感词过滤算法（trie 树） ","date":"2024-09-06","objectID":"/posts/server/chat_server/:0:1","series":null,"tags":null,"title":"游戏聊天","uri":"/posts/server/chat_server/#chat-服务器逻辑设计"},{"categories":["游戏服务器"],"content":"\r游戏聊天实现 聊天功能基本需求 支持玩家与玩家私聊，保存私聊数据 支持多个频道聊天（如队伍频道，公会频道，世界频道，系统频道） 支持敏感词过滤 游戏聊天 游戏聊天模块通常是一个单独的进程（也有做法是把聊天模块放到游戏逻辑进程中） 功能相对独立，移植性强 减小逻辑服务器的压力 玩家登录 chat 服务器 玩家登录 gameworld 进程时，会把登录消息通知到 chat 进程 chat 进程生成玩家登录 token 通过 gameworld 进程（此时客户端和 chat 没有建立起连接）返回给客户端 玩家客户端使用 token 连接到 chat 进程完成登录 chat 服务器逻辑设计 channel：频道对象，其中主要成员是关注该频道的玩家 id 列表和该频道的历史消息 chat 服务器中有多个 channel，如 世界频道，系统频道，帮派频道（不同帮派对应不同 channel）等 服务器启动时会创建固定频道（如世界频道和系统频道），帮派创建时会创建新的帮派频道（惰性创建） 玩家登录会把玩家加入到其关注频道的玩家列表中，玩家下线会从频道的玩家列表中移除 玩家发送信息会根据类型投递到对应的 channel 中，channel 会广播给关注该频道的用户并缓冲历史信息 玩家聊天 玩家私聊的信息一般会保存起来，而且能给离线玩家发送信息，这个在玩家逻辑进程中实现更加方便 逻辑服务器中会维护一份玩家的缓存数据，这份数据会一直存在于服务器内存中，定时存盘 玩家的私聊信息存放在玩家的缓存数据中，这样即使时玩家离线也能收到其他玩家的信息 敏感词过滤 敏感词过滤算法（trie 树） ","date":"2024-09-06","objectID":"/posts/server/chat_server/:0:1","series":null,"tags":null,"title":"游戏聊天","uri":"/posts/server/chat_server/#玩家聊天"},{"categories":["游戏服务器"],"content":"\r游戏聊天实现 聊天功能基本需求 支持玩家与玩家私聊，保存私聊数据 支持多个频道聊天（如队伍频道，公会频道，世界频道，系统频道） 支持敏感词过滤 游戏聊天 游戏聊天模块通常是一个单独的进程（也有做法是把聊天模块放到游戏逻辑进程中） 功能相对独立，移植性强 减小逻辑服务器的压力 玩家登录 chat 服务器 玩家登录 gameworld 进程时，会把登录消息通知到 chat 进程 chat 进程生成玩家登录 token 通过 gameworld 进程（此时客户端和 chat 没有建立起连接）返回给客户端 玩家客户端使用 token 连接到 chat 进程完成登录 chat 服务器逻辑设计 channel：频道对象，其中主要成员是关注该频道的玩家 id 列表和该频道的历史消息 chat 服务器中有多个 channel，如 世界频道，系统频道，帮派频道（不同帮派对应不同 channel）等 服务器启动时会创建固定频道（如世界频道和系统频道），帮派创建时会创建新的帮派频道（惰性创建） 玩家登录会把玩家加入到其关注频道的玩家列表中，玩家下线会从频道的玩家列表中移除 玩家发送信息会根据类型投递到对应的 channel 中，channel 会广播给关注该频道的用户并缓冲历史信息 玩家聊天 玩家私聊的信息一般会保存起来，而且能给离线玩家发送信息，这个在玩家逻辑进程中实现更加方便 逻辑服务器中会维护一份玩家的缓存数据，这份数据会一直存在于服务器内存中，定时存盘 玩家的私聊信息存放在玩家的缓存数据中，这样即使时玩家离线也能收到其他玩家的信息 敏感词过滤 敏感词过滤算法（trie 树） ","date":"2024-09-06","objectID":"/posts/server/chat_server/:0:1","series":null,"tags":null,"title":"游戏聊天","uri":"/posts/server/chat_server/#敏感词过滤"},{"categories":["网络编程"],"content":"\r三次握手\r各个 socket 函数 connect 函数： 调用 connect 将发起第一次握手，阻塞套接字 connect 会在第二次握完成时返回成功 connect 失败的套接字不可再用，必须关闭 常见返回错误： ETIMEOUT：超时没有收到对端的 SYN 分节响应 ECONNERFUSED：对端没有监听该端口（收到对端发送的 RST 分节） bind 函数： 没有 bind 的套接字调用 connect 或 listen 时，内核会为套接字选择一个临时端口 常见返回错误： EADDRINUSE：绑定正在使用的端口（如处于 TIME_WAIT 状态下的端口或还有连接的端口，通常使用设置套接字 SO_RESUEADDR 选项来解决） listen 函数： listen 函数的作用是将一个未建立连接的套接字从主动套接字转换为被动套接字 内核会为监听套接字维护（1）未完成连接队列和（2）已完成连接队列，未完成队列中连接的存留时间为一个 RTT backlog 参数：已连接队列的最大容量 连接队列已满的情况下接受到新的 SYN 分节，TCP 的处理是不予理会 常见错误： EBADF：参数 fd 非法 accept 函数： accept 用于从已完成连接队列返回下一个已完成连接（不论是处理 ESTABLISHED 状态还是 CLOSE_WAIT 状态），若队列为空则被阻塞（对于阻塞套接字） 常见返回错误： EINVAL：表示第一个参数不是监听套接字 EMFILE：当前进程打开文件描述符已达上限 ","date":"2024-09-04","objectID":"/posts/network/socket_connect/:0:1","series":null,"tags":null,"title":"socket 连接","uri":"/posts/network/socket_connect/#三次握手"},{"categories":["网络编程"],"content":"\r三次握手\r各个 socket 函数 connect 函数： 调用 connect 将发起第一次握手，阻塞套接字 connect 会在第二次握完成时返回成功 connect 失败的套接字不可再用，必须关闭 常见返回错误： ETIMEOUT：超时没有收到对端的 SYN 分节响应 ECONNERFUSED：对端没有监听该端口（收到对端发送的 RST 分节） bind 函数： 没有 bind 的套接字调用 connect 或 listen 时，内核会为套接字选择一个临时端口 常见返回错误： EADDRINUSE：绑定正在使用的端口（如处于 TIME_WAIT 状态下的端口或还有连接的端口，通常使用设置套接字 SO_RESUEADDR 选项来解决） listen 函数： listen 函数的作用是将一个未建立连接的套接字从主动套接字转换为被动套接字 内核会为监听套接字维护（1）未完成连接队列和（2）已完成连接队列，未完成队列中连接的存留时间为一个 RTT backlog 参数：已连接队列的最大容量 连接队列已满的情况下接受到新的 SYN 分节，TCP 的处理是不予理会 常见错误： EBADF：参数 fd 非法 accept 函数： accept 用于从已完成连接队列返回下一个已完成连接（不论是处理 ESTABLISHED 状态还是 CLOSE_WAIT 状态），若队列为空则被阻塞（对于阻塞套接字） 常见返回错误： EINVAL：表示第一个参数不是监听套接字 EMFILE：当前进程打开文件描述符已达上限 ","date":"2024-09-04","objectID":"/posts/network/socket_connect/:0:1","series":null,"tags":null,"title":"socket 连接","uri":"/posts/network/socket_connect/#各个-socket-函数"},{"categories":["网络编程"],"content":"\r多进程网络编程中子进程在 fork 后应该先做什么处理再进行 fd 读写处理pid_t pid; int listenfd, connfd; listenfd = Socket(...); Bind(listenfd, ...); Listen(listenfd, LISTENQ); for(;;) { connfd = Accept(listenfd, ...); if( (pid = Fork()) == 0) { Close(listenfd); doit(connfd); Close(connfd); exit(0); } Close(connfd); } 以上为经典并发服务器轮廓（节选至UNIX 网络变成卷1） 对于子线程来说不需要关注父进程的 listenfd，所以 fork 后应该先 close listenfd 新连接由子进程提供服务，所以父进程再 fork 后可以 close connfd 调用 close 是否会关闭 socket 实际上调用 close 只是导致相应的描述符引用计数减一，套接字真正的清理和资源释放发生需要其引用计数到达 0 时才发生 父进程为什么需要 close connfd 若不关闭，fork 结束后，connfd 的引用计数为2，子进程结束后，connfd 引用计数由 2递减为 1，（a）这将导致 fd 一直存在，耗尽父进程的可用文件描述符而且（b）连接会一直存在占用资源 ","date":"2024-09-04","objectID":"/posts/network/multiprocess_accept/:0:0","series":null,"tags":null,"title":"多进程 accept","uri":"/posts/network/multiprocess_accept/#"},{"categories":["网络编程"],"content":"\r多进程网络编程中子进程在 fork 后应该先做什么处理再进行 fd 读写处理pid_t pid; int listenfd, connfd; listenfd = Socket(...); Bind(listenfd, ...); Listen(listenfd, LISTENQ); for(;;) { connfd = Accept(listenfd, ...); if( (pid = Fork()) == 0) { Close(listenfd); doit(connfd); Close(connfd); exit(0); } Close(connfd); } 以上为经典并发服务器轮廓（节选至UNIX 网络变成卷1） 对于子线程来说不需要关注父进程的 listenfd，所以 fork 后应该先 close listenfd 新连接由子进程提供服务，所以父进程再 fork 后可以 close connfd 调用 close 是否会关闭 socket 实际上调用 close 只是导致相应的描述符引用计数减一，套接字真正的清理和资源释放发生需要其引用计数到达 0 时才发生 父进程为什么需要 close connfd 若不关闭，fork 结束后，connfd 的引用计数为2，子进程结束后，connfd 引用计数由 2递减为 1，（a）这将导致 fd 一直存在，耗尽父进程的可用文件描述符而且（b）连接会一直存在占用资源 ","date":"2024-09-04","objectID":"/posts/network/multiprocess_accept/:0:0","series":null,"tags":null,"title":"多进程 accept","uri":"/posts/network/multiprocess_accept/#多进程网络编程中子进程在-fork-后应该先做什么处理再进行-fd-读写处理"},{"categories":["游戏服务器"],"content":"\r框架示意图\r各个进程及其作用 main 进程：游戏逻辑进程 chat 进程：聊天进程，和 main 进程一一对应 fight 进程：战斗进程，和 main 进程一一对应 record 进程：日志管理进程，和 main 进程一一对应 centor 进程：中心服进程 wms 进程：后台管理服务进程 main 进程 main 进程是游戏逻辑进程，main 进程，fight 进程，record 进程和 chat 进程组成一个游戏服 基本服务： watchdog 服务：main 进程的入口服务，所有服务的创建都是在 watchdog 服务中 gate 服务：网关服务，所有客户端连接发送的信息都会由 socket 线程发送到 gate 服务中，再由 gate 服务 转发到其他服务中 多个 agent 服务：处理玩家逻辑的服务，每个 agent 有一个 mongodb 长连接 cached 服务：玩家的缓存服务，缓存信息存放在该服务 gangd 服务：公会服务 rank 服务：排行榜服务 fight 进程 fight 进程主要为 main 进程提供战斗服务 chat 进程 chat 进程主要为客户端提供聊天服务 基本服务： chat_mng 服务：客户端连接认证服务 gate 服务：网关服务，管理客户端连接和消息分发 chat 服务：聊天服务，管理玩家聊天 centor 进程 中心服进程，主要执行跨服的玩法逻辑，每个 centor 进程对应一个游戏区（每个游戏区由多个游戏服组成） wms 进程 wms 进程连接多个 centor 进程 基本服务： wms 服务：进程入口服务，用于启动 wms_client 和 agent_client 服务，节点记录，错误上报等功能 wms_client 服务：向后台上报 http/https 的服务 多个 agent_client 服务：接受后台指令的服务，如gm 指令，充值 SDK 返回等 ","date":"2024-09-04","objectID":"/posts/server/server_frame/:0:1","series":null,"tags":null,"title":"基于 skynet 的卡牌游戏服务器框架","uri":"/posts/server/server_frame/#框架示意图"},{"categories":["游戏服务器"],"content":"\r框架示意图\r各个进程及其作用 main 进程：游戏逻辑进程 chat 进程：聊天进程，和 main 进程一一对应 fight 进程：战斗进程，和 main 进程一一对应 record 进程：日志管理进程，和 main 进程一一对应 centor 进程：中心服进程 wms 进程：后台管理服务进程 main 进程 main 进程是游戏逻辑进程，main 进程，fight 进程，record 进程和 chat 进程组成一个游戏服 基本服务： watchdog 服务：main 进程的入口服务，所有服务的创建都是在 watchdog 服务中 gate 服务：网关服务，所有客户端连接发送的信息都会由 socket 线程发送到 gate 服务中，再由 gate 服务 转发到其他服务中 多个 agent 服务：处理玩家逻辑的服务，每个 agent 有一个 mongodb 长连接 cached 服务：玩家的缓存服务，缓存信息存放在该服务 gangd 服务：公会服务 rank 服务：排行榜服务 fight 进程 fight 进程主要为 main 进程提供战斗服务 chat 进程 chat 进程主要为客户端提供聊天服务 基本服务： chat_mng 服务：客户端连接认证服务 gate 服务：网关服务，管理客户端连接和消息分发 chat 服务：聊天服务，管理玩家聊天 centor 进程 中心服进程，主要执行跨服的玩法逻辑，每个 centor 进程对应一个游戏区（每个游戏区由多个游戏服组成） wms 进程 wms 进程连接多个 centor 进程 基本服务： wms 服务：进程入口服务，用于启动 wms_client 和 agent_client 服务，节点记录，错误上报等功能 wms_client 服务：向后台上报 http/https 的服务 多个 agent_client 服务：接受后台指令的服务，如gm 指令，充值 SDK 返回等 ","date":"2024-09-04","objectID":"/posts/server/server_frame/:0:1","series":null,"tags":null,"title":"基于 skynet 的卡牌游戏服务器框架","uri":"/posts/server/server_frame/#各个进程及其作用"},{"categories":["游戏服务器"],"content":"\r框架示意图\r各个进程及其作用 main 进程：游戏逻辑进程 chat 进程：聊天进程，和 main 进程一一对应 fight 进程：战斗进程，和 main 进程一一对应 record 进程：日志管理进程，和 main 进程一一对应 centor 进程：中心服进程 wms 进程：后台管理服务进程 main 进程 main 进程是游戏逻辑进程，main 进程，fight 进程，record 进程和 chat 进程组成一个游戏服 基本服务： watchdog 服务：main 进程的入口服务，所有服务的创建都是在 watchdog 服务中 gate 服务：网关服务，所有客户端连接发送的信息都会由 socket 线程发送到 gate 服务中，再由 gate 服务 转发到其他服务中 多个 agent 服务：处理玩家逻辑的服务，每个 agent 有一个 mongodb 长连接 cached 服务：玩家的缓存服务，缓存信息存放在该服务 gangd 服务：公会服务 rank 服务：排行榜服务 fight 进程 fight 进程主要为 main 进程提供战斗服务 chat 进程 chat 进程主要为客户端提供聊天服务 基本服务： chat_mng 服务：客户端连接认证服务 gate 服务：网关服务，管理客户端连接和消息分发 chat 服务：聊天服务，管理玩家聊天 centor 进程 中心服进程，主要执行跨服的玩法逻辑，每个 centor 进程对应一个游戏区（每个游戏区由多个游戏服组成） wms 进程 wms 进程连接多个 centor 进程 基本服务： wms 服务：进程入口服务，用于启动 wms_client 和 agent_client 服务，节点记录，错误上报等功能 wms_client 服务：向后台上报 http/https 的服务 多个 agent_client 服务：接受后台指令的服务，如gm 指令，充值 SDK 返回等 ","date":"2024-09-04","objectID":"/posts/server/server_frame/:0:1","series":null,"tags":null,"title":"基于 skynet 的卡牌游戏服务器框架","uri":"/posts/server/server_frame/#main-进程"},{"categories":["游戏服务器"],"content":"\r框架示意图\r各个进程及其作用 main 进程：游戏逻辑进程 chat 进程：聊天进程，和 main 进程一一对应 fight 进程：战斗进程，和 main 进程一一对应 record 进程：日志管理进程，和 main 进程一一对应 centor 进程：中心服进程 wms 进程：后台管理服务进程 main 进程 main 进程是游戏逻辑进程，main 进程，fight 进程，record 进程和 chat 进程组成一个游戏服 基本服务： watchdog 服务：main 进程的入口服务，所有服务的创建都是在 watchdog 服务中 gate 服务：网关服务，所有客户端连接发送的信息都会由 socket 线程发送到 gate 服务中，再由 gate 服务 转发到其他服务中 多个 agent 服务：处理玩家逻辑的服务，每个 agent 有一个 mongodb 长连接 cached 服务：玩家的缓存服务，缓存信息存放在该服务 gangd 服务：公会服务 rank 服务：排行榜服务 fight 进程 fight 进程主要为 main 进程提供战斗服务 chat 进程 chat 进程主要为客户端提供聊天服务 基本服务： chat_mng 服务：客户端连接认证服务 gate 服务：网关服务，管理客户端连接和消息分发 chat 服务：聊天服务，管理玩家聊天 centor 进程 中心服进程，主要执行跨服的玩法逻辑，每个 centor 进程对应一个游戏区（每个游戏区由多个游戏服组成） wms 进程 wms 进程连接多个 centor 进程 基本服务： wms 服务：进程入口服务，用于启动 wms_client 和 agent_client 服务，节点记录，错误上报等功能 wms_client 服务：向后台上报 http/https 的服务 多个 agent_client 服务：接受后台指令的服务，如gm 指令，充值 SDK 返回等 ","date":"2024-09-04","objectID":"/posts/server/server_frame/:0:1","series":null,"tags":null,"title":"基于 skynet 的卡牌游戏服务器框架","uri":"/posts/server/server_frame/#fight-进程"},{"categories":["游戏服务器"],"content":"\r框架示意图\r各个进程及其作用 main 进程：游戏逻辑进程 chat 进程：聊天进程，和 main 进程一一对应 fight 进程：战斗进程，和 main 进程一一对应 record 进程：日志管理进程，和 main 进程一一对应 centor 进程：中心服进程 wms 进程：后台管理服务进程 main 进程 main 进程是游戏逻辑进程，main 进程，fight 进程，record 进程和 chat 进程组成一个游戏服 基本服务： watchdog 服务：main 进程的入口服务，所有服务的创建都是在 watchdog 服务中 gate 服务：网关服务，所有客户端连接发送的信息都会由 socket 线程发送到 gate 服务中，再由 gate 服务 转发到其他服务中 多个 agent 服务：处理玩家逻辑的服务，每个 agent 有一个 mongodb 长连接 cached 服务：玩家的缓存服务，缓存信息存放在该服务 gangd 服务：公会服务 rank 服务：排行榜服务 fight 进程 fight 进程主要为 main 进程提供战斗服务 chat 进程 chat 进程主要为客户端提供聊天服务 基本服务： chat_mng 服务：客户端连接认证服务 gate 服务：网关服务，管理客户端连接和消息分发 chat 服务：聊天服务，管理玩家聊天 centor 进程 中心服进程，主要执行跨服的玩法逻辑，每个 centor 进程对应一个游戏区（每个游戏区由多个游戏服组成） wms 进程 wms 进程连接多个 centor 进程 基本服务： wms 服务：进程入口服务，用于启动 wms_client 和 agent_client 服务，节点记录，错误上报等功能 wms_client 服务：向后台上报 http/https 的服务 多个 agent_client 服务：接受后台指令的服务，如gm 指令，充值 SDK 返回等 ","date":"2024-09-04","objectID":"/posts/server/server_frame/:0:1","series":null,"tags":null,"title":"基于 skynet 的卡牌游戏服务器框架","uri":"/posts/server/server_frame/#chat-进程"},{"categories":["游戏服务器"],"content":"\r框架示意图\r各个进程及其作用 main 进程：游戏逻辑进程 chat 进程：聊天进程，和 main 进程一一对应 fight 进程：战斗进程，和 main 进程一一对应 record 进程：日志管理进程，和 main 进程一一对应 centor 进程：中心服进程 wms 进程：后台管理服务进程 main 进程 main 进程是游戏逻辑进程，main 进程，fight 进程，record 进程和 chat 进程组成一个游戏服 基本服务： watchdog 服务：main 进程的入口服务，所有服务的创建都是在 watchdog 服务中 gate 服务：网关服务，所有客户端连接发送的信息都会由 socket 线程发送到 gate 服务中，再由 gate 服务 转发到其他服务中 多个 agent 服务：处理玩家逻辑的服务，每个 agent 有一个 mongodb 长连接 cached 服务：玩家的缓存服务，缓存信息存放在该服务 gangd 服务：公会服务 rank 服务：排行榜服务 fight 进程 fight 进程主要为 main 进程提供战斗服务 chat 进程 chat 进程主要为客户端提供聊天服务 基本服务： chat_mng 服务：客户端连接认证服务 gate 服务：网关服务，管理客户端连接和消息分发 chat 服务：聊天服务，管理玩家聊天 centor 进程 中心服进程，主要执行跨服的玩法逻辑，每个 centor 进程对应一个游戏区（每个游戏区由多个游戏服组成） wms 进程 wms 进程连接多个 centor 进程 基本服务： wms 服务：进程入口服务，用于启动 wms_client 和 agent_client 服务，节点记录，错误上报等功能 wms_client 服务：向后台上报 http/https 的服务 多个 agent_client 服务：接受后台指令的服务，如gm 指令，充值 SDK 返回等 ","date":"2024-09-04","objectID":"/posts/server/server_frame/:0:1","series":null,"tags":null,"title":"基于 skynet 的卡牌游戏服务器框架","uri":"/posts/server/server_frame/#centor-进程"},{"categories":["游戏服务器"],"content":"\r框架示意图\r各个进程及其作用 main 进程：游戏逻辑进程 chat 进程：聊天进程，和 main 进程一一对应 fight 进程：战斗进程，和 main 进程一一对应 record 进程：日志管理进程，和 main 进程一一对应 centor 进程：中心服进程 wms 进程：后台管理服务进程 main 进程 main 进程是游戏逻辑进程，main 进程，fight 进程，record 进程和 chat 进程组成一个游戏服 基本服务： watchdog 服务：main 进程的入口服务，所有服务的创建都是在 watchdog 服务中 gate 服务：网关服务，所有客户端连接发送的信息都会由 socket 线程发送到 gate 服务中，再由 gate 服务 转发到其他服务中 多个 agent 服务：处理玩家逻辑的服务，每个 agent 有一个 mongodb 长连接 cached 服务：玩家的缓存服务，缓存信息存放在该服务 gangd 服务：公会服务 rank 服务：排行榜服务 fight 进程 fight 进程主要为 main 进程提供战斗服务 chat 进程 chat 进程主要为客户端提供聊天服务 基本服务： chat_mng 服务：客户端连接认证服务 gate 服务：网关服务，管理客户端连接和消息分发 chat 服务：聊天服务，管理玩家聊天 centor 进程 中心服进程，主要执行跨服的玩法逻辑，每个 centor 进程对应一个游戏区（每个游戏区由多个游戏服组成） wms 进程 wms 进程连接多个 centor 进程 基本服务： wms 服务：进程入口服务，用于启动 wms_client 和 agent_client 服务，节点记录，错误上报等功能 wms_client 服务：向后台上报 http/https 的服务 多个 agent_client 服务：接受后台指令的服务，如gm 指令，充值 SDK 返回等 ","date":"2024-09-04","objectID":"/posts/server/server_frame/:0:1","series":null,"tags":null,"title":"基于 skynet 的卡牌游戏服务器框架","uri":"/posts/server/server_frame/#wms-进程"},{"categories":["网络编程"],"content":"\r四次挥手\r客户端主动发起连接，发送 FIN 包 服务端接收到 FIN 包传递给应用程序 服务端应用程序接收到 FIN 包后关闭连接（TCP 向对端发送 FIN） 客户端接收到 FIN 包向对端发送确认包 TIME_WAIT 状态存在的两个原因 防止第三次挥手的数据包丢失 确保每次成功建立的连接不是旧连接的化身 close() 和 shutdown() close() 函数： 调用 close 只是导致相应的描述符引用计数减一，套接字真正的清理和资源释放发生需要其引用计数到达 0 时才发生（计数为 0 时才会发送 FIN 包） SO_LINGER 套接字选项 默认情况下为关闭选项（即 l_onoff 为 0） 选项开启的情况（l_onoff = 1）： l_linger = 0 时，调用 close 函数 TCP 将丢弃套接字发送缓冲区的所有数据并向对端发送 RST 包，这种情况下避免了 TCP 的 TIME_WAIT 状态 l_linger \u003e0 ： 阻塞套接字调用 close：进程会睡眠直到所有数据被对端确认（收到 ack 包）或超时 非阻塞套接字调用 close：调用马上返回，返回值为 EWOULDBLOCK 表示数据被对端确认前超时，此时套接字发送缓冲区的残留数据都会被丢弃 shutdown() 函数： SHUT_RD：套接字接受缓冲区的数据及往后接收到的任何数据都会被丢弃，进程不能对该套接字进行读操作，对套接字发送缓冲区没任何影响（不会发送 FIN 包） SHUT_WR：套接字可以继续接受数据，发送缓冲区的数据和 FIN 包数据会发送到对端 如何关闭连接\r方式1：正常 write 后直接 close 默认情况下 close 会直接返回，TCP 将尝试发送已排队等待发送到对端的数据，发送完后再发送 FIN 包 该方式下客户端无法知道对端 TCP 是否接收该数据（正常情况下 TCP 的可靠性能保证对端接收到剩余数据，但对端主机崩溃的情况下会造成数据丢失） 方式2：设置套接字 SO_LINGER 选项（linger time \u003e 0） 当 close 超时时，情况和方式 1 一样 当 close 成功返回，说明数据已被对端 TCP 确认，但是不能确保数据被对端应用程读取（如对端程序在从缓冲区读出数据前崩溃） 方式3：write 后 shutdown 写端，调用 read 等到对端的 FIN 才返回 该方式下可以获知对端应用程序已读取我们的数据 ","date":"2024-09-03","objectID":"/posts/network/socket_close/:0:0","series":null,"tags":null,"title":"socket 关闭","uri":"/posts/network/socket_close/#"},{"categories":["网络编程"],"content":"\r四次挥手\r客户端主动发起连接，发送 FIN 包 服务端接收到 FIN 包传递给应用程序 服务端应用程序接收到 FIN 包后关闭连接（TCP 向对端发送 FIN） 客户端接收到 FIN 包向对端发送确认包 TIME_WAIT 状态存在的两个原因 防止第三次挥手的数据包丢失 确保每次成功建立的连接不是旧连接的化身 close() 和 shutdown() close() 函数： 调用 close 只是导致相应的描述符引用计数减一，套接字真正的清理和资源释放发生需要其引用计数到达 0 时才发生（计数为 0 时才会发送 FIN 包） SO_LINGER 套接字选项 默认情况下为关闭选项（即 l_onoff 为 0） 选项开启的情况（l_onoff = 1）： l_linger = 0 时，调用 close 函数 TCP 将丢弃套接字发送缓冲区的所有数据并向对端发送 RST 包，这种情况下避免了 TCP 的 TIME_WAIT 状态 l_linger \u003e0 ： 阻塞套接字调用 close：进程会睡眠直到所有数据被对端确认（收到 ack 包）或超时 非阻塞套接字调用 close：调用马上返回，返回值为 EWOULDBLOCK 表示数据被对端确认前超时，此时套接字发送缓冲区的残留数据都会被丢弃 shutdown() 函数： SHUT_RD：套接字接受缓冲区的数据及往后接收到的任何数据都会被丢弃，进程不能对该套接字进行读操作，对套接字发送缓冲区没任何影响（不会发送 FIN 包） SHUT_WR：套接字可以继续接受数据，发送缓冲区的数据和 FIN 包数据会发送到对端 如何关闭连接\r方式1：正常 write 后直接 close 默认情况下 close 会直接返回，TCP 将尝试发送已排队等待发送到对端的数据，发送完后再发送 FIN 包 该方式下客户端无法知道对端 TCP 是否接收该数据（正常情况下 TCP 的可靠性能保证对端接收到剩余数据，但对端主机崩溃的情况下会造成数据丢失） 方式2：设置套接字 SO_LINGER 选项（linger time \u003e 0） 当 close 超时时，情况和方式 1 一样 当 close 成功返回，说明数据已被对端 TCP 确认，但是不能确保数据被对端应用程读取（如对端程序在从缓冲区读出数据前崩溃） 方式3：write 后 shutdown 写端，调用 read 等到对端的 FIN 才返回 该方式下可以获知对端应用程序已读取我们的数据 ","date":"2024-09-03","objectID":"/posts/network/socket_close/:0:0","series":null,"tags":null,"title":"socket 关闭","uri":"/posts/network/socket_close/#四次挥手"},{"categories":["网络编程"],"content":"\r四次挥手\r客户端主动发起连接，发送 FIN 包 服务端接收到 FIN 包传递给应用程序 服务端应用程序接收到 FIN 包后关闭连接（TCP 向对端发送 FIN） 客户端接收到 FIN 包向对端发送确认包 TIME_WAIT 状态存在的两个原因 防止第三次挥手的数据包丢失 确保每次成功建立的连接不是旧连接的化身 close() 和 shutdown() close() 函数： 调用 close 只是导致相应的描述符引用计数减一，套接字真正的清理和资源释放发生需要其引用计数到达 0 时才发生（计数为 0 时才会发送 FIN 包） SO_LINGER 套接字选项 默认情况下为关闭选项（即 l_onoff 为 0） 选项开启的情况（l_onoff = 1）： l_linger = 0 时，调用 close 函数 TCP 将丢弃套接字发送缓冲区的所有数据并向对端发送 RST 包，这种情况下避免了 TCP 的 TIME_WAIT 状态 l_linger \u003e0 ： 阻塞套接字调用 close：进程会睡眠直到所有数据被对端确认（收到 ack 包）或超时 非阻塞套接字调用 close：调用马上返回，返回值为 EWOULDBLOCK 表示数据被对端确认前超时，此时套接字发送缓冲区的残留数据都会被丢弃 shutdown() 函数： SHUT_RD：套接字接受缓冲区的数据及往后接收到的任何数据都会被丢弃，进程不能对该套接字进行读操作，对套接字发送缓冲区没任何影响（不会发送 FIN 包） SHUT_WR：套接字可以继续接受数据，发送缓冲区的数据和 FIN 包数据会发送到对端 如何关闭连接\r方式1：正常 write 后直接 close 默认情况下 close 会直接返回，TCP 将尝试发送已排队等待发送到对端的数据，发送完后再发送 FIN 包 该方式下客户端无法知道对端 TCP 是否接收该数据（正常情况下 TCP 的可靠性能保证对端接收到剩余数据，但对端主机崩溃的情况下会造成数据丢失） 方式2：设置套接字 SO_LINGER 选项（linger time \u003e 0） 当 close 超时时，情况和方式 1 一样 当 close 成功返回，说明数据已被对端 TCP 确认，但是不能确保数据被对端应用程读取（如对端程序在从缓冲区读出数据前崩溃） 方式3：write 后 shutdown 写端，调用 read 等到对端的 FIN 才返回 该方式下可以获知对端应用程序已读取我们的数据 ","date":"2024-09-03","objectID":"/posts/network/socket_close/:0:0","series":null,"tags":null,"title":"socket 关闭","uri":"/posts/network/socket_close/#close-和-shutdown"},{"categories":["网络编程"],"content":"\r四次挥手\r客户端主动发起连接，发送 FIN 包 服务端接收到 FIN 包传递给应用程序 服务端应用程序接收到 FIN 包后关闭连接（TCP 向对端发送 FIN） 客户端接收到 FIN 包向对端发送确认包 TIME_WAIT 状态存在的两个原因 防止第三次挥手的数据包丢失 确保每次成功建立的连接不是旧连接的化身 close() 和 shutdown() close() 函数： 调用 close 只是导致相应的描述符引用计数减一，套接字真正的清理和资源释放发生需要其引用计数到达 0 时才发生（计数为 0 时才会发送 FIN 包） SO_LINGER 套接字选项 默认情况下为关闭选项（即 l_onoff 为 0） 选项开启的情况（l_onoff = 1）： l_linger = 0 时，调用 close 函数 TCP 将丢弃套接字发送缓冲区的所有数据并向对端发送 RST 包，这种情况下避免了 TCP 的 TIME_WAIT 状态 l_linger \u003e0 ： 阻塞套接字调用 close：进程会睡眠直到所有数据被对端确认（收到 ack 包）或超时 非阻塞套接字调用 close：调用马上返回，返回值为 EWOULDBLOCK 表示数据被对端确认前超时，此时套接字发送缓冲区的残留数据都会被丢弃 shutdown() 函数： SHUT_RD：套接字接受缓冲区的数据及往后接收到的任何数据都会被丢弃，进程不能对该套接字进行读操作，对套接字发送缓冲区没任何影响（不会发送 FIN 包） SHUT_WR：套接字可以继续接受数据，发送缓冲区的数据和 FIN 包数据会发送到对端 如何关闭连接\r方式1：正常 write 后直接 close 默认情况下 close 会直接返回，TCP 将尝试发送已排队等待发送到对端的数据，发送完后再发送 FIN 包 该方式下客户端无法知道对端 TCP 是否接收该数据（正常情况下 TCP 的可靠性能保证对端接收到剩余数据，但对端主机崩溃的情况下会造成数据丢失） 方式2：设置套接字 SO_LINGER 选项（linger time \u003e 0） 当 close 超时时，情况和方式 1 一样 当 close 成功返回，说明数据已被对端 TCP 确认，但是不能确保数据被对端应用程读取（如对端程序在从缓冲区读出数据前崩溃） 方式3：write 后 shutdown 写端，调用 read 等到对端的 FIN 才返回 该方式下可以获知对端应用程序已读取我们的数据 ","date":"2024-09-03","objectID":"/posts/network/socket_close/:0:0","series":null,"tags":null,"title":"socket 关闭","uri":"/posts/network/socket_close/#如何关闭连接"},{"categories":["其他"],"content":"\r主要的数据结构 gobal_State 的成员变量 GCObject **rootgc：存放待 GC 对象的链表，所有对象创建之后都会放入该链表中 GCObject *gray：存放标记为灰色的 GC 对象的链表 struct lua_State：*mainthread：主线程 lu_byte currentwhite：存放当前GC的白色 流程 GC 对象的创建 将对象挂载到扫描过程会遍历的链表（ rootgc ）上 将对象的颜色设置为白色，意指本次GC还未扫描到的对象 初始化阶段（一次完成） 将 mainthread、G 表、registry 表的对象进行标记为灰色 这阶段的标记只是对对象单纯的标记，并没有进行递归的标记，为的是希望这个阶段尽快完成 扫描标记阶段（多次完成） 递归的扫描 gray 链表的对象，将它们及其引用的对象标记为黑色 回收阶段（多次完成） 遍历 rootgc 链表，回收标记为本次回收的白色的对象 结束阶段（多次完成） 回收自带 GC 元方法的 udata 对象 几个注意的点 关于 GC 的两种白色及为什么会有两种白色 假如一个对象在 GC 过程的标记阶段之后创建，它应该是白色的，这样在回收阶段，这个对象就会在没有被扫描标记的情况下被认为是没有被引用的对象而删除 关于局部 GC 对象 局部对象就存在于 mainthread 中，查看代码可以知道，在扫描标记阶段的时候，会对 mainthread（lua thread对象） 进行递归的标记，而对 thread 对象进行标记就是递归遍历其局部变量进行标记的 这里只对 GC 过程进行了简单的概括，各种 GC 对象的 GC 操作，barrier 操作等都没涉及，具体可以查看《Lua 设计与实现》第七章的内容 ","date":"2023-12-18","objectID":"/posts/other/lua_gc/:0:0","series":null,"tags":null,"title":"Lua的GC","uri":"/posts/other/lua_gc/#"},{"categories":["其他"],"content":"\r主要的数据结构 gobal_State 的成员变量 GCObject **rootgc：存放待 GC 对象的链表，所有对象创建之后都会放入该链表中 GCObject *gray：存放标记为灰色的 GC 对象的链表 struct lua_State：*mainthread：主线程 lu_byte currentwhite：存放当前GC的白色 流程 GC 对象的创建 将对象挂载到扫描过程会遍历的链表（ rootgc ）上 将对象的颜色设置为白色，意指本次GC还未扫描到的对象 初始化阶段（一次完成） 将 mainthread、G 表、registry 表的对象进行标记为灰色 这阶段的标记只是对对象单纯的标记，并没有进行递归的标记，为的是希望这个阶段尽快完成 扫描标记阶段（多次完成） 递归的扫描 gray 链表的对象，将它们及其引用的对象标记为黑色 回收阶段（多次完成） 遍历 rootgc 链表，回收标记为本次回收的白色的对象 结束阶段（多次完成） 回收自带 GC 元方法的 udata 对象 几个注意的点 关于 GC 的两种白色及为什么会有两种白色 假如一个对象在 GC 过程的标记阶段之后创建，它应该是白色的，这样在回收阶段，这个对象就会在没有被扫描标记的情况下被认为是没有被引用的对象而删除 关于局部 GC 对象 局部对象就存在于 mainthread 中，查看代码可以知道，在扫描标记阶段的时候，会对 mainthread（lua thread对象） 进行递归的标记，而对 thread 对象进行标记就是递归遍历其局部变量进行标记的 这里只对 GC 过程进行了简单的概括，各种 GC 对象的 GC 操作，barrier 操作等都没涉及，具体可以查看《Lua 设计与实现》第七章的内容 ","date":"2023-12-18","objectID":"/posts/other/lua_gc/:0:0","series":null,"tags":null,"title":"Lua的GC","uri":"/posts/other/lua_gc/#主要的数据结构"},{"categories":["其他"],"content":"\r主要的数据结构 gobal_State 的成员变量 GCObject **rootgc：存放待 GC 对象的链表，所有对象创建之后都会放入该链表中 GCObject *gray：存放标记为灰色的 GC 对象的链表 struct lua_State：*mainthread：主线程 lu_byte currentwhite：存放当前GC的白色 流程 GC 对象的创建 将对象挂载到扫描过程会遍历的链表（ rootgc ）上 将对象的颜色设置为白色，意指本次GC还未扫描到的对象 初始化阶段（一次完成） 将 mainthread、G 表、registry 表的对象进行标记为灰色 这阶段的标记只是对对象单纯的标记，并没有进行递归的标记，为的是希望这个阶段尽快完成 扫描标记阶段（多次完成） 递归的扫描 gray 链表的对象，将它们及其引用的对象标记为黑色 回收阶段（多次完成） 遍历 rootgc 链表，回收标记为本次回收的白色的对象 结束阶段（多次完成） 回收自带 GC 元方法的 udata 对象 几个注意的点 关于 GC 的两种白色及为什么会有两种白色 假如一个对象在 GC 过程的标记阶段之后创建，它应该是白色的，这样在回收阶段，这个对象就会在没有被扫描标记的情况下被认为是没有被引用的对象而删除 关于局部 GC 对象 局部对象就存在于 mainthread 中，查看代码可以知道，在扫描标记阶段的时候，会对 mainthread（lua thread对象） 进行递归的标记，而对 thread 对象进行标记就是递归遍历其局部变量进行标记的 这里只对 GC 过程进行了简单的概括，各种 GC 对象的 GC 操作，barrier 操作等都没涉及，具体可以查看《Lua 设计与实现》第七章的内容 ","date":"2023-12-18","objectID":"/posts/other/lua_gc/:0:0","series":null,"tags":null,"title":"Lua的GC","uri":"/posts/other/lua_gc/#流程"},{"categories":["其他"],"content":"\r主要的数据结构 gobal_State 的成员变量 GCObject **rootgc：存放待 GC 对象的链表，所有对象创建之后都会放入该链表中 GCObject *gray：存放标记为灰色的 GC 对象的链表 struct lua_State：*mainthread：主线程 lu_byte currentwhite：存放当前GC的白色 流程 GC 对象的创建 将对象挂载到扫描过程会遍历的链表（ rootgc ）上 将对象的颜色设置为白色，意指本次GC还未扫描到的对象 初始化阶段（一次完成） 将 mainthread、G 表、registry 表的对象进行标记为灰色 这阶段的标记只是对对象单纯的标记，并没有进行递归的标记，为的是希望这个阶段尽快完成 扫描标记阶段（多次完成） 递归的扫描 gray 链表的对象，将它们及其引用的对象标记为黑色 回收阶段（多次完成） 遍历 rootgc 链表，回收标记为本次回收的白色的对象 结束阶段（多次完成） 回收自带 GC 元方法的 udata 对象 几个注意的点 关于 GC 的两种白色及为什么会有两种白色 假如一个对象在 GC 过程的标记阶段之后创建，它应该是白色的，这样在回收阶段，这个对象就会在没有被扫描标记的情况下被认为是没有被引用的对象而删除 关于局部 GC 对象 局部对象就存在于 mainthread 中，查看代码可以知道，在扫描标记阶段的时候，会对 mainthread（lua thread对象） 进行递归的标记，而对 thread 对象进行标记就是递归遍历其局部变量进行标记的 这里只对 GC 过程进行了简单的概括，各种 GC 对象的 GC 操作，barrier 操作等都没涉及，具体可以查看《Lua 设计与实现》第七章的内容 ","date":"2023-12-18","objectID":"/posts/other/lua_gc/:0:0","series":null,"tags":null,"title":"Lua的GC","uri":"/posts/other/lua_gc/#几个注意的点"},{"categories":["skynet相关"],"content":"\r简述 skynet 可以根据动态库（具有一定的规范）动态的创建出服务 skynet启用多线程处理服务处理消息 服务并不会主动执行逻辑，只有当服务接受到消息时才会执行相应的逻辑（包括定时器，skynet 的定时器是以消息的方式通知给服务的） skynet 自带有多个服务如 logger，snlua等，要想比较快的理解 skynet 的服务，可以从比较简单的服务入手（如 logger ） 服务的结构 skynet 服务对应结构体 skynet_context // skynet-src/skynet_server.c struct skynet_context { void * instance; // 服务实例指针 struct skynet_module * mod; // 动态库指针 void * cb_ud; // 用于回调的指针 skynet_cb cb; // 回调函数指针 struct message_queue *queue; // 服务消息队列 FILE * logfile; // for 服务日志 uint64_t cpu_cost; uint64_t cpu_start; char result[32]; // 存放性能指标的查询结果 uint32_t handle; // 服务的id int session_id; // 消息的session id分配器 int ref; // 服务引用计数 int message_count; // 已处理过的消息总数 bool init; // 初始化成功的标识 bool endless; // 死循环标识 bool profile; // cpu 性能指标开启开关 CHECKCALLING_DECL }; 服务的创建 skynet 通过接口 skynet_context_new 动态创建服务 // skynet-src/skynet_server.c 124 struct skynet_context * 125 skynet_context_new(const char * name, const char *param) { 126 struct skynet_module * mod = skynet_module_query(name); 127 128 if (mod == NULL) 129 return NULL; 130 131 void *inst = skynet_module_instance_create(mod); 132 if (inst == NULL) 133 return NULL; 134 struct skynet_context * ctx = skynet_malloc(sizeof(*ctx)); 135 CHECKCALLING_INIT(ctx) 136 137 ctx-\u003emod = mod; 138 ctx-\u003einstance = inst; 139 ATOM_INIT(\u0026ctx-\u003eref , 2); 140 ctx-\u003ecb = NULL; 141 ctx-\u003ecb_ud = NULL; 142 ctx-\u003esession_id = 0; 143 ATOM_INIT(\u0026ctx-\u003elogfile, (uintptr_t)NULL); 144 145 ctx-\u003einit = false; 146 ctx-\u003eendless = false; 147 148 ctx-\u003ecpu_cost = 0; 149 ctx-\u003ecpu_start = 0; 150 ctx-\u003emessage_count = 0; 151 ctx-\u003eprofile = G_NODE.profile; 152 // Should set to 0 first to avoid skynet_handle_retireall get an uninitialized handle 153 ctx-\u003ehandle = 0; 154 ctx-\u003ehandle = skynet_handle_register(ctx); 155 struct message_queue * queue = ctx-\u003equeue = skynet_mq_create(ctx-\u003ehandle); 156 // init function maybe use ctx-\u003ehandle, so it must init at last 157 context_inc(); 158 159 CHECKCALLING_BEGIN(ctx) 160 int r = skynet_module_instance_init(mod, inst, ctx, param); 161 CHECKCALLING_END(ctx) 162 if (r == 0) { 163 struct skynet_context * ret = skynet_context_release(ctx); 164 if (ret) { 165 ctx-\u003einit = true; 166 } 167 skynet_globalmq_push(queue); 168 if (ret) { 169 skynet_error(ret, \"LAUNCH %s %s\", name, param ? param : \"\"); 170 } 171 return ret; 172 } else { 173 skynet_error(ctx, \"FAILED launch %s\", name); 174 uint32_t handle = ctx-\u003ehandle; 175 skynet_context_release(ctx); 176 skynet_handle_retire(handle); 177 struct drop_t d = { handle }; 178 skynet_mq_release(queue, drop_message, \u0026d); 179 return NULL; 180 } 181 } 根据 name 通过接口 skynet_module_query 获取对应动态库句柄 mod（ 126 行），mod 中包含四个函数指针（create，init，signal，release） 调用mod 的 create 函数创建实例 inst（ 131 行） 创建 skynet_context 实例 ctx，并进行一系列赋值，把 ctx 注册到 handle_storage （ 134 ~ 154） 创建该服务的消息队列 queue，调用 so 库的 init 函数进行初始化（ 160 行），将 queue 加入到全局队列中 消息队列 // skynet-src/skynet_mq.c 21 struct message_queue { 22 \u003e struct spinlock lock; // 自旋锁 23 \u003e uint32_t handle; // 服务id 24 \u003e int cap; 25 \u003e int head; 26 \u003e int tail; 27 \u003e int release; 28 \u003e int in_global; 29 \u003e int overload; 30 \u003e int overload_threshold; 31 \u003e struct skynet_message *queue; // 消息队列 32 \u003e struct message_queue *next; // 下个队列的指针 33 }; 每个服务实例有都有一个消息队列 message_queue，服务待处理的消息都放在 queue 中 所有的消息队列连成一个链表（ next 指针），全局队列 global_queue 的 head 和 tail 存放这个链表的头指针和尾指针 消息的发送 向服务发送消息，其实就是把消息压入服务的消息队列中 700 int 701 skynet_send(struct skynet_context * context, uint32_t source, uint32_t destination , int type, int session, void * data, size_t sz ) { 702 \u003e if ((sz \u0026 MESSAGE_TYPE_MASK) != sz) { 703 \u003e \u003e skynet_error(context, \"The message to %x is too large\", destination); 704 \u003e \u003e if (type \u0026 PTYPE_TAG_DONTCOPY) { 705 \u003e \u003e \u003e skynet_free(data); 706 \u003e \u003e } 707 \u003e \u003e return -2; 708 \u003e } 709 \u003e _filter_args(context, type, \u0026session, (void **)\u0026data, \u0026sz); 710 711 \u003e if (source == 0) { 712 \u003e \u003e source = context-\u003ehandle; 713 \u003e } 714 715 \u003e if (destination == 0) { 716 \u003e \u003e if (data) { 717 \u003e \u003e \u003e skynet_error(context, \"Destination address can't be 0\"); 718 \u003e \u003e \u003e skynet_free(data); 719 \u003e \u003e \u003e return -1; 720 \u003e \u003e } ","date":"2023-12-16","objectID":"/posts/skynet/skynet_cxt/:0:0","series":null,"tags":null,"title":"skynet的服务","uri":"/posts/skynet/skynet_cxt/#"},{"categories":["skynet相关"],"content":"\r简述 skynet 可以根据动态库（具有一定的规范）动态的创建出服务 skynet启用多线程处理服务处理消息 服务并不会主动执行逻辑，只有当服务接受到消息时才会执行相应的逻辑（包括定时器，skynet 的定时器是以消息的方式通知给服务的） skynet 自带有多个服务如 logger，snlua等，要想比较快的理解 skynet 的服务，可以从比较简单的服务入手（如 logger ） 服务的结构 skynet 服务对应结构体 skynet_context // skynet-src/skynet_server.c struct skynet_context { void * instance; // 服务实例指针 struct skynet_module * mod; // 动态库指针 void * cb_ud; // 用于回调的指针 skynet_cb cb; // 回调函数指针 struct message_queue *queue; // 服务消息队列 FILE * logfile; // for 服务日志 uint64_t cpu_cost; uint64_t cpu_start; char result[32]; // 存放性能指标的查询结果 uint32_t handle; // 服务的id int session_id; // 消息的session id分配器 int ref; // 服务引用计数 int message_count; // 已处理过的消息总数 bool init; // 初始化成功的标识 bool endless; // 死循环标识 bool profile; // cpu 性能指标开启开关 CHECKCALLING_DECL }; 服务的创建 skynet 通过接口 skynet_context_new 动态创建服务 // skynet-src/skynet_server.c 124 struct skynet_context * 125 skynet_context_new(const char * name, const char *param) { 126 struct skynet_module * mod = skynet_module_query(name); 127 128 if (mod == NULL) 129 return NULL; 130 131 void *inst = skynet_module_instance_create(mod); 132 if (inst == NULL) 133 return NULL; 134 struct skynet_context * ctx = skynet_malloc(sizeof(*ctx)); 135 CHECKCALLING_INIT(ctx) 136 137 ctx-\u003emod = mod; 138 ctx-\u003einstance = inst; 139 ATOM_INIT(\u0026ctx-\u003eref , 2); 140 ctx-\u003ecb = NULL; 141 ctx-\u003ecb_ud = NULL; 142 ctx-\u003esession_id = 0; 143 ATOM_INIT(\u0026ctx-\u003elogfile, (uintptr_t)NULL); 144 145 ctx-\u003einit = false; 146 ctx-\u003eendless = false; 147 148 ctx-\u003ecpu_cost = 0; 149 ctx-\u003ecpu_start = 0; 150 ctx-\u003emessage_count = 0; 151 ctx-\u003eprofile = G_NODE.profile; 152 // Should set to 0 first to avoid skynet_handle_retireall get an uninitialized handle 153 ctx-\u003ehandle = 0; 154 ctx-\u003ehandle = skynet_handle_register(ctx); 155 struct message_queue * queue = ctx-\u003equeue = skynet_mq_create(ctx-\u003ehandle); 156 // init function maybe use ctx-\u003ehandle, so it must init at last 157 context_inc(); 158 159 CHECKCALLING_BEGIN(ctx) 160 int r = skynet_module_instance_init(mod, inst, ctx, param); 161 CHECKCALLING_END(ctx) 162 if (r == 0) { 163 struct skynet_context * ret = skynet_context_release(ctx); 164 if (ret) { 165 ctx-\u003einit = true; 166 } 167 skynet_globalmq_push(queue); 168 if (ret) { 169 skynet_error(ret, \"LAUNCH %s %s\", name, param ? param : \"\"); 170 } 171 return ret; 172 } else { 173 skynet_error(ctx, \"FAILED launch %s\", name); 174 uint32_t handle = ctx-\u003ehandle; 175 skynet_context_release(ctx); 176 skynet_handle_retire(handle); 177 struct drop_t d = { handle }; 178 skynet_mq_release(queue, drop_message, \u0026d); 179 return NULL; 180 } 181 } 根据 name 通过接口 skynet_module_query 获取对应动态库句柄 mod（ 126 行），mod 中包含四个函数指针（create，init，signal，release） 调用mod 的 create 函数创建实例 inst（ 131 行） 创建 skynet_context 实例 ctx，并进行一系列赋值，把 ctx 注册到 handle_storage （ 134 ~ 154） 创建该服务的消息队列 queue，调用 so 库的 init 函数进行初始化（ 160 行），将 queue 加入到全局队列中 消息队列 // skynet-src/skynet_mq.c 21 struct message_queue { 22 \u003e struct spinlock lock; // 自旋锁 23 \u003e uint32_t handle; // 服务id 24 \u003e int cap; 25 \u003e int head; 26 \u003e int tail; 27 \u003e int release; 28 \u003e int in_global; 29 \u003e int overload; 30 \u003e int overload_threshold; 31 \u003e struct skynet_message *queue; // 消息队列 32 \u003e struct message_queue *next; // 下个队列的指针 33 }; 每个服务实例有都有一个消息队列 message_queue，服务待处理的消息都放在 queue 中 所有的消息队列连成一个链表（ next 指针），全局队列 global_queue 的 head 和 tail 存放这个链表的头指针和尾指针 消息的发送 向服务发送消息，其实就是把消息压入服务的消息队列中 700 int 701 skynet_send(struct skynet_context * context, uint32_t source, uint32_t destination , int type, int session, void * data, size_t sz ) { 702 \u003e if ((sz \u0026 MESSAGE_TYPE_MASK) != sz) { 703 \u003e \u003e skynet_error(context, \"The message to %x is too large\", destination); 704 \u003e \u003e if (type \u0026 PTYPE_TAG_DONTCOPY) { 705 \u003e \u003e \u003e skynet_free(data); 706 \u003e \u003e } 707 \u003e \u003e return -2; 708 \u003e } 709 \u003e _filter_args(context, type, \u0026session, (void **)\u0026data, \u0026sz); 710 711 \u003e if (source == 0) { 712 \u003e \u003e source = context-\u003ehandle; 713 \u003e } 714 715 \u003e if (destination == 0) { 716 \u003e \u003e if (data) { 717 \u003e \u003e \u003e skynet_error(context, \"Destination address can't be 0\"); 718 \u003e \u003e \u003e skynet_free(data); 719 \u003e \u003e \u003e return -1; 720 \u003e \u003e } ","date":"2023-12-16","objectID":"/posts/skynet/skynet_cxt/:0:0","series":null,"tags":null,"title":"skynet的服务","uri":"/posts/skynet/skynet_cxt/#简述"},{"categories":["skynet相关"],"content":"\r简述 skynet 可以根据动态库（具有一定的规范）动态的创建出服务 skynet启用多线程处理服务处理消息 服务并不会主动执行逻辑，只有当服务接受到消息时才会执行相应的逻辑（包括定时器，skynet 的定时器是以消息的方式通知给服务的） skynet 自带有多个服务如 logger，snlua等，要想比较快的理解 skynet 的服务，可以从比较简单的服务入手（如 logger ） 服务的结构 skynet 服务对应结构体 skynet_context // skynet-src/skynet_server.c struct skynet_context { void * instance; // 服务实例指针 struct skynet_module * mod; // 动态库指针 void * cb_ud; // 用于回调的指针 skynet_cb cb; // 回调函数指针 struct message_queue *queue; // 服务消息队列 FILE * logfile; // for 服务日志 uint64_t cpu_cost; uint64_t cpu_start; char result[32]; // 存放性能指标的查询结果 uint32_t handle; // 服务的id int session_id; // 消息的session id分配器 int ref; // 服务引用计数 int message_count; // 已处理过的消息总数 bool init; // 初始化成功的标识 bool endless; // 死循环标识 bool profile; // cpu 性能指标开启开关 CHECKCALLING_DECL }; 服务的创建 skynet 通过接口 skynet_context_new 动态创建服务 // skynet-src/skynet_server.c 124 struct skynet_context * 125 skynet_context_new(const char * name, const char *param) { 126 struct skynet_module * mod = skynet_module_query(name); 127 128 if (mod == NULL) 129 return NULL; 130 131 void *inst = skynet_module_instance_create(mod); 132 if (inst == NULL) 133 return NULL; 134 struct skynet_context * ctx = skynet_malloc(sizeof(*ctx)); 135 CHECKCALLING_INIT(ctx) 136 137 ctx-\u003emod = mod; 138 ctx-\u003einstance = inst; 139 ATOM_INIT(\u0026ctx-\u003eref , 2); 140 ctx-\u003ecb = NULL; 141 ctx-\u003ecb_ud = NULL; 142 ctx-\u003esession_id = 0; 143 ATOM_INIT(\u0026ctx-\u003elogfile, (uintptr_t)NULL); 144 145 ctx-\u003einit = false; 146 ctx-\u003eendless = false; 147 148 ctx-\u003ecpu_cost = 0; 149 ctx-\u003ecpu_start = 0; 150 ctx-\u003emessage_count = 0; 151 ctx-\u003eprofile = G_NODE.profile; 152 // Should set to 0 first to avoid skynet_handle_retireall get an uninitialized handle 153 ctx-\u003ehandle = 0; 154 ctx-\u003ehandle = skynet_handle_register(ctx); 155 struct message_queue * queue = ctx-\u003equeue = skynet_mq_create(ctx-\u003ehandle); 156 // init function maybe use ctx-\u003ehandle, so it must init at last 157 context_inc(); 158 159 CHECKCALLING_BEGIN(ctx) 160 int r = skynet_module_instance_init(mod, inst, ctx, param); 161 CHECKCALLING_END(ctx) 162 if (r == 0) { 163 struct skynet_context * ret = skynet_context_release(ctx); 164 if (ret) { 165 ctx-\u003einit = true; 166 } 167 skynet_globalmq_push(queue); 168 if (ret) { 169 skynet_error(ret, \"LAUNCH %s %s\", name, param ? param : \"\"); 170 } 171 return ret; 172 } else { 173 skynet_error(ctx, \"FAILED launch %s\", name); 174 uint32_t handle = ctx-\u003ehandle; 175 skynet_context_release(ctx); 176 skynet_handle_retire(handle); 177 struct drop_t d = { handle }; 178 skynet_mq_release(queue, drop_message, \u0026d); 179 return NULL; 180 } 181 } 根据 name 通过接口 skynet_module_query 获取对应动态库句柄 mod（ 126 行），mod 中包含四个函数指针（create，init，signal，release） 调用mod 的 create 函数创建实例 inst（ 131 行） 创建 skynet_context 实例 ctx，并进行一系列赋值，把 ctx 注册到 handle_storage （ 134 ~ 154） 创建该服务的消息队列 queue，调用 so 库的 init 函数进行初始化（ 160 行），将 queue 加入到全局队列中 消息队列 // skynet-src/skynet_mq.c 21 struct message_queue { 22 \u003e struct spinlock lock; // 自旋锁 23 \u003e uint32_t handle; // 服务id 24 \u003e int cap; 25 \u003e int head; 26 \u003e int tail; 27 \u003e int release; 28 \u003e int in_global; 29 \u003e int overload; 30 \u003e int overload_threshold; 31 \u003e struct skynet_message *queue; // 消息队列 32 \u003e struct message_queue *next; // 下个队列的指针 33 }; 每个服务实例有都有一个消息队列 message_queue，服务待处理的消息都放在 queue 中 所有的消息队列连成一个链表（ next 指针），全局队列 global_queue 的 head 和 tail 存放这个链表的头指针和尾指针 消息的发送 向服务发送消息，其实就是把消息压入服务的消息队列中 700 int 701 skynet_send(struct skynet_context * context, uint32_t source, uint32_t destination , int type, int session, void * data, size_t sz ) { 702 \u003e if ((sz \u0026 MESSAGE_TYPE_MASK) != sz) { 703 \u003e \u003e skynet_error(context, \"The message to %x is too large\", destination); 704 \u003e \u003e if (type \u0026 PTYPE_TAG_DONTCOPY) { 705 \u003e \u003e \u003e skynet_free(data); 706 \u003e \u003e } 707 \u003e \u003e return -2; 708 \u003e } 709 \u003e _filter_args(context, type, \u0026session, (void **)\u0026data, \u0026sz); 710 711 \u003e if (source == 0) { 712 \u003e \u003e source = context-\u003ehandle; 713 \u003e } 714 715 \u003e if (destination == 0) { 716 \u003e \u003e if (data) { 717 \u003e \u003e \u003e skynet_error(context, \"Destination address can't be 0\"); 718 \u003e \u003e \u003e skynet_free(data); 719 \u003e \u003e \u003e return -1; 720 \u003e \u003e } ","date":"2023-12-16","objectID":"/posts/skynet/skynet_cxt/:0:0","series":null,"tags":null,"title":"skynet的服务","uri":"/posts/skynet/skynet_cxt/#服务的结构"},{"categories":["skynet相关"],"content":"\r简述 skynet 可以根据动态库（具有一定的规范）动态的创建出服务 skynet启用多线程处理服务处理消息 服务并不会主动执行逻辑，只有当服务接受到消息时才会执行相应的逻辑（包括定时器，skynet 的定时器是以消息的方式通知给服务的） skynet 自带有多个服务如 logger，snlua等，要想比较快的理解 skynet 的服务，可以从比较简单的服务入手（如 logger ） 服务的结构 skynet 服务对应结构体 skynet_context // skynet-src/skynet_server.c struct skynet_context { void * instance; // 服务实例指针 struct skynet_module * mod; // 动态库指针 void * cb_ud; // 用于回调的指针 skynet_cb cb; // 回调函数指针 struct message_queue *queue; // 服务消息队列 FILE * logfile; // for 服务日志 uint64_t cpu_cost; uint64_t cpu_start; char result[32]; // 存放性能指标的查询结果 uint32_t handle; // 服务的id int session_id; // 消息的session id分配器 int ref; // 服务引用计数 int message_count; // 已处理过的消息总数 bool init; // 初始化成功的标识 bool endless; // 死循环标识 bool profile; // cpu 性能指标开启开关 CHECKCALLING_DECL }; 服务的创建 skynet 通过接口 skynet_context_new 动态创建服务 // skynet-src/skynet_server.c 124 struct skynet_context * 125 skynet_context_new(const char * name, const char *param) { 126 struct skynet_module * mod = skynet_module_query(name); 127 128 if (mod == NULL) 129 return NULL; 130 131 void *inst = skynet_module_instance_create(mod); 132 if (inst == NULL) 133 return NULL; 134 struct skynet_context * ctx = skynet_malloc(sizeof(*ctx)); 135 CHECKCALLING_INIT(ctx) 136 137 ctx-\u003emod = mod; 138 ctx-\u003einstance = inst; 139 ATOM_INIT(\u0026ctx-\u003eref , 2); 140 ctx-\u003ecb = NULL; 141 ctx-\u003ecb_ud = NULL; 142 ctx-\u003esession_id = 0; 143 ATOM_INIT(\u0026ctx-\u003elogfile, (uintptr_t)NULL); 144 145 ctx-\u003einit = false; 146 ctx-\u003eendless = false; 147 148 ctx-\u003ecpu_cost = 0; 149 ctx-\u003ecpu_start = 0; 150 ctx-\u003emessage_count = 0; 151 ctx-\u003eprofile = G_NODE.profile; 152 // Should set to 0 first to avoid skynet_handle_retireall get an uninitialized handle 153 ctx-\u003ehandle = 0; 154 ctx-\u003ehandle = skynet_handle_register(ctx); 155 struct message_queue * queue = ctx-\u003equeue = skynet_mq_create(ctx-\u003ehandle); 156 // init function maybe use ctx-\u003ehandle, so it must init at last 157 context_inc(); 158 159 CHECKCALLING_BEGIN(ctx) 160 int r = skynet_module_instance_init(mod, inst, ctx, param); 161 CHECKCALLING_END(ctx) 162 if (r == 0) { 163 struct skynet_context * ret = skynet_context_release(ctx); 164 if (ret) { 165 ctx-\u003einit = true; 166 } 167 skynet_globalmq_push(queue); 168 if (ret) { 169 skynet_error(ret, \"LAUNCH %s %s\", name, param ? param : \"\"); 170 } 171 return ret; 172 } else { 173 skynet_error(ctx, \"FAILED launch %s\", name); 174 uint32_t handle = ctx-\u003ehandle; 175 skynet_context_release(ctx); 176 skynet_handle_retire(handle); 177 struct drop_t d = { handle }; 178 skynet_mq_release(queue, drop_message, \u0026d); 179 return NULL; 180 } 181 } 根据 name 通过接口 skynet_module_query 获取对应动态库句柄 mod（ 126 行），mod 中包含四个函数指针（create，init，signal，release） 调用mod 的 create 函数创建实例 inst（ 131 行） 创建 skynet_context 实例 ctx，并进行一系列赋值，把 ctx 注册到 handle_storage （ 134 ~ 154） 创建该服务的消息队列 queue，调用 so 库的 init 函数进行初始化（ 160 行），将 queue 加入到全局队列中 消息队列 // skynet-src/skynet_mq.c 21 struct message_queue { 22 \u003e struct spinlock lock; // 自旋锁 23 \u003e uint32_t handle; // 服务id 24 \u003e int cap; 25 \u003e int head; 26 \u003e int tail; 27 \u003e int release; 28 \u003e int in_global; 29 \u003e int overload; 30 \u003e int overload_threshold; 31 \u003e struct skynet_message *queue; // 消息队列 32 \u003e struct message_queue *next; // 下个队列的指针 33 }; 每个服务实例有都有一个消息队列 message_queue，服务待处理的消息都放在 queue 中 所有的消息队列连成一个链表（ next 指针），全局队列 global_queue 的 head 和 tail 存放这个链表的头指针和尾指针 消息的发送 向服务发送消息，其实就是把消息压入服务的消息队列中 700 int 701 skynet_send(struct skynet_context * context, uint32_t source, uint32_t destination , int type, int session, void * data, size_t sz ) { 702 \u003e if ((sz \u0026 MESSAGE_TYPE_MASK) != sz) { 703 \u003e \u003e skynet_error(context, \"The message to %x is too large\", destination); 704 \u003e \u003e if (type \u0026 PTYPE_TAG_DONTCOPY) { 705 \u003e \u003e \u003e skynet_free(data); 706 \u003e \u003e } 707 \u003e \u003e return -2; 708 \u003e } 709 \u003e _filter_args(context, type, \u0026session, (void **)\u0026data, \u0026sz); 710 711 \u003e if (source == 0) { 712 \u003e \u003e source = context-\u003ehandle; 713 \u003e } 714 715 \u003e if (destination == 0) { 716 \u003e \u003e if (data) { 717 \u003e \u003e \u003e skynet_error(context, \"Destination address can't be 0\"); 718 \u003e \u003e \u003e skynet_free(data); 719 \u003e \u003e \u003e return -1; 720 \u003e \u003e } ","date":"2023-12-16","objectID":"/posts/skynet/skynet_cxt/:0:0","series":null,"tags":null,"title":"skynet的服务","uri":"/posts/skynet/skynet_cxt/#服务的创建"},{"categories":["skynet相关"],"content":"\r简述 skynet 可以根据动态库（具有一定的规范）动态的创建出服务 skynet启用多线程处理服务处理消息 服务并不会主动执行逻辑，只有当服务接受到消息时才会执行相应的逻辑（包括定时器，skynet 的定时器是以消息的方式通知给服务的） skynet 自带有多个服务如 logger，snlua等，要想比较快的理解 skynet 的服务，可以从比较简单的服务入手（如 logger ） 服务的结构 skynet 服务对应结构体 skynet_context // skynet-src/skynet_server.c struct skynet_context { void * instance; // 服务实例指针 struct skynet_module * mod; // 动态库指针 void * cb_ud; // 用于回调的指针 skynet_cb cb; // 回调函数指针 struct message_queue *queue; // 服务消息队列 FILE * logfile; // for 服务日志 uint64_t cpu_cost; uint64_t cpu_start; char result[32]; // 存放性能指标的查询结果 uint32_t handle; // 服务的id int session_id; // 消息的session id分配器 int ref; // 服务引用计数 int message_count; // 已处理过的消息总数 bool init; // 初始化成功的标识 bool endless; // 死循环标识 bool profile; // cpu 性能指标开启开关 CHECKCALLING_DECL }; 服务的创建 skynet 通过接口 skynet_context_new 动态创建服务 // skynet-src/skynet_server.c 124 struct skynet_context * 125 skynet_context_new(const char * name, const char *param) { 126 struct skynet_module * mod = skynet_module_query(name); 127 128 if (mod == NULL) 129 return NULL; 130 131 void *inst = skynet_module_instance_create(mod); 132 if (inst == NULL) 133 return NULL; 134 struct skynet_context * ctx = skynet_malloc(sizeof(*ctx)); 135 CHECKCALLING_INIT(ctx) 136 137 ctx-\u003emod = mod; 138 ctx-\u003einstance = inst; 139 ATOM_INIT(\u0026ctx-\u003eref , 2); 140 ctx-\u003ecb = NULL; 141 ctx-\u003ecb_ud = NULL; 142 ctx-\u003esession_id = 0; 143 ATOM_INIT(\u0026ctx-\u003elogfile, (uintptr_t)NULL); 144 145 ctx-\u003einit = false; 146 ctx-\u003eendless = false; 147 148 ctx-\u003ecpu_cost = 0; 149 ctx-\u003ecpu_start = 0; 150 ctx-\u003emessage_count = 0; 151 ctx-\u003eprofile = G_NODE.profile; 152 // Should set to 0 first to avoid skynet_handle_retireall get an uninitialized handle 153 ctx-\u003ehandle = 0; 154 ctx-\u003ehandle = skynet_handle_register(ctx); 155 struct message_queue * queue = ctx-\u003equeue = skynet_mq_create(ctx-\u003ehandle); 156 // init function maybe use ctx-\u003ehandle, so it must init at last 157 context_inc(); 158 159 CHECKCALLING_BEGIN(ctx) 160 int r = skynet_module_instance_init(mod, inst, ctx, param); 161 CHECKCALLING_END(ctx) 162 if (r == 0) { 163 struct skynet_context * ret = skynet_context_release(ctx); 164 if (ret) { 165 ctx-\u003einit = true; 166 } 167 skynet_globalmq_push(queue); 168 if (ret) { 169 skynet_error(ret, \"LAUNCH %s %s\", name, param ? param : \"\"); 170 } 171 return ret; 172 } else { 173 skynet_error(ctx, \"FAILED launch %s\", name); 174 uint32_t handle = ctx-\u003ehandle; 175 skynet_context_release(ctx); 176 skynet_handle_retire(handle); 177 struct drop_t d = { handle }; 178 skynet_mq_release(queue, drop_message, \u0026d); 179 return NULL; 180 } 181 } 根据 name 通过接口 skynet_module_query 获取对应动态库句柄 mod（ 126 行），mod 中包含四个函数指针（create，init，signal，release） 调用mod 的 create 函数创建实例 inst（ 131 行） 创建 skynet_context 实例 ctx，并进行一系列赋值，把 ctx 注册到 handle_storage （ 134 ~ 154） 创建该服务的消息队列 queue，调用 so 库的 init 函数进行初始化（ 160 行），将 queue 加入到全局队列中 消息队列 // skynet-src/skynet_mq.c 21 struct message_queue { 22 \u003e struct spinlock lock; // 自旋锁 23 \u003e uint32_t handle; // 服务id 24 \u003e int cap; 25 \u003e int head; 26 \u003e int tail; 27 \u003e int release; 28 \u003e int in_global; 29 \u003e int overload; 30 \u003e int overload_threshold; 31 \u003e struct skynet_message *queue; // 消息队列 32 \u003e struct message_queue *next; // 下个队列的指针 33 }; 每个服务实例有都有一个消息队列 message_queue，服务待处理的消息都放在 queue 中 所有的消息队列连成一个链表（ next 指针），全局队列 global_queue 的 head 和 tail 存放这个链表的头指针和尾指针 消息的发送 向服务发送消息，其实就是把消息压入服务的消息队列中 700 int 701 skynet_send(struct skynet_context * context, uint32_t source, uint32_t destination , int type, int session, void * data, size_t sz ) { 702 \u003e if ((sz \u0026 MESSAGE_TYPE_MASK) != sz) { 703 \u003e \u003e skynet_error(context, \"The message to %x is too large\", destination); 704 \u003e \u003e if (type \u0026 PTYPE_TAG_DONTCOPY) { 705 \u003e \u003e \u003e skynet_free(data); 706 \u003e \u003e } 707 \u003e \u003e return -2; 708 \u003e } 709 \u003e _filter_args(context, type, \u0026session, (void **)\u0026data, \u0026sz); 710 711 \u003e if (source == 0) { 712 \u003e \u003e source = context-\u003ehandle; 713 \u003e } 714 715 \u003e if (destination == 0) { 716 \u003e \u003e if (data) { 717 \u003e \u003e \u003e skynet_error(context, \"Destination address can't be 0\"); 718 \u003e \u003e \u003e skynet_free(data); 719 \u003e \u003e \u003e return -1; 720 \u003e \u003e } ","date":"2023-12-16","objectID":"/posts/skynet/skynet_cxt/:0:0","series":null,"tags":null,"title":"skynet的服务","uri":"/posts/skynet/skynet_cxt/#消息队列"},{"categories":["skynet相关"],"content":"\r简述 skynet 可以根据动态库（具有一定的规范）动态的创建出服务 skynet启用多线程处理服务处理消息 服务并不会主动执行逻辑，只有当服务接受到消息时才会执行相应的逻辑（包括定时器，skynet 的定时器是以消息的方式通知给服务的） skynet 自带有多个服务如 logger，snlua等，要想比较快的理解 skynet 的服务，可以从比较简单的服务入手（如 logger ） 服务的结构 skynet 服务对应结构体 skynet_context // skynet-src/skynet_server.c struct skynet_context { void * instance; // 服务实例指针 struct skynet_module * mod; // 动态库指针 void * cb_ud; // 用于回调的指针 skynet_cb cb; // 回调函数指针 struct message_queue *queue; // 服务消息队列 FILE * logfile; // for 服务日志 uint64_t cpu_cost; uint64_t cpu_start; char result[32]; // 存放性能指标的查询结果 uint32_t handle; // 服务的id int session_id; // 消息的session id分配器 int ref; // 服务引用计数 int message_count; // 已处理过的消息总数 bool init; // 初始化成功的标识 bool endless; // 死循环标识 bool profile; // cpu 性能指标开启开关 CHECKCALLING_DECL }; 服务的创建 skynet 通过接口 skynet_context_new 动态创建服务 // skynet-src/skynet_server.c 124 struct skynet_context * 125 skynet_context_new(const char * name, const char *param) { 126 struct skynet_module * mod = skynet_module_query(name); 127 128 if (mod == NULL) 129 return NULL; 130 131 void *inst = skynet_module_instance_create(mod); 132 if (inst == NULL) 133 return NULL; 134 struct skynet_context * ctx = skynet_malloc(sizeof(*ctx)); 135 CHECKCALLING_INIT(ctx) 136 137 ctx-\u003emod = mod; 138 ctx-\u003einstance = inst; 139 ATOM_INIT(\u0026ctx-\u003eref , 2); 140 ctx-\u003ecb = NULL; 141 ctx-\u003ecb_ud = NULL; 142 ctx-\u003esession_id = 0; 143 ATOM_INIT(\u0026ctx-\u003elogfile, (uintptr_t)NULL); 144 145 ctx-\u003einit = false; 146 ctx-\u003eendless = false; 147 148 ctx-\u003ecpu_cost = 0; 149 ctx-\u003ecpu_start = 0; 150 ctx-\u003emessage_count = 0; 151 ctx-\u003eprofile = G_NODE.profile; 152 // Should set to 0 first to avoid skynet_handle_retireall get an uninitialized handle 153 ctx-\u003ehandle = 0; 154 ctx-\u003ehandle = skynet_handle_register(ctx); 155 struct message_queue * queue = ctx-\u003equeue = skynet_mq_create(ctx-\u003ehandle); 156 // init function maybe use ctx-\u003ehandle, so it must init at last 157 context_inc(); 158 159 CHECKCALLING_BEGIN(ctx) 160 int r = skynet_module_instance_init(mod, inst, ctx, param); 161 CHECKCALLING_END(ctx) 162 if (r == 0) { 163 struct skynet_context * ret = skynet_context_release(ctx); 164 if (ret) { 165 ctx-\u003einit = true; 166 } 167 skynet_globalmq_push(queue); 168 if (ret) { 169 skynet_error(ret, \"LAUNCH %s %s\", name, param ? param : \"\"); 170 } 171 return ret; 172 } else { 173 skynet_error(ctx, \"FAILED launch %s\", name); 174 uint32_t handle = ctx-\u003ehandle; 175 skynet_context_release(ctx); 176 skynet_handle_retire(handle); 177 struct drop_t d = { handle }; 178 skynet_mq_release(queue, drop_message, \u0026d); 179 return NULL; 180 } 181 } 根据 name 通过接口 skynet_module_query 获取对应动态库句柄 mod（ 126 行），mod 中包含四个函数指针（create，init，signal，release） 调用mod 的 create 函数创建实例 inst（ 131 行） 创建 skynet_context 实例 ctx，并进行一系列赋值，把 ctx 注册到 handle_storage （ 134 ~ 154） 创建该服务的消息队列 queue，调用 so 库的 init 函数进行初始化（ 160 行），将 queue 加入到全局队列中 消息队列 // skynet-src/skynet_mq.c 21 struct message_queue { 22 \u003e struct spinlock lock; // 自旋锁 23 \u003e uint32_t handle; // 服务id 24 \u003e int cap; 25 \u003e int head; 26 \u003e int tail; 27 \u003e int release; 28 \u003e int in_global; 29 \u003e int overload; 30 \u003e int overload_threshold; 31 \u003e struct skynet_message *queue; // 消息队列 32 \u003e struct message_queue *next; // 下个队列的指针 33 }; 每个服务实例有都有一个消息队列 message_queue，服务待处理的消息都放在 queue 中 所有的消息队列连成一个链表（ next 指针），全局队列 global_queue 的 head 和 tail 存放这个链表的头指针和尾指针 消息的发送 向服务发送消息，其实就是把消息压入服务的消息队列中 700 int 701 skynet_send(struct skynet_context * context, uint32_t source, uint32_t destination , int type, int session, void * data, size_t sz ) { 702 \u003e if ((sz \u0026 MESSAGE_TYPE_MASK) != sz) { 703 \u003e \u003e skynet_error(context, \"The message to %x is too large\", destination); 704 \u003e \u003e if (type \u0026 PTYPE_TAG_DONTCOPY) { 705 \u003e \u003e \u003e skynet_free(data); 706 \u003e \u003e } 707 \u003e \u003e return -2; 708 \u003e } 709 \u003e _filter_args(context, type, \u0026session, (void **)\u0026data, \u0026sz); 710 711 \u003e if (source == 0) { 712 \u003e \u003e source = context-\u003ehandle; 713 \u003e } 714 715 \u003e if (destination == 0) { 716 \u003e \u003e if (data) { 717 \u003e \u003e \u003e skynet_error(context, \"Destination address can't be 0\"); 718 \u003e \u003e \u003e skynet_free(data); 719 \u003e \u003e \u003e return -1; 720 \u003e \u003e } ","date":"2023-12-16","objectID":"/posts/skynet/skynet_cxt/:0:0","series":null,"tags":null,"title":"skynet的服务","uri":"/posts/skynet/skynet_cxt/#消息的发送"},{"categories":["skynet相关"],"content":"\r简述 skynet 可以根据动态库（具有一定的规范）动态的创建出服务 skynet启用多线程处理服务处理消息 服务并不会主动执行逻辑，只有当服务接受到消息时才会执行相应的逻辑（包括定时器，skynet 的定时器是以消息的方式通知给服务的） skynet 自带有多个服务如 logger，snlua等，要想比较快的理解 skynet 的服务，可以从比较简单的服务入手（如 logger ） 服务的结构 skynet 服务对应结构体 skynet_context // skynet-src/skynet_server.c struct skynet_context { void * instance; // 服务实例指针 struct skynet_module * mod; // 动态库指针 void * cb_ud; // 用于回调的指针 skynet_cb cb; // 回调函数指针 struct message_queue *queue; // 服务消息队列 FILE * logfile; // for 服务日志 uint64_t cpu_cost; uint64_t cpu_start; char result[32]; // 存放性能指标的查询结果 uint32_t handle; // 服务的id int session_id; // 消息的session id分配器 int ref; // 服务引用计数 int message_count; // 已处理过的消息总数 bool init; // 初始化成功的标识 bool endless; // 死循环标识 bool profile; // cpu 性能指标开启开关 CHECKCALLING_DECL }; 服务的创建 skynet 通过接口 skynet_context_new 动态创建服务 // skynet-src/skynet_server.c 124 struct skynet_context * 125 skynet_context_new(const char * name, const char *param) { 126 struct skynet_module * mod = skynet_module_query(name); 127 128 if (mod == NULL) 129 return NULL; 130 131 void *inst = skynet_module_instance_create(mod); 132 if (inst == NULL) 133 return NULL; 134 struct skynet_context * ctx = skynet_malloc(sizeof(*ctx)); 135 CHECKCALLING_INIT(ctx) 136 137 ctx-\u003emod = mod; 138 ctx-\u003einstance = inst; 139 ATOM_INIT(\u0026ctx-\u003eref , 2); 140 ctx-\u003ecb = NULL; 141 ctx-\u003ecb_ud = NULL; 142 ctx-\u003esession_id = 0; 143 ATOM_INIT(\u0026ctx-\u003elogfile, (uintptr_t)NULL); 144 145 ctx-\u003einit = false; 146 ctx-\u003eendless = false; 147 148 ctx-\u003ecpu_cost = 0; 149 ctx-\u003ecpu_start = 0; 150 ctx-\u003emessage_count = 0; 151 ctx-\u003eprofile = G_NODE.profile; 152 // Should set to 0 first to avoid skynet_handle_retireall get an uninitialized handle 153 ctx-\u003ehandle = 0; 154 ctx-\u003ehandle = skynet_handle_register(ctx); 155 struct message_queue * queue = ctx-\u003equeue = skynet_mq_create(ctx-\u003ehandle); 156 // init function maybe use ctx-\u003ehandle, so it must init at last 157 context_inc(); 158 159 CHECKCALLING_BEGIN(ctx) 160 int r = skynet_module_instance_init(mod, inst, ctx, param); 161 CHECKCALLING_END(ctx) 162 if (r == 0) { 163 struct skynet_context * ret = skynet_context_release(ctx); 164 if (ret) { 165 ctx-\u003einit = true; 166 } 167 skynet_globalmq_push(queue); 168 if (ret) { 169 skynet_error(ret, \"LAUNCH %s %s\", name, param ? param : \"\"); 170 } 171 return ret; 172 } else { 173 skynet_error(ctx, \"FAILED launch %s\", name); 174 uint32_t handle = ctx-\u003ehandle; 175 skynet_context_release(ctx); 176 skynet_handle_retire(handle); 177 struct drop_t d = { handle }; 178 skynet_mq_release(queue, drop_message, \u0026d); 179 return NULL; 180 } 181 } 根据 name 通过接口 skynet_module_query 获取对应动态库句柄 mod（ 126 行），mod 中包含四个函数指针（create，init，signal，release） 调用mod 的 create 函数创建实例 inst（ 131 行） 创建 skynet_context 实例 ctx，并进行一系列赋值，把 ctx 注册到 handle_storage （ 134 ~ 154） 创建该服务的消息队列 queue，调用 so 库的 init 函数进行初始化（ 160 行），将 queue 加入到全局队列中 消息队列 // skynet-src/skynet_mq.c 21 struct message_queue { 22 \u003e struct spinlock lock; // 自旋锁 23 \u003e uint32_t handle; // 服务id 24 \u003e int cap; 25 \u003e int head; 26 \u003e int tail; 27 \u003e int release; 28 \u003e int in_global; 29 \u003e int overload; 30 \u003e int overload_threshold; 31 \u003e struct skynet_message *queue; // 消息队列 32 \u003e struct message_queue *next; // 下个队列的指针 33 }; 每个服务实例有都有一个消息队列 message_queue，服务待处理的消息都放在 queue 中 所有的消息队列连成一个链表（ next 指针），全局队列 global_queue 的 head 和 tail 存放这个链表的头指针和尾指针 消息的发送 向服务发送消息，其实就是把消息压入服务的消息队列中 700 int 701 skynet_send(struct skynet_context * context, uint32_t source, uint32_t destination , int type, int session, void * data, size_t sz ) { 702 \u003e if ((sz \u0026 MESSAGE_TYPE_MASK) != sz) { 703 \u003e \u003e skynet_error(context, \"The message to %x is too large\", destination); 704 \u003e \u003e if (type \u0026 PTYPE_TAG_DONTCOPY) { 705 \u003e \u003e \u003e skynet_free(data); 706 \u003e \u003e } 707 \u003e \u003e return -2; 708 \u003e } 709 \u003e _filter_args(context, type, \u0026session, (void **)\u0026data, \u0026sz); 710 711 \u003e if (source == 0) { 712 \u003e \u003e source = context-\u003ehandle; 713 \u003e } 714 715 \u003e if (destination == 0) { 716 \u003e \u003e if (data) { 717 \u003e \u003e \u003e skynet_error(context, \"Destination address can't be 0\"); 718 \u003e \u003e \u003e skynet_free(data); 719 \u003e \u003e \u003e return -1; 720 \u003e \u003e } ","date":"2023-12-16","objectID":"/posts/skynet/skynet_cxt/:0:0","series":null,"tags":null,"title":"skynet的服务","uri":"/posts/skynet/skynet_cxt/#消息的处理"},{"categories":["skynet相关"],"content":"\r简析 skynet 是一个 actor 模型的框架，actor 就是 skynet 服务，对应代码中的结构体 skynet_context skynet 中存在若干个 skynet_context ，这些对象通过 handle_storage 来进行管理，根据 handle_storage 可以进行增删查操作 ","date":"2023-12-15","objectID":"/posts/skynet/skynet_hs/:0:1","series":null,"tags":null,"title":"skynet的服务管理","uri":"/posts/skynet/skynet_hs/#简析"},{"categories":["skynet相关"],"content":"\r服务管理器 // skynet-src/skynet_handle.c 19 struct handle_storage { 20 struct rwlock lock; 21 22 uint32_t harbor; 23 uint32_t handle_index; // 最后一个服务在数组中的下标 24 int slot_size; // 服务数组的size（最大容量，不足时动态扩展） 25 struct skynet_context ** slot; // 服务的指针数组 26 27 int name_cap; 28 int name_count; 29 struct handle_name *name; // 服务 name 数组，用于根据 name 查询到服务 30 }; skynet 用结构体 handle_storge 来管理其服务 服务的注册 服务的注册接口是 skynet_handle_register ，新创建的服务都是通过该接口加入到管理器中，函数返回服务在 slot 中的下标 // skynet-src/skynet_handle.c 34 uint32_t 35 skynet_handle_register(struct skynet_context *ctx) { 36 struct handle_storage *s = H; 37 38 rwlock_wlock(\u0026s-\u003elock); 39 40 for (;;) { 41 int i; 42 uint32_t handle = s-\u003ehandle_index; 43 for (i=0;i\u003cs-\u003eslot_size;i++,handle++) { 44 if (handle \u003e HANDLE_MASK) { 45 // 0 is reserved 46 handle = 1; 47 } 48 int hash = handle \u0026 (s-\u003eslot_size-1); 49 if (s-\u003eslot[hash] == NULL) { 50 s-\u003eslot[hash] = ctx; 51 s-\u003ehandle_index = handle + 1; 52 53 rwlock_wunlock(\u0026s-\u003elock); 54 55 handle |= s-\u003eharbor; 56 return handle; 57 } 58 } 59 assert((s-\u003eslot_size*2 - 1) \u003c= HANDLE_MASK); 60 struct skynet_context ** new_slot = skynet_malloc(s-\u003eslot_size * 2 * sizeof(struct skynet_context *)); 61 memset(new_slot, 0, s-\u003eslot_size * 2 * sizeof(struct skynet_context *)); 62 for (i=0;i\u003cs-\u003eslot_size;i++) { 63 if (s-\u003eslot[i]) { 64 int hash = skynet_context_handle(s-\u003eslot[i]) \u0026 (s-\u003eslot_size * 2 - 1); 65 assert(new_slot[hash] == NULL); 66 new_slot[hash] = s-\u003eslot[i]; 67 } 68 } 69 skynet_free(s-\u003eslot); 70 s-\u003eslot = new_slot; 71 s-\u003eslot_size *= 2; 72 } 73 } 如果容量足够，直接加入到 slot 中（ 43 ~ 56），并返回其在 slot 中的下标 容量不足时动态扩容为原来容量的两倍，将旧数据迁移到新数组中（ 60 ~ 71 ） 服务的查询// skynet-src/skynet_handle.c 138 struct skynet_context * 139 skynet_handle_grab(uint32_t handle) { 140 struct handle_storage *s = H; 141 struct skynet_context * result = NULL; 142 143 rwlock_rlock(\u0026s-\u003elock); 144 145 uint32_t hash = handle \u0026 (s-\u003eslot_size-1); 146 struct skynet_context * ctx = s-\u003eslot[hash]; 147 if (ctx \u0026\u0026 skynet_context_handle(ctx) == handle) { 148 result = ctx; 149 skynet_context_grab(result); 150 } 151 152 rwlock_runlock(\u0026s-\u003elock); 153 154 return result; 155 } skynet 提供根据 handle 查询到 skynet_context 的接口 skynet_handle_grab skynet 提供为服务命名的接口（ skynet_handle_namehandle ），该接口将 name 和 handle 建立对应关系，这样就可以凭借 name 查询到对应的服务（ skynet_handle_findname ），代码都在文件 skynet-src/skynet_handle.h 中，这里不多赘述 服务的调度 skynet 启用多线程来分发消息，线程最终都会调用到函数 skynet_context_message_dispatch // skynet-src/skynet_server.c 297 struct message_queue *- 298 skynet_context_message_dispatch(struct skynet_monitor *sm, struct message_queue *q, int weight) { 299 \u003e if (q == NULL) { 300 \u003e \u003e q = skynet_globalmq_pop(); 301 \u003e \u003e if (q==NULL) 302 \u003e \u003e \u003e return NULL; 303 \u003e } 304 305 \u003e uint32_t handle = skynet_mq_handle(q); 306 307 \u003e struct skynet_context * ctx = skynet_handle_grab(handle); 308 \u003e if (ctx == NULL) { 309 \u003e \u003e struct drop_t d = { handle }; 310 \u003e \u003e skynet_mq_release(q, drop_message, \u0026d); 311 \u003e \u003e return skynet_globalmq_pop(); 312 \u003e } 313 314 \u003e int i,n=1; 315 \u003e struct skynet_message msg; 316 317 \u003e for (i=0;i\u003cn;i++) { 318 \u003e \u003e if (skynet_mq_pop(q,\u0026msg)) { 319 \u003e \u003e \u003e skynet_context_release(ctx); 320 \u003e \u003e \u003e return skynet_globalmq_pop(); 321 \u003e \u003e } else if (i==0 \u0026\u0026 weight \u003e= 0) { 322 \u003e \u003e \u003e n = skynet_mq_length(q); 323 \u003e \u003e \u003e n \u003e\u003e= weight; 324 \u003e \u003e } 325 \u003e \u003e int overload = skynet_mq_overload(q); 326 \u003e \u003e if (overload) { 327 \u003e \u003e \u003e skynet_error(ctx, \"May overload, message queue length = %d\", overload); 328 \u003e \u003e } 329 330 \u003e \u003e skynet_monitor_trigger(sm, msg.source , handle); 331 332 \u003e \u003e if (ctx-\u003ecb == NULL) { 333 \u003e \u003e \u003e skynet_free(msg.data); 334 \u003e \u003e } else { 335 \u003e \u003e \u003e dispatch_message(ctx, \u0026msg); 336 \u003e \u003e } 337 338 \u003e \u003e skynet_monitor_trigger(sm, 0,0); 339 \u003e } 340 341 \u003e assert(q == ctx-\u003equeue); 342 \u003e struct message_queue *nq = skynet_globalmq_pop(); 343 \u003e if (nq) { 344 \u003e \u003e // If global mq is not empty , push q back, and return next queue (nq) 345 \u003e \u003e // Else (global mq is empty or block, don't push q back, and return q again (for next dispatch) 346 \u003e \u003e skynet_globalmq_push(q); 347 \u003e \u003e q = n","date":"2023-12-15","objectID":"/posts/skynet/skynet_hs/:0:2","series":null,"tags":null,"title":"skynet的服务管理","uri":"/posts/skynet/skynet_hs/#服务管理器"},{"categories":["skynet相关"],"content":"\r服务管理器 // skynet-src/skynet_handle.c 19 struct handle_storage { 20 struct rwlock lock; 21 22 uint32_t harbor; 23 uint32_t handle_index; // 最后一个服务在数组中的下标 24 int slot_size; // 服务数组的size（最大容量，不足时动态扩展） 25 struct skynet_context ** slot; // 服务的指针数组 26 27 int name_cap; 28 int name_count; 29 struct handle_name *name; // 服务 name 数组，用于根据 name 查询到服务 30 }; skynet 用结构体 handle_storge 来管理其服务 服务的注册 服务的注册接口是 skynet_handle_register ，新创建的服务都是通过该接口加入到管理器中，函数返回服务在 slot 中的下标 // skynet-src/skynet_handle.c 34 uint32_t 35 skynet_handle_register(struct skynet_context *ctx) { 36 struct handle_storage *s = H; 37 38 rwlock_wlock(\u0026s-\u003elock); 39 40 for (;;) { 41 int i; 42 uint32_t handle = s-\u003ehandle_index; 43 for (i=0;islot_size;i++,handle++) { 44 if (handle \u003e HANDLE_MASK) { 45 // 0 is reserved 46 handle = 1; 47 } 48 int hash = handle \u0026 (s-\u003eslot_size-1); 49 if (s-\u003eslot[hash] == NULL) { 50 s-\u003eslot[hash] = ctx; 51 s-\u003ehandle_index = handle + 1; 52 53 rwlock_wunlock(\u0026s-\u003elock); 54 55 handle |= s-\u003eharbor; 56 return handle; 57 } 58 } 59 assert((s-\u003eslot_size*2 - 1) \u003c= HANDLE_MASK); 60 struct skynet_context ** new_slot = skynet_malloc(s-\u003eslot_size * 2 * sizeof(struct skynet_context *)); 61 memset(new_slot, 0, s-\u003eslot_size * 2 * sizeof(struct skynet_context *)); 62 for (i=0;islot_size;i++) { 63 if (s-\u003eslot[i]) { 64 int hash = skynet_context_handle(s-\u003eslot[i]) \u0026 (s-\u003eslot_size * 2 - 1); 65 assert(new_slot[hash] == NULL); 66 new_slot[hash] = s-\u003eslot[i]; 67 } 68 } 69 skynet_free(s-\u003eslot); 70 s-\u003eslot = new_slot; 71 s-\u003eslot_size *= 2; 72 } 73 } 如果容量足够，直接加入到 slot 中（ 43 ~ 56），并返回其在 slot 中的下标 容量不足时动态扩容为原来容量的两倍，将旧数据迁移到新数组中（ 60 ~ 71 ） 服务的查询// skynet-src/skynet_handle.c 138 struct skynet_context * 139 skynet_handle_grab(uint32_t handle) { 140 struct handle_storage *s = H; 141 struct skynet_context * result = NULL; 142 143 rwlock_rlock(\u0026s-\u003elock); 144 145 uint32_t hash = handle \u0026 (s-\u003eslot_size-1); 146 struct skynet_context * ctx = s-\u003eslot[hash]; 147 if (ctx \u0026\u0026 skynet_context_handle(ctx) == handle) { 148 result = ctx; 149 skynet_context_grab(result); 150 } 151 152 rwlock_runlock(\u0026s-\u003elock); 153 154 return result; 155 } skynet 提供根据 handle 查询到 skynet_context 的接口 skynet_handle_grab skynet 提供为服务命名的接口（ skynet_handle_namehandle ），该接口将 name 和 handle 建立对应关系，这样就可以凭借 name 查询到对应的服务（ skynet_handle_findname ），代码都在文件 skynet-src/skynet_handle.h 中，这里不多赘述 服务的调度 skynet 启用多线程来分发消息，线程最终都会调用到函数 skynet_context_message_dispatch // skynet-src/skynet_server.c 297 struct message_queue *- 298 skynet_context_message_dispatch(struct skynet_monitor *sm, struct message_queue *q, int weight) { 299 \u003e if (q == NULL) { 300 \u003e \u003e q = skynet_globalmq_pop(); 301 \u003e \u003e if (q==NULL) 302 \u003e \u003e \u003e return NULL; 303 \u003e } 304 305 \u003e uint32_t handle = skynet_mq_handle(q); 306 307 \u003e struct skynet_context * ctx = skynet_handle_grab(handle); 308 \u003e if (ctx == NULL) { 309 \u003e \u003e struct drop_t d = { handle }; 310 \u003e \u003e skynet_mq_release(q, drop_message, \u0026d); 311 \u003e \u003e return skynet_globalmq_pop(); 312 \u003e } 313 314 \u003e int i,n=1; 315 \u003e struct skynet_message msg; 316 317 \u003e for (i=0;i \u003e if (skynet_mq_pop(q,\u0026msg)) { 319 \u003e \u003e \u003e skynet_context_release(ctx); 320 \u003e \u003e \u003e return skynet_globalmq_pop(); 321 \u003e \u003e } else if (i==0 \u0026\u0026 weight \u003e= 0) { 322 \u003e \u003e \u003e n = skynet_mq_length(q); 323 \u003e \u003e \u003e n \u003e\u003e= weight; 324 \u003e \u003e } 325 \u003e \u003e int overload = skynet_mq_overload(q); 326 \u003e \u003e if (overload) { 327 \u003e \u003e \u003e skynet_error(ctx, \"May overload, message queue length = %d\", overload); 328 \u003e \u003e } 329 330 \u003e \u003e skynet_monitor_trigger(sm, msg.source , handle); 331 332 \u003e \u003e if (ctx-\u003ecb == NULL) { 333 \u003e \u003e \u003e skynet_free(msg.data); 334 \u003e \u003e } else { 335 \u003e \u003e \u003e dispatch_message(ctx, \u0026msg); 336 \u003e \u003e } 337 338 \u003e \u003e skynet_monitor_trigger(sm, 0,0); 339 \u003e } 340 341 \u003e assert(q == ctx-\u003equeue); 342 \u003e struct message_queue *nq = skynet_globalmq_pop(); 343 \u003e if (nq) { 344 \u003e \u003e // If global mq is not empty , push q back, and return next queue (nq) 345 \u003e \u003e // Else (global mq is empty or block, don't push q back, and return q again (for next dispatch) 346 \u003e \u003e skynet_globalmq_push(q); 347 \u003e \u003e q = n","date":"2023-12-15","objectID":"/posts/skynet/skynet_hs/:0:2","series":null,"tags":null,"title":"skynet的服务管理","uri":"/posts/skynet/skynet_hs/#服务的注册"},{"categories":["skynet相关"],"content":"\r服务管理器 // skynet-src/skynet_handle.c 19 struct handle_storage { 20 struct rwlock lock; 21 22 uint32_t harbor; 23 uint32_t handle_index; // 最后一个服务在数组中的下标 24 int slot_size; // 服务数组的size（最大容量，不足时动态扩展） 25 struct skynet_context ** slot; // 服务的指针数组 26 27 int name_cap; 28 int name_count; 29 struct handle_name *name; // 服务 name 数组，用于根据 name 查询到服务 30 }; skynet 用结构体 handle_storge 来管理其服务 服务的注册 服务的注册接口是 skynet_handle_register ，新创建的服务都是通过该接口加入到管理器中，函数返回服务在 slot 中的下标 // skynet-src/skynet_handle.c 34 uint32_t 35 skynet_handle_register(struct skynet_context *ctx) { 36 struct handle_storage *s = H; 37 38 rwlock_wlock(\u0026s-\u003elock); 39 40 for (;;) { 41 int i; 42 uint32_t handle = s-\u003ehandle_index; 43 for (i=0;islot_size;i++,handle++) { 44 if (handle \u003e HANDLE_MASK) { 45 // 0 is reserved 46 handle = 1; 47 } 48 int hash = handle \u0026 (s-\u003eslot_size-1); 49 if (s-\u003eslot[hash] == NULL) { 50 s-\u003eslot[hash] = ctx; 51 s-\u003ehandle_index = handle + 1; 52 53 rwlock_wunlock(\u0026s-\u003elock); 54 55 handle |= s-\u003eharbor; 56 return handle; 57 } 58 } 59 assert((s-\u003eslot_size*2 - 1) \u003c= HANDLE_MASK); 60 struct skynet_context ** new_slot = skynet_malloc(s-\u003eslot_size * 2 * sizeof(struct skynet_context *)); 61 memset(new_slot, 0, s-\u003eslot_size * 2 * sizeof(struct skynet_context *)); 62 for (i=0;islot_size;i++) { 63 if (s-\u003eslot[i]) { 64 int hash = skynet_context_handle(s-\u003eslot[i]) \u0026 (s-\u003eslot_size * 2 - 1); 65 assert(new_slot[hash] == NULL); 66 new_slot[hash] = s-\u003eslot[i]; 67 } 68 } 69 skynet_free(s-\u003eslot); 70 s-\u003eslot = new_slot; 71 s-\u003eslot_size *= 2; 72 } 73 } 如果容量足够，直接加入到 slot 中（ 43 ~ 56），并返回其在 slot 中的下标 容量不足时动态扩容为原来容量的两倍，将旧数据迁移到新数组中（ 60 ~ 71 ） 服务的查询// skynet-src/skynet_handle.c 138 struct skynet_context * 139 skynet_handle_grab(uint32_t handle) { 140 struct handle_storage *s = H; 141 struct skynet_context * result = NULL; 142 143 rwlock_rlock(\u0026s-\u003elock); 144 145 uint32_t hash = handle \u0026 (s-\u003eslot_size-1); 146 struct skynet_context * ctx = s-\u003eslot[hash]; 147 if (ctx \u0026\u0026 skynet_context_handle(ctx) == handle) { 148 result = ctx; 149 skynet_context_grab(result); 150 } 151 152 rwlock_runlock(\u0026s-\u003elock); 153 154 return result; 155 } skynet 提供根据 handle 查询到 skynet_context 的接口 skynet_handle_grab skynet 提供为服务命名的接口（ skynet_handle_namehandle ），该接口将 name 和 handle 建立对应关系，这样就可以凭借 name 查询到对应的服务（ skynet_handle_findname ），代码都在文件 skynet-src/skynet_handle.h 中，这里不多赘述 服务的调度 skynet 启用多线程来分发消息，线程最终都会调用到函数 skynet_context_message_dispatch // skynet-src/skynet_server.c 297 struct message_queue *- 298 skynet_context_message_dispatch(struct skynet_monitor *sm, struct message_queue *q, int weight) { 299 \u003e if (q == NULL) { 300 \u003e \u003e q = skynet_globalmq_pop(); 301 \u003e \u003e if (q==NULL) 302 \u003e \u003e \u003e return NULL; 303 \u003e } 304 305 \u003e uint32_t handle = skynet_mq_handle(q); 306 307 \u003e struct skynet_context * ctx = skynet_handle_grab(handle); 308 \u003e if (ctx == NULL) { 309 \u003e \u003e struct drop_t d = { handle }; 310 \u003e \u003e skynet_mq_release(q, drop_message, \u0026d); 311 \u003e \u003e return skynet_globalmq_pop(); 312 \u003e } 313 314 \u003e int i,n=1; 315 \u003e struct skynet_message msg; 316 317 \u003e for (i=0;i \u003e if (skynet_mq_pop(q,\u0026msg)) { 319 \u003e \u003e \u003e skynet_context_release(ctx); 320 \u003e \u003e \u003e return skynet_globalmq_pop(); 321 \u003e \u003e } else if (i==0 \u0026\u0026 weight \u003e= 0) { 322 \u003e \u003e \u003e n = skynet_mq_length(q); 323 \u003e \u003e \u003e n \u003e\u003e= weight; 324 \u003e \u003e } 325 \u003e \u003e int overload = skynet_mq_overload(q); 326 \u003e \u003e if (overload) { 327 \u003e \u003e \u003e skynet_error(ctx, \"May overload, message queue length = %d\", overload); 328 \u003e \u003e } 329 330 \u003e \u003e skynet_monitor_trigger(sm, msg.source , handle); 331 332 \u003e \u003e if (ctx-\u003ecb == NULL) { 333 \u003e \u003e \u003e skynet_free(msg.data); 334 \u003e \u003e } else { 335 \u003e \u003e \u003e dispatch_message(ctx, \u0026msg); 336 \u003e \u003e } 337 338 \u003e \u003e skynet_monitor_trigger(sm, 0,0); 339 \u003e } 340 341 \u003e assert(q == ctx-\u003equeue); 342 \u003e struct message_queue *nq = skynet_globalmq_pop(); 343 \u003e if (nq) { 344 \u003e \u003e // If global mq is not empty , push q back, and return next queue (nq) 345 \u003e \u003e // Else (global mq is empty or block, don't push q back, and return q again (for next dispatch) 346 \u003e \u003e skynet_globalmq_push(q); 347 \u003e \u003e q = n","date":"2023-12-15","objectID":"/posts/skynet/skynet_hs/:0:2","series":null,"tags":null,"title":"skynet的服务管理","uri":"/posts/skynet/skynet_hs/#服务的查询"},{"categories":["skynet相关"],"content":"\r服务管理器 // skynet-src/skynet_handle.c 19 struct handle_storage { 20 struct rwlock lock; 21 22 uint32_t harbor; 23 uint32_t handle_index; // 最后一个服务在数组中的下标 24 int slot_size; // 服务数组的size（最大容量，不足时动态扩展） 25 struct skynet_context ** slot; // 服务的指针数组 26 27 int name_cap; 28 int name_count; 29 struct handle_name *name; // 服务 name 数组，用于根据 name 查询到服务 30 }; skynet 用结构体 handle_storge 来管理其服务 服务的注册 服务的注册接口是 skynet_handle_register ，新创建的服务都是通过该接口加入到管理器中，函数返回服务在 slot 中的下标 // skynet-src/skynet_handle.c 34 uint32_t 35 skynet_handle_register(struct skynet_context *ctx) { 36 struct handle_storage *s = H; 37 38 rwlock_wlock(\u0026s-\u003elock); 39 40 for (;;) { 41 int i; 42 uint32_t handle = s-\u003ehandle_index; 43 for (i=0;islot_size;i++,handle++) { 44 if (handle \u003e HANDLE_MASK) { 45 // 0 is reserved 46 handle = 1; 47 } 48 int hash = handle \u0026 (s-\u003eslot_size-1); 49 if (s-\u003eslot[hash] == NULL) { 50 s-\u003eslot[hash] = ctx; 51 s-\u003ehandle_index = handle + 1; 52 53 rwlock_wunlock(\u0026s-\u003elock); 54 55 handle |= s-\u003eharbor; 56 return handle; 57 } 58 } 59 assert((s-\u003eslot_size*2 - 1) \u003c= HANDLE_MASK); 60 struct skynet_context ** new_slot = skynet_malloc(s-\u003eslot_size * 2 * sizeof(struct skynet_context *)); 61 memset(new_slot, 0, s-\u003eslot_size * 2 * sizeof(struct skynet_context *)); 62 for (i=0;islot_size;i++) { 63 if (s-\u003eslot[i]) { 64 int hash = skynet_context_handle(s-\u003eslot[i]) \u0026 (s-\u003eslot_size * 2 - 1); 65 assert(new_slot[hash] == NULL); 66 new_slot[hash] = s-\u003eslot[i]; 67 } 68 } 69 skynet_free(s-\u003eslot); 70 s-\u003eslot = new_slot; 71 s-\u003eslot_size *= 2; 72 } 73 } 如果容量足够，直接加入到 slot 中（ 43 ~ 56），并返回其在 slot 中的下标 容量不足时动态扩容为原来容量的两倍，将旧数据迁移到新数组中（ 60 ~ 71 ） 服务的查询// skynet-src/skynet_handle.c 138 struct skynet_context * 139 skynet_handle_grab(uint32_t handle) { 140 struct handle_storage *s = H; 141 struct skynet_context * result = NULL; 142 143 rwlock_rlock(\u0026s-\u003elock); 144 145 uint32_t hash = handle \u0026 (s-\u003eslot_size-1); 146 struct skynet_context * ctx = s-\u003eslot[hash]; 147 if (ctx \u0026\u0026 skynet_context_handle(ctx) == handle) { 148 result = ctx; 149 skynet_context_grab(result); 150 } 151 152 rwlock_runlock(\u0026s-\u003elock); 153 154 return result; 155 } skynet 提供根据 handle 查询到 skynet_context 的接口 skynet_handle_grab skynet 提供为服务命名的接口（ skynet_handle_namehandle ），该接口将 name 和 handle 建立对应关系，这样就可以凭借 name 查询到对应的服务（ skynet_handle_findname ），代码都在文件 skynet-src/skynet_handle.h 中，这里不多赘述 服务的调度 skynet 启用多线程来分发消息，线程最终都会调用到函数 skynet_context_message_dispatch // skynet-src/skynet_server.c 297 struct message_queue *- 298 skynet_context_message_dispatch(struct skynet_monitor *sm, struct message_queue *q, int weight) { 299 \u003e if (q == NULL) { 300 \u003e \u003e q = skynet_globalmq_pop(); 301 \u003e \u003e if (q==NULL) 302 \u003e \u003e \u003e return NULL; 303 \u003e } 304 305 \u003e uint32_t handle = skynet_mq_handle(q); 306 307 \u003e struct skynet_context * ctx = skynet_handle_grab(handle); 308 \u003e if (ctx == NULL) { 309 \u003e \u003e struct drop_t d = { handle }; 310 \u003e \u003e skynet_mq_release(q, drop_message, \u0026d); 311 \u003e \u003e return skynet_globalmq_pop(); 312 \u003e } 313 314 \u003e int i,n=1; 315 \u003e struct skynet_message msg; 316 317 \u003e for (i=0;i \u003e if (skynet_mq_pop(q,\u0026msg)) { 319 \u003e \u003e \u003e skynet_context_release(ctx); 320 \u003e \u003e \u003e return skynet_globalmq_pop(); 321 \u003e \u003e } else if (i==0 \u0026\u0026 weight \u003e= 0) { 322 \u003e \u003e \u003e n = skynet_mq_length(q); 323 \u003e \u003e \u003e n \u003e\u003e= weight; 324 \u003e \u003e } 325 \u003e \u003e int overload = skynet_mq_overload(q); 326 \u003e \u003e if (overload) { 327 \u003e \u003e \u003e skynet_error(ctx, \"May overload, message queue length = %d\", overload); 328 \u003e \u003e } 329 330 \u003e \u003e skynet_monitor_trigger(sm, msg.source , handle); 331 332 \u003e \u003e if (ctx-\u003ecb == NULL) { 333 \u003e \u003e \u003e skynet_free(msg.data); 334 \u003e \u003e } else { 335 \u003e \u003e \u003e dispatch_message(ctx, \u0026msg); 336 \u003e \u003e } 337 338 \u003e \u003e skynet_monitor_trigger(sm, 0,0); 339 \u003e } 340 341 \u003e assert(q == ctx-\u003equeue); 342 \u003e struct message_queue *nq = skynet_globalmq_pop(); 343 \u003e if (nq) { 344 \u003e \u003e // If global mq is not empty , push q back, and return next queue (nq) 345 \u003e \u003e // Else (global mq is empty or block, don't push q back, and return q again (for next dispatch) 346 \u003e \u003e skynet_globalmq_push(q); 347 \u003e \u003e q = n","date":"2023-12-15","objectID":"/posts/skynet/skynet_hs/:0:2","series":null,"tags":null,"title":"skynet的服务管理","uri":"/posts/skynet/skynet_hs/#服务的调度"},{"categories":["skynet相关"],"content":"\r服务管理器 // skynet-src/skynet_handle.c 19 struct handle_storage { 20 struct rwlock lock; 21 22 uint32_t harbor; 23 uint32_t handle_index; // 最后一个服务在数组中的下标 24 int slot_size; // 服务数组的size（最大容量，不足时动态扩展） 25 struct skynet_context ** slot; // 服务的指针数组 26 27 int name_cap; 28 int name_count; 29 struct handle_name *name; // 服务 name 数组，用于根据 name 查询到服务 30 }; skynet 用结构体 handle_storge 来管理其服务 服务的注册 服务的注册接口是 skynet_handle_register ，新创建的服务都是通过该接口加入到管理器中，函数返回服务在 slot 中的下标 // skynet-src/skynet_handle.c 34 uint32_t 35 skynet_handle_register(struct skynet_context *ctx) { 36 struct handle_storage *s = H; 37 38 rwlock_wlock(\u0026s-\u003elock); 39 40 for (;;) { 41 int i; 42 uint32_t handle = s-\u003ehandle_index; 43 for (i=0;islot_size;i++,handle++) { 44 if (handle \u003e HANDLE_MASK) { 45 // 0 is reserved 46 handle = 1; 47 } 48 int hash = handle \u0026 (s-\u003eslot_size-1); 49 if (s-\u003eslot[hash] == NULL) { 50 s-\u003eslot[hash] = ctx; 51 s-\u003ehandle_index = handle + 1; 52 53 rwlock_wunlock(\u0026s-\u003elock); 54 55 handle |= s-\u003eharbor; 56 return handle; 57 } 58 } 59 assert((s-\u003eslot_size*2 - 1) \u003c= HANDLE_MASK); 60 struct skynet_context ** new_slot = skynet_malloc(s-\u003eslot_size * 2 * sizeof(struct skynet_context *)); 61 memset(new_slot, 0, s-\u003eslot_size * 2 * sizeof(struct skynet_context *)); 62 for (i=0;islot_size;i++) { 63 if (s-\u003eslot[i]) { 64 int hash = skynet_context_handle(s-\u003eslot[i]) \u0026 (s-\u003eslot_size * 2 - 1); 65 assert(new_slot[hash] == NULL); 66 new_slot[hash] = s-\u003eslot[i]; 67 } 68 } 69 skynet_free(s-\u003eslot); 70 s-\u003eslot = new_slot; 71 s-\u003eslot_size *= 2; 72 } 73 } 如果容量足够，直接加入到 slot 中（ 43 ~ 56），并返回其在 slot 中的下标 容量不足时动态扩容为原来容量的两倍，将旧数据迁移到新数组中（ 60 ~ 71 ） 服务的查询// skynet-src/skynet_handle.c 138 struct skynet_context * 139 skynet_handle_grab(uint32_t handle) { 140 struct handle_storage *s = H; 141 struct skynet_context * result = NULL; 142 143 rwlock_rlock(\u0026s-\u003elock); 144 145 uint32_t hash = handle \u0026 (s-\u003eslot_size-1); 146 struct skynet_context * ctx = s-\u003eslot[hash]; 147 if (ctx \u0026\u0026 skynet_context_handle(ctx) == handle) { 148 result = ctx; 149 skynet_context_grab(result); 150 } 151 152 rwlock_runlock(\u0026s-\u003elock); 153 154 return result; 155 } skynet 提供根据 handle 查询到 skynet_context 的接口 skynet_handle_grab skynet 提供为服务命名的接口（ skynet_handle_namehandle ），该接口将 name 和 handle 建立对应关系，这样就可以凭借 name 查询到对应的服务（ skynet_handle_findname ），代码都在文件 skynet-src/skynet_handle.h 中，这里不多赘述 服务的调度 skynet 启用多线程来分发消息，线程最终都会调用到函数 skynet_context_message_dispatch // skynet-src/skynet_server.c 297 struct message_queue *- 298 skynet_context_message_dispatch(struct skynet_monitor *sm, struct message_queue *q, int weight) { 299 \u003e if (q == NULL) { 300 \u003e \u003e q = skynet_globalmq_pop(); 301 \u003e \u003e if (q==NULL) 302 \u003e \u003e \u003e return NULL; 303 \u003e } 304 305 \u003e uint32_t handle = skynet_mq_handle(q); 306 307 \u003e struct skynet_context * ctx = skynet_handle_grab(handle); 308 \u003e if (ctx == NULL) { 309 \u003e \u003e struct drop_t d = { handle }; 310 \u003e \u003e skynet_mq_release(q, drop_message, \u0026d); 311 \u003e \u003e return skynet_globalmq_pop(); 312 \u003e } 313 314 \u003e int i,n=1; 315 \u003e struct skynet_message msg; 316 317 \u003e for (i=0;i \u003e if (skynet_mq_pop(q,\u0026msg)) { 319 \u003e \u003e \u003e skynet_context_release(ctx); 320 \u003e \u003e \u003e return skynet_globalmq_pop(); 321 \u003e \u003e } else if (i==0 \u0026\u0026 weight \u003e= 0) { 322 \u003e \u003e \u003e n = skynet_mq_length(q); 323 \u003e \u003e \u003e n \u003e\u003e= weight; 324 \u003e \u003e } 325 \u003e \u003e int overload = skynet_mq_overload(q); 326 \u003e \u003e if (overload) { 327 \u003e \u003e \u003e skynet_error(ctx, \"May overload, message queue length = %d\", overload); 328 \u003e \u003e } 329 330 \u003e \u003e skynet_monitor_trigger(sm, msg.source , handle); 331 332 \u003e \u003e if (ctx-\u003ecb == NULL) { 333 \u003e \u003e \u003e skynet_free(msg.data); 334 \u003e \u003e } else { 335 \u003e \u003e \u003e dispatch_message(ctx, \u0026msg); 336 \u003e \u003e } 337 338 \u003e \u003e skynet_monitor_trigger(sm, 0,0); 339 \u003e } 340 341 \u003e assert(q == ctx-\u003equeue); 342 \u003e struct message_queue *nq = skynet_globalmq_pop(); 343 \u003e if (nq) { 344 \u003e \u003e // If global mq is not empty , push q back, and return next queue (nq) 345 \u003e \u003e // Else (global mq is empty or block, don't push q back, and return q again (for next dispatch) 346 \u003e \u003e skynet_globalmq_push(q); 347 \u003e \u003e q = n","date":"2023-12-15","objectID":"/posts/skynet/skynet_hs/:0:2","series":null,"tags":null,"title":"skynet的服务管理","uri":"/posts/skynet/skynet_hs/#服务的关闭"},{"categories":["skynet相关"],"content":"\r简析 skyent 用 5 个链表数组来存储定时事件，分别是 near[256] 和 t[4]，其中 near 数组用来存放即将到期的定时事件，t[0] ~ t[3] 数组分别存放不同时间段的定时事件 插入时，定时器根据过期时间 expire 和当前时间 now 的相对时间，将事件放到对应的链表当中 tick的时候，定时器直接取 near 中的到期事件触发 每过一定时间，定时器将 t[0] ~ t[3] 中的事件重新分配到合适的位置 主要结构 39 \u003e struct timer { 40 \u003e struct link_list near[TIME_NEAR]; //到期时间较近定时器事件链表 41 \u003e struct link_list t[4][TIME_LEVEL]; //定时器事件链表 42 \u003e struct spinlock lock; //自旋锁 43 \u003e uint32_t time; //启动后tick的时间 44 \u003e uint32_t starttime; //启动时的系统时间 45 \u003e uint64_t current; //skynet自己计算出的实际时间 46 \u003e uint64_t current_point; //系统启动后的绝对时间 47 }; skynet定时器的基本思想就是把到期时间较近的事件放到near数组中，其余的放到t数值中，每次tick后就从near数组中找到到期的事件触发 tick函数 165 static void- 166 timer_update(struct timer *T) { 167 \u003e SPIN_LOCK(T); 168 169 \u003e // try to dispatch timeout 0 (rare condition) 170 \u003e timer_execute(T); 171 172 \u003e // shift time first, and then dispatch timer message 173 \u003e timer_shift(T); 174 175 \u003e timer_execute(T); 176 177 \u003e SPIN_UNLOCK(T); 178 } 245 void 246 skynet_updatetime(void) { 247 \u003e uint64_t cp = gettime(); 248 \u003e if(cp \u003c TI-\u003ecurrent_point) { 249 \u003e \u003e skynet_error(NULL, \"time diff error: change from %lld to %lld\", cp, TI-\u003ecurrent_point); 250 \u003e \u003e TI-\u003ecurrent_point = cp; 251 \u003e } else if (cp != TI-\u003ecurrent_point) { 252 \u003e \u003e uint32_t diff = (uint32_t)(cp - TI-\u003ecurrent_point); 253 \u003e \u003e TI-\u003ecurrent_point = cp; 254 \u003e \u003e TI-\u003ecurrent += diff; 255 \u003e \u003e int i; 256 \u003e \u003e for (i=0;i\u003cdiff;i++) { 257 \u003e \u003e \u003e timer_update(TI); 258 \u003e \u003e } 259 \u003e } 260 } skynet的timer线程每2500微秒（即2.5毫秒）执行一次skynet_updatetime 主要流程 165 static void- 166 timer_update(struct timer *T) { 167 \u003e SPIN_LOCK(T); 168 169 \u003e // try to dispatch timeout 0 (rare condition) 170 \u003e timer_execute(T); // 触发定时器 171 172 \u003e // shift time first, and then dispatch timer message 173 \u003e timer_shift(T); // 过期时间再分配 174 175 \u003e timer_execute(T); // 触发定时器 176 177 \u003e SPIN_UNLOCK(T); 178 } 这里执行了两次timer_execute，按理来说，第一次timer_execute并不是必须的，因为注册过期时间为现在的事件是直接分发事件而不进入到定时器中。对于为啥执行两次的解释，作者希望timer 不需要关注 skynet 的用法实现，即不管skynet有没有处理time==0的情况，timer都不会因此出现bug 定时器的插入 67 static void 68 add_node(struct timer *T,struct timer_node *node) { 69 \u003e uint32_t time=node-\u003eexpire; // 过期时间 70 \u003e uint32_t current_time=T-\u003etime; // 当前时间 71 \u003e 72 \u003e if ((time|TIME_NEAR_MASK)==(current_time|TIME_NEAR_MASK)) { // 将较近的放到near数组中 73 \u003e \u003e link(\u0026T-\u003enear[time\u0026TIME_NEAR_MASK],node); 74 \u003e } else { 75 \u003e \u003e int i; 76 \u003e \u003e uint32_t mask=TIME_NEAR \u003c\u003c TIME_LEVEL_SHIFT; 77 \u003e \u003e for (i=0;i\u003c3;i++) { 78 \u003e \u003e \u003e if ((time|(mask-1))==(current_time|(mask-1))) { 79 \u003e \u003e \u003e \u003e break; 80 \u003e \u003e \u003e } 81 \u003e \u003e \u003e mask \u003c\u003c= TIME_LEVEL_SHIFT; 82 \u003e \u003e } 83 84 \u003e \u003e link(\u0026T-\u003et[i][((time\u003e\u003e(TIME_NEAR_SHIFT + i*TIME_LEVEL_SHIFT)) \u0026 TIME_LEVEL_MASK)],node);\u003e // 根据过期时间的长短放到 t 中 85 \u003e } 86 } 分析： 将和 current_time 高 24 位相同的 time 放到 near[x] 中，x 为 time 的第 0 ~ 7 位表示的十进制值（如 time 的第 0 ~ 7 位为 00000010时，放到 near[2] 中） 将和 current_time 高 18 位相同的 time 放到 t[0][x]中，x 为 time 的第 8 ~ 14 位表示的十进制值（如 time 的第 8 ~ 14 位为 000010时，放到 t[0][2] 中） 将和 current_time 高 12 位相同的 time 放到 t[1][x]中，x 为 time 的第 15 ~ 21 位表示的十进制值（如 time 的第 15 ~ 21 位为 000010时，放到 t[1][2] 中） 将和 current_time 高 6 位相同的 time 放到 t[2][x]中，x 为 time 的第 22 ~ 27 位表示的十进制值（如 time 的第 22 ~ 27 位为 000010时，放到 t[2][2] 中） 将其余的 time 放到 t[3][x] 中，x 为 time 的第 28 ~ 32 位表示的十进制值（如 time 的第 28 ~ 32 位为 000010时，放到 t[3][2] 中） 定时器的执行 152 static inline void 153 timer_execute(struct timer *T) { 154 \u003e int idx = T-\u003etime \u0026 TIME_NEAR_MASK; 155 \u003e 156 \u003e while (T-\u003enear[idx].head.next) { 157 \u003e \u003e struct timer_node *current = link_clear(\u0026T-\u003enear[idx]); 158 \u003e \u003e SPIN_UNLOCK(T); 159 \u003e \u003e // dispatch_list don't need lock T 160 \u003e \u003e dispatch_list(current); 161 \u003e \u003e SPIN_LOCK(T); 162 \u003e } 163 } 因为最近的事件都在near上，只需要根据idx就能找到到期的事件 过期事件的再分配 101 static void 102 move_list(struct timer *T, int level, int idx) { 103 \u003e struct timer_node *current = link_clear(\u0026T-\u003et[level][idx]); 104 \u003e while (current) { 105 \u003e \u003e struct timer_node *temp=current-\u003enext; 106 \u003e \u003e add_node(T,current); 107 \u003e \u003e current=temp; 108 \u003e } 109 } 111 static void 112 timer_shift(struct timer *T) { 113 \u003e int mask = TIME_NEAR; 114 \u003e uint32_t ct = ++T-\u003etime; 115 \u003e if (ct == 0) { 116 \u003e \u003e move_list(T, ","date":"2023-11-14","objectID":"/posts/skynet/skynet_timer/:0:0","series":null,"tags":null,"title":"skynet的定时器","uri":"/posts/skynet/skynet_timer/#"},{"categories":["skynet相关"],"content":"\r简析 skyent 用 5 个链表数组来存储定时事件，分别是 near[256] 和 t[4]，其中 near 数组用来存放即将到期的定时事件，t[0] ~ t[3] 数组分别存放不同时间段的定时事件 插入时，定时器根据过期时间 expire 和当前时间 now 的相对时间，将事件放到对应的链表当中 tick的时候，定时器直接取 near 中的到期事件触发 每过一定时间，定时器将 t[0] ~ t[3] 中的事件重新分配到合适的位置 主要结构 39 \u003e struct timer { 40 \u003e struct link_list near[TIME_NEAR]; //到期时间较近定时器事件链表 41 \u003e struct link_list t[4][TIME_LEVEL]; //定时器事件链表 42 \u003e struct spinlock lock; //自旋锁 43 \u003e uint32_t time; //启动后tick的时间 44 \u003e uint32_t starttime; //启动时的系统时间 45 \u003e uint64_t current; //skynet自己计算出的实际时间 46 \u003e uint64_t current_point; //系统启动后的绝对时间 47 }; skynet定时器的基本思想就是把到期时间较近的事件放到near数组中，其余的放到t数值中，每次tick后就从near数组中找到到期的事件触发 tick函数 165 static void- 166 timer_update(struct timer *T) { 167 \u003e SPIN_LOCK(T); 168 169 \u003e // try to dispatch timeout 0 (rare condition) 170 \u003e timer_execute(T); 171 172 \u003e // shift time first, and then dispatch timer message 173 \u003e timer_shift(T); 174 175 \u003e timer_execute(T); 176 177 \u003e SPIN_UNLOCK(T); 178 } 245 void 246 skynet_updatetime(void) { 247 \u003e uint64_t cp = gettime(); 248 \u003e if(cp \u003c TI-\u003ecurrent_point) { 249 \u003e \u003e skynet_error(NULL, \"time diff error: change from %lld to %lld\", cp, TI-\u003ecurrent_point); 250 \u003e \u003e TI-\u003ecurrent_point = cp; 251 \u003e } else if (cp != TI-\u003ecurrent_point) { 252 \u003e \u003e uint32_t diff = (uint32_t)(cp - TI-\u003ecurrent_point); 253 \u003e \u003e TI-\u003ecurrent_point = cp; 254 \u003e \u003e TI-\u003ecurrent += diff; 255 \u003e \u003e int i; 256 \u003e \u003e for (i=0;i \u003e \u003e timer_update(TI); 258 \u003e \u003e } 259 \u003e } 260 } skynet的timer线程每2500微秒（即2.5毫秒）执行一次skynet_updatetime 主要流程 165 static void- 166 timer_update(struct timer *T) { 167 \u003e SPIN_LOCK(T); 168 169 \u003e // try to dispatch timeout 0 (rare condition) 170 \u003e timer_execute(T); // 触发定时器 171 172 \u003e // shift time first, and then dispatch timer message 173 \u003e timer_shift(T); // 过期时间再分配 174 175 \u003e timer_execute(T); // 触发定时器 176 177 \u003e SPIN_UNLOCK(T); 178 } 这里执行了两次timer_execute，按理来说，第一次timer_execute并不是必须的，因为注册过期时间为现在的事件是直接分发事件而不进入到定时器中。对于为啥执行两次的解释，作者希望timer 不需要关注 skynet 的用法实现，即不管skynet有没有处理time==0的情况，timer都不会因此出现bug 定时器的插入 67 static void 68 add_node(struct timer *T,struct timer_node *node) { 69 \u003e uint32_t time=node-\u003eexpire; // 过期时间 70 \u003e uint32_t current_time=T-\u003etime; // 当前时间 71 \u003e 72 \u003e if ((time|TIME_NEAR_MASK)==(current_time|TIME_NEAR_MASK)) { // 将较近的放到near数组中 73 \u003e \u003e link(\u0026T-\u003enear[time\u0026TIME_NEAR_MASK],node); 74 \u003e } else { 75 \u003e \u003e int i; 76 \u003e \u003e uint32_t mask=TIME_NEAR \u003c\u003c TIME_LEVEL_SHIFT; 77 \u003e \u003e for (i=0;i\u003c3;i++) { 78 \u003e \u003e \u003e if ((time|(mask-1))==(current_time|(mask-1))) { 79 \u003e \u003e \u003e \u003e break; 80 \u003e \u003e \u003e } 81 \u003e \u003e \u003e mask \u003c\u003c= TIME_LEVEL_SHIFT; 82 \u003e \u003e } 83 84 \u003e \u003e link(\u0026T-\u003et[i][((time\u003e\u003e(TIME_NEAR_SHIFT + i*TIME_LEVEL_SHIFT)) \u0026 TIME_LEVEL_MASK)],node);\u003e // 根据过期时间的长短放到 t 中 85 \u003e } 86 } 分析： 将和 current_time 高 24 位相同的 time 放到 near[x] 中，x 为 time 的第 0 ~ 7 位表示的十进制值（如 time 的第 0 ~ 7 位为 00000010时，放到 near[2] 中） 将和 current_time 高 18 位相同的 time 放到 t[0][x]中，x 为 time 的第 8 ~ 14 位表示的十进制值（如 time 的第 8 ~ 14 位为 000010时，放到 t[0][2] 中） 将和 current_time 高 12 位相同的 time 放到 t[1][x]中，x 为 time 的第 15 ~ 21 位表示的十进制值（如 time 的第 15 ~ 21 位为 000010时，放到 t[1][2] 中） 将和 current_time 高 6 位相同的 time 放到 t[2][x]中，x 为 time 的第 22 ~ 27 位表示的十进制值（如 time 的第 22 ~ 27 位为 000010时，放到 t[2][2] 中） 将其余的 time 放到 t[3][x] 中，x 为 time 的第 28 ~ 32 位表示的十进制值（如 time 的第 28 ~ 32 位为 000010时，放到 t[3][2] 中） 定时器的执行 152 static inline void 153 timer_execute(struct timer *T) { 154 \u003e int idx = T-\u003etime \u0026 TIME_NEAR_MASK; 155 \u003e 156 \u003e while (T-\u003enear[idx].head.next) { 157 \u003e \u003e struct timer_node *current = link_clear(\u0026T-\u003enear[idx]); 158 \u003e \u003e SPIN_UNLOCK(T); 159 \u003e \u003e // dispatch_list don't need lock T 160 \u003e \u003e dispatch_list(current); 161 \u003e \u003e SPIN_LOCK(T); 162 \u003e } 163 } 因为最近的事件都在near上，只需要根据idx就能找到到期的事件 过期事件的再分配 101 static void 102 move_list(struct timer *T, int level, int idx) { 103 \u003e struct timer_node *current = link_clear(\u0026T-\u003et[level][idx]); 104 \u003e while (current) { 105 \u003e \u003e struct timer_node *temp=current-\u003enext; 106 \u003e \u003e add_node(T,current); 107 \u003e \u003e current=temp; 108 \u003e } 109 } 111 static void 112 timer_shift(struct timer *T) { 113 \u003e int mask = TIME_NEAR; 114 \u003e uint32_t ct = ++T-\u003etime; 115 \u003e if (ct == 0) { 116 \u003e \u003e move_list(T, ","date":"2023-11-14","objectID":"/posts/skynet/skynet_timer/:0:0","series":null,"tags":null,"title":"skynet的定时器","uri":"/posts/skynet/skynet_timer/#简析"},{"categories":["skynet相关"],"content":"\r简析 skyent 用 5 个链表数组来存储定时事件，分别是 near[256] 和 t[4]，其中 near 数组用来存放即将到期的定时事件，t[0] ~ t[3] 数组分别存放不同时间段的定时事件 插入时，定时器根据过期时间 expire 和当前时间 now 的相对时间，将事件放到对应的链表当中 tick的时候，定时器直接取 near 中的到期事件触发 每过一定时间，定时器将 t[0] ~ t[3] 中的事件重新分配到合适的位置 主要结构 39 \u003e struct timer { 40 \u003e struct link_list near[TIME_NEAR]; //到期时间较近定时器事件链表 41 \u003e struct link_list t[4][TIME_LEVEL]; //定时器事件链表 42 \u003e struct spinlock lock; //自旋锁 43 \u003e uint32_t time; //启动后tick的时间 44 \u003e uint32_t starttime; //启动时的系统时间 45 \u003e uint64_t current; //skynet自己计算出的实际时间 46 \u003e uint64_t current_point; //系统启动后的绝对时间 47 }; skynet定时器的基本思想就是把到期时间较近的事件放到near数组中，其余的放到t数值中，每次tick后就从near数组中找到到期的事件触发 tick函数 165 static void- 166 timer_update(struct timer *T) { 167 \u003e SPIN_LOCK(T); 168 169 \u003e // try to dispatch timeout 0 (rare condition) 170 \u003e timer_execute(T); 171 172 \u003e // shift time first, and then dispatch timer message 173 \u003e timer_shift(T); 174 175 \u003e timer_execute(T); 176 177 \u003e SPIN_UNLOCK(T); 178 } 245 void 246 skynet_updatetime(void) { 247 \u003e uint64_t cp = gettime(); 248 \u003e if(cp \u003c TI-\u003ecurrent_point) { 249 \u003e \u003e skynet_error(NULL, \"time diff error: change from %lld to %lld\", cp, TI-\u003ecurrent_point); 250 \u003e \u003e TI-\u003ecurrent_point = cp; 251 \u003e } else if (cp != TI-\u003ecurrent_point) { 252 \u003e \u003e uint32_t diff = (uint32_t)(cp - TI-\u003ecurrent_point); 253 \u003e \u003e TI-\u003ecurrent_point = cp; 254 \u003e \u003e TI-\u003ecurrent += diff; 255 \u003e \u003e int i; 256 \u003e \u003e for (i=0;i \u003e \u003e timer_update(TI); 258 \u003e \u003e } 259 \u003e } 260 } skynet的timer线程每2500微秒（即2.5毫秒）执行一次skynet_updatetime 主要流程 165 static void- 166 timer_update(struct timer *T) { 167 \u003e SPIN_LOCK(T); 168 169 \u003e // try to dispatch timeout 0 (rare condition) 170 \u003e timer_execute(T); // 触发定时器 171 172 \u003e // shift time first, and then dispatch timer message 173 \u003e timer_shift(T); // 过期时间再分配 174 175 \u003e timer_execute(T); // 触发定时器 176 177 \u003e SPIN_UNLOCK(T); 178 } 这里执行了两次timer_execute，按理来说，第一次timer_execute并不是必须的，因为注册过期时间为现在的事件是直接分发事件而不进入到定时器中。对于为啥执行两次的解释，作者希望timer 不需要关注 skynet 的用法实现，即不管skynet有没有处理time==0的情况，timer都不会因此出现bug 定时器的插入 67 static void 68 add_node(struct timer *T,struct timer_node *node) { 69 \u003e uint32_t time=node-\u003eexpire; // 过期时间 70 \u003e uint32_t current_time=T-\u003etime; // 当前时间 71 \u003e 72 \u003e if ((time|TIME_NEAR_MASK)==(current_time|TIME_NEAR_MASK)) { // 将较近的放到near数组中 73 \u003e \u003e link(\u0026T-\u003enear[time\u0026TIME_NEAR_MASK],node); 74 \u003e } else { 75 \u003e \u003e int i; 76 \u003e \u003e uint32_t mask=TIME_NEAR \u003c\u003c TIME_LEVEL_SHIFT; 77 \u003e \u003e for (i=0;i\u003c3;i++) { 78 \u003e \u003e \u003e if ((time|(mask-1))==(current_time|(mask-1))) { 79 \u003e \u003e \u003e \u003e break; 80 \u003e \u003e \u003e } 81 \u003e \u003e \u003e mask \u003c\u003c= TIME_LEVEL_SHIFT; 82 \u003e \u003e } 83 84 \u003e \u003e link(\u0026T-\u003et[i][((time\u003e\u003e(TIME_NEAR_SHIFT + i*TIME_LEVEL_SHIFT)) \u0026 TIME_LEVEL_MASK)],node);\u003e // 根据过期时间的长短放到 t 中 85 \u003e } 86 } 分析： 将和 current_time 高 24 位相同的 time 放到 near[x] 中，x 为 time 的第 0 ~ 7 位表示的十进制值（如 time 的第 0 ~ 7 位为 00000010时，放到 near[2] 中） 将和 current_time 高 18 位相同的 time 放到 t[0][x]中，x 为 time 的第 8 ~ 14 位表示的十进制值（如 time 的第 8 ~ 14 位为 000010时，放到 t[0][2] 中） 将和 current_time 高 12 位相同的 time 放到 t[1][x]中，x 为 time 的第 15 ~ 21 位表示的十进制值（如 time 的第 15 ~ 21 位为 000010时，放到 t[1][2] 中） 将和 current_time 高 6 位相同的 time 放到 t[2][x]中，x 为 time 的第 22 ~ 27 位表示的十进制值（如 time 的第 22 ~ 27 位为 000010时，放到 t[2][2] 中） 将其余的 time 放到 t[3][x] 中，x 为 time 的第 28 ~ 32 位表示的十进制值（如 time 的第 28 ~ 32 位为 000010时，放到 t[3][2] 中） 定时器的执行 152 static inline void 153 timer_execute(struct timer *T) { 154 \u003e int idx = T-\u003etime \u0026 TIME_NEAR_MASK; 155 \u003e 156 \u003e while (T-\u003enear[idx].head.next) { 157 \u003e \u003e struct timer_node *current = link_clear(\u0026T-\u003enear[idx]); 158 \u003e \u003e SPIN_UNLOCK(T); 159 \u003e \u003e // dispatch_list don't need lock T 160 \u003e \u003e dispatch_list(current); 161 \u003e \u003e SPIN_LOCK(T); 162 \u003e } 163 } 因为最近的事件都在near上，只需要根据idx就能找到到期的事件 过期事件的再分配 101 static void 102 move_list(struct timer *T, int level, int idx) { 103 \u003e struct timer_node *current = link_clear(\u0026T-\u003et[level][idx]); 104 \u003e while (current) { 105 \u003e \u003e struct timer_node *temp=current-\u003enext; 106 \u003e \u003e add_node(T,current); 107 \u003e \u003e current=temp; 108 \u003e } 109 } 111 static void 112 timer_shift(struct timer *T) { 113 \u003e int mask = TIME_NEAR; 114 \u003e uint32_t ct = ++T-\u003etime; 115 \u003e if (ct == 0) { 116 \u003e \u003e move_list(T, ","date":"2023-11-14","objectID":"/posts/skynet/skynet_timer/:0:0","series":null,"tags":null,"title":"skynet的定时器","uri":"/posts/skynet/skynet_timer/#主要结构"},{"categories":["skynet相关"],"content":"\r简析 skyent 用 5 个链表数组来存储定时事件，分别是 near[256] 和 t[4]，其中 near 数组用来存放即将到期的定时事件，t[0] ~ t[3] 数组分别存放不同时间段的定时事件 插入时，定时器根据过期时间 expire 和当前时间 now 的相对时间，将事件放到对应的链表当中 tick的时候，定时器直接取 near 中的到期事件触发 每过一定时间，定时器将 t[0] ~ t[3] 中的事件重新分配到合适的位置 主要结构 39 \u003e struct timer { 40 \u003e struct link_list near[TIME_NEAR]; //到期时间较近定时器事件链表 41 \u003e struct link_list t[4][TIME_LEVEL]; //定时器事件链表 42 \u003e struct spinlock lock; //自旋锁 43 \u003e uint32_t time; //启动后tick的时间 44 \u003e uint32_t starttime; //启动时的系统时间 45 \u003e uint64_t current; //skynet自己计算出的实际时间 46 \u003e uint64_t current_point; //系统启动后的绝对时间 47 }; skynet定时器的基本思想就是把到期时间较近的事件放到near数组中，其余的放到t数值中，每次tick后就从near数组中找到到期的事件触发 tick函数 165 static void- 166 timer_update(struct timer *T) { 167 \u003e SPIN_LOCK(T); 168 169 \u003e // try to dispatch timeout 0 (rare condition) 170 \u003e timer_execute(T); 171 172 \u003e // shift time first, and then dispatch timer message 173 \u003e timer_shift(T); 174 175 \u003e timer_execute(T); 176 177 \u003e SPIN_UNLOCK(T); 178 } 245 void 246 skynet_updatetime(void) { 247 \u003e uint64_t cp = gettime(); 248 \u003e if(cp \u003c TI-\u003ecurrent_point) { 249 \u003e \u003e skynet_error(NULL, \"time diff error: change from %lld to %lld\", cp, TI-\u003ecurrent_point); 250 \u003e \u003e TI-\u003ecurrent_point = cp; 251 \u003e } else if (cp != TI-\u003ecurrent_point) { 252 \u003e \u003e uint32_t diff = (uint32_t)(cp - TI-\u003ecurrent_point); 253 \u003e \u003e TI-\u003ecurrent_point = cp; 254 \u003e \u003e TI-\u003ecurrent += diff; 255 \u003e \u003e int i; 256 \u003e \u003e for (i=0;i \u003e \u003e timer_update(TI); 258 \u003e \u003e } 259 \u003e } 260 } skynet的timer线程每2500微秒（即2.5毫秒）执行一次skynet_updatetime 主要流程 165 static void- 166 timer_update(struct timer *T) { 167 \u003e SPIN_LOCK(T); 168 169 \u003e // try to dispatch timeout 0 (rare condition) 170 \u003e timer_execute(T); // 触发定时器 171 172 \u003e // shift time first, and then dispatch timer message 173 \u003e timer_shift(T); // 过期时间再分配 174 175 \u003e timer_execute(T); // 触发定时器 176 177 \u003e SPIN_UNLOCK(T); 178 } 这里执行了两次timer_execute，按理来说，第一次timer_execute并不是必须的，因为注册过期时间为现在的事件是直接分发事件而不进入到定时器中。对于为啥执行两次的解释，作者希望timer 不需要关注 skynet 的用法实现，即不管skynet有没有处理time==0的情况，timer都不会因此出现bug 定时器的插入 67 static void 68 add_node(struct timer *T,struct timer_node *node) { 69 \u003e uint32_t time=node-\u003eexpire; // 过期时间 70 \u003e uint32_t current_time=T-\u003etime; // 当前时间 71 \u003e 72 \u003e if ((time|TIME_NEAR_MASK)==(current_time|TIME_NEAR_MASK)) { // 将较近的放到near数组中 73 \u003e \u003e link(\u0026T-\u003enear[time\u0026TIME_NEAR_MASK],node); 74 \u003e } else { 75 \u003e \u003e int i; 76 \u003e \u003e uint32_t mask=TIME_NEAR \u003c\u003c TIME_LEVEL_SHIFT; 77 \u003e \u003e for (i=0;i\u003c3;i++) { 78 \u003e \u003e \u003e if ((time|(mask-1))==(current_time|(mask-1))) { 79 \u003e \u003e \u003e \u003e break; 80 \u003e \u003e \u003e } 81 \u003e \u003e \u003e mask \u003c\u003c= TIME_LEVEL_SHIFT; 82 \u003e \u003e } 83 84 \u003e \u003e link(\u0026T-\u003et[i][((time\u003e\u003e(TIME_NEAR_SHIFT + i*TIME_LEVEL_SHIFT)) \u0026 TIME_LEVEL_MASK)],node);\u003e // 根据过期时间的长短放到 t 中 85 \u003e } 86 } 分析： 将和 current_time 高 24 位相同的 time 放到 near[x] 中，x 为 time 的第 0 ~ 7 位表示的十进制值（如 time 的第 0 ~ 7 位为 00000010时，放到 near[2] 中） 将和 current_time 高 18 位相同的 time 放到 t[0][x]中，x 为 time 的第 8 ~ 14 位表示的十进制值（如 time 的第 8 ~ 14 位为 000010时，放到 t[0][2] 中） 将和 current_time 高 12 位相同的 time 放到 t[1][x]中，x 为 time 的第 15 ~ 21 位表示的十进制值（如 time 的第 15 ~ 21 位为 000010时，放到 t[1][2] 中） 将和 current_time 高 6 位相同的 time 放到 t[2][x]中，x 为 time 的第 22 ~ 27 位表示的十进制值（如 time 的第 22 ~ 27 位为 000010时，放到 t[2][2] 中） 将其余的 time 放到 t[3][x] 中，x 为 time 的第 28 ~ 32 位表示的十进制值（如 time 的第 28 ~ 32 位为 000010时，放到 t[3][2] 中） 定时器的执行 152 static inline void 153 timer_execute(struct timer *T) { 154 \u003e int idx = T-\u003etime \u0026 TIME_NEAR_MASK; 155 \u003e 156 \u003e while (T-\u003enear[idx].head.next) { 157 \u003e \u003e struct timer_node *current = link_clear(\u0026T-\u003enear[idx]); 158 \u003e \u003e SPIN_UNLOCK(T); 159 \u003e \u003e // dispatch_list don't need lock T 160 \u003e \u003e dispatch_list(current); 161 \u003e \u003e SPIN_LOCK(T); 162 \u003e } 163 } 因为最近的事件都在near上，只需要根据idx就能找到到期的事件 过期事件的再分配 101 static void 102 move_list(struct timer *T, int level, int idx) { 103 \u003e struct timer_node *current = link_clear(\u0026T-\u003et[level][idx]); 104 \u003e while (current) { 105 \u003e \u003e struct timer_node *temp=current-\u003enext; 106 \u003e \u003e add_node(T,current); 107 \u003e \u003e current=temp; 108 \u003e } 109 } 111 static void 112 timer_shift(struct timer *T) { 113 \u003e int mask = TIME_NEAR; 114 \u003e uint32_t ct = ++T-\u003etime; 115 \u003e if (ct == 0) { 116 \u003e \u003e move_list(T, ","date":"2023-11-14","objectID":"/posts/skynet/skynet_timer/:0:0","series":null,"tags":null,"title":"skynet的定时器","uri":"/posts/skynet/skynet_timer/#tick函数"},{"categories":["skynet相关"],"content":"\r简析 skyent 用 5 个链表数组来存储定时事件，分别是 near[256] 和 t[4]，其中 near 数组用来存放即将到期的定时事件，t[0] ~ t[3] 数组分别存放不同时间段的定时事件 插入时，定时器根据过期时间 expire 和当前时间 now 的相对时间，将事件放到对应的链表当中 tick的时候，定时器直接取 near 中的到期事件触发 每过一定时间，定时器将 t[0] ~ t[3] 中的事件重新分配到合适的位置 主要结构 39 \u003e struct timer { 40 \u003e struct link_list near[TIME_NEAR]; //到期时间较近定时器事件链表 41 \u003e struct link_list t[4][TIME_LEVEL]; //定时器事件链表 42 \u003e struct spinlock lock; //自旋锁 43 \u003e uint32_t time; //启动后tick的时间 44 \u003e uint32_t starttime; //启动时的系统时间 45 \u003e uint64_t current; //skynet自己计算出的实际时间 46 \u003e uint64_t current_point; //系统启动后的绝对时间 47 }; skynet定时器的基本思想就是把到期时间较近的事件放到near数组中，其余的放到t数值中，每次tick后就从near数组中找到到期的事件触发 tick函数 165 static void- 166 timer_update(struct timer *T) { 167 \u003e SPIN_LOCK(T); 168 169 \u003e // try to dispatch timeout 0 (rare condition) 170 \u003e timer_execute(T); 171 172 \u003e // shift time first, and then dispatch timer message 173 \u003e timer_shift(T); 174 175 \u003e timer_execute(T); 176 177 \u003e SPIN_UNLOCK(T); 178 } 245 void 246 skynet_updatetime(void) { 247 \u003e uint64_t cp = gettime(); 248 \u003e if(cp \u003c TI-\u003ecurrent_point) { 249 \u003e \u003e skynet_error(NULL, \"time diff error: change from %lld to %lld\", cp, TI-\u003ecurrent_point); 250 \u003e \u003e TI-\u003ecurrent_point = cp; 251 \u003e } else if (cp != TI-\u003ecurrent_point) { 252 \u003e \u003e uint32_t diff = (uint32_t)(cp - TI-\u003ecurrent_point); 253 \u003e \u003e TI-\u003ecurrent_point = cp; 254 \u003e \u003e TI-\u003ecurrent += diff; 255 \u003e \u003e int i; 256 \u003e \u003e for (i=0;i \u003e \u003e timer_update(TI); 258 \u003e \u003e } 259 \u003e } 260 } skynet的timer线程每2500微秒（即2.5毫秒）执行一次skynet_updatetime 主要流程 165 static void- 166 timer_update(struct timer *T) { 167 \u003e SPIN_LOCK(T); 168 169 \u003e // try to dispatch timeout 0 (rare condition) 170 \u003e timer_execute(T); // 触发定时器 171 172 \u003e // shift time first, and then dispatch timer message 173 \u003e timer_shift(T); // 过期时间再分配 174 175 \u003e timer_execute(T); // 触发定时器 176 177 \u003e SPIN_UNLOCK(T); 178 } 这里执行了两次timer_execute，按理来说，第一次timer_execute并不是必须的，因为注册过期时间为现在的事件是直接分发事件而不进入到定时器中。对于为啥执行两次的解释，作者希望timer 不需要关注 skynet 的用法实现，即不管skynet有没有处理time==0的情况，timer都不会因此出现bug 定时器的插入 67 static void 68 add_node(struct timer *T,struct timer_node *node) { 69 \u003e uint32_t time=node-\u003eexpire; // 过期时间 70 \u003e uint32_t current_time=T-\u003etime; // 当前时间 71 \u003e 72 \u003e if ((time|TIME_NEAR_MASK)==(current_time|TIME_NEAR_MASK)) { // 将较近的放到near数组中 73 \u003e \u003e link(\u0026T-\u003enear[time\u0026TIME_NEAR_MASK],node); 74 \u003e } else { 75 \u003e \u003e int i; 76 \u003e \u003e uint32_t mask=TIME_NEAR \u003c\u003c TIME_LEVEL_SHIFT; 77 \u003e \u003e for (i=0;i\u003c3;i++) { 78 \u003e \u003e \u003e if ((time|(mask-1))==(current_time|(mask-1))) { 79 \u003e \u003e \u003e \u003e break; 80 \u003e \u003e \u003e } 81 \u003e \u003e \u003e mask \u003c\u003c= TIME_LEVEL_SHIFT; 82 \u003e \u003e } 83 84 \u003e \u003e link(\u0026T-\u003et[i][((time\u003e\u003e(TIME_NEAR_SHIFT + i*TIME_LEVEL_SHIFT)) \u0026 TIME_LEVEL_MASK)],node);\u003e // 根据过期时间的长短放到 t 中 85 \u003e } 86 } 分析： 将和 current_time 高 24 位相同的 time 放到 near[x] 中，x 为 time 的第 0 ~ 7 位表示的十进制值（如 time 的第 0 ~ 7 位为 00000010时，放到 near[2] 中） 将和 current_time 高 18 位相同的 time 放到 t[0][x]中，x 为 time 的第 8 ~ 14 位表示的十进制值（如 time 的第 8 ~ 14 位为 000010时，放到 t[0][2] 中） 将和 current_time 高 12 位相同的 time 放到 t[1][x]中，x 为 time 的第 15 ~ 21 位表示的十进制值（如 time 的第 15 ~ 21 位为 000010时，放到 t[1][2] 中） 将和 current_time 高 6 位相同的 time 放到 t[2][x]中，x 为 time 的第 22 ~ 27 位表示的十进制值（如 time 的第 22 ~ 27 位为 000010时，放到 t[2][2] 中） 将其余的 time 放到 t[3][x] 中，x 为 time 的第 28 ~ 32 位表示的十进制值（如 time 的第 28 ~ 32 位为 000010时，放到 t[3][2] 中） 定时器的执行 152 static inline void 153 timer_execute(struct timer *T) { 154 \u003e int idx = T-\u003etime \u0026 TIME_NEAR_MASK; 155 \u003e 156 \u003e while (T-\u003enear[idx].head.next) { 157 \u003e \u003e struct timer_node *current = link_clear(\u0026T-\u003enear[idx]); 158 \u003e \u003e SPIN_UNLOCK(T); 159 \u003e \u003e // dispatch_list don't need lock T 160 \u003e \u003e dispatch_list(current); 161 \u003e \u003e SPIN_LOCK(T); 162 \u003e } 163 } 因为最近的事件都在near上，只需要根据idx就能找到到期的事件 过期事件的再分配 101 static void 102 move_list(struct timer *T, int level, int idx) { 103 \u003e struct timer_node *current = link_clear(\u0026T-\u003et[level][idx]); 104 \u003e while (current) { 105 \u003e \u003e struct timer_node *temp=current-\u003enext; 106 \u003e \u003e add_node(T,current); 107 \u003e \u003e current=temp; 108 \u003e } 109 } 111 static void 112 timer_shift(struct timer *T) { 113 \u003e int mask = TIME_NEAR; 114 \u003e uint32_t ct = ++T-\u003etime; 115 \u003e if (ct == 0) { 116 \u003e \u003e move_list(T, ","date":"2023-11-14","objectID":"/posts/skynet/skynet_timer/:0:0","series":null,"tags":null,"title":"skynet的定时器","uri":"/posts/skynet/skynet_timer/#主要流程"},{"categories":["skynet相关"],"content":"\r简析 skyent 用 5 个链表数组来存储定时事件，分别是 near[256] 和 t[4]，其中 near 数组用来存放即将到期的定时事件，t[0] ~ t[3] 数组分别存放不同时间段的定时事件 插入时，定时器根据过期时间 expire 和当前时间 now 的相对时间，将事件放到对应的链表当中 tick的时候，定时器直接取 near 中的到期事件触发 每过一定时间，定时器将 t[0] ~ t[3] 中的事件重新分配到合适的位置 主要结构 39 \u003e struct timer { 40 \u003e struct link_list near[TIME_NEAR]; //到期时间较近定时器事件链表 41 \u003e struct link_list t[4][TIME_LEVEL]; //定时器事件链表 42 \u003e struct spinlock lock; //自旋锁 43 \u003e uint32_t time; //启动后tick的时间 44 \u003e uint32_t starttime; //启动时的系统时间 45 \u003e uint64_t current; //skynet自己计算出的实际时间 46 \u003e uint64_t current_point; //系统启动后的绝对时间 47 }; skynet定时器的基本思想就是把到期时间较近的事件放到near数组中，其余的放到t数值中，每次tick后就从near数组中找到到期的事件触发 tick函数 165 static void- 166 timer_update(struct timer *T) { 167 \u003e SPIN_LOCK(T); 168 169 \u003e // try to dispatch timeout 0 (rare condition) 170 \u003e timer_execute(T); 171 172 \u003e // shift time first, and then dispatch timer message 173 \u003e timer_shift(T); 174 175 \u003e timer_execute(T); 176 177 \u003e SPIN_UNLOCK(T); 178 } 245 void 246 skynet_updatetime(void) { 247 \u003e uint64_t cp = gettime(); 248 \u003e if(cp \u003c TI-\u003ecurrent_point) { 249 \u003e \u003e skynet_error(NULL, \"time diff error: change from %lld to %lld\", cp, TI-\u003ecurrent_point); 250 \u003e \u003e TI-\u003ecurrent_point = cp; 251 \u003e } else if (cp != TI-\u003ecurrent_point) { 252 \u003e \u003e uint32_t diff = (uint32_t)(cp - TI-\u003ecurrent_point); 253 \u003e \u003e TI-\u003ecurrent_point = cp; 254 \u003e \u003e TI-\u003ecurrent += diff; 255 \u003e \u003e int i; 256 \u003e \u003e for (i=0;i \u003e \u003e timer_update(TI); 258 \u003e \u003e } 259 \u003e } 260 } skynet的timer线程每2500微秒（即2.5毫秒）执行一次skynet_updatetime 主要流程 165 static void- 166 timer_update(struct timer *T) { 167 \u003e SPIN_LOCK(T); 168 169 \u003e // try to dispatch timeout 0 (rare condition) 170 \u003e timer_execute(T); // 触发定时器 171 172 \u003e // shift time first, and then dispatch timer message 173 \u003e timer_shift(T); // 过期时间再分配 174 175 \u003e timer_execute(T); // 触发定时器 176 177 \u003e SPIN_UNLOCK(T); 178 } 这里执行了两次timer_execute，按理来说，第一次timer_execute并不是必须的，因为注册过期时间为现在的事件是直接分发事件而不进入到定时器中。对于为啥执行两次的解释，作者希望timer 不需要关注 skynet 的用法实现，即不管skynet有没有处理time==0的情况，timer都不会因此出现bug 定时器的插入 67 static void 68 add_node(struct timer *T,struct timer_node *node) { 69 \u003e uint32_t time=node-\u003eexpire; // 过期时间 70 \u003e uint32_t current_time=T-\u003etime; // 当前时间 71 \u003e 72 \u003e if ((time|TIME_NEAR_MASK)==(current_time|TIME_NEAR_MASK)) { // 将较近的放到near数组中 73 \u003e \u003e link(\u0026T-\u003enear[time\u0026TIME_NEAR_MASK],node); 74 \u003e } else { 75 \u003e \u003e int i; 76 \u003e \u003e uint32_t mask=TIME_NEAR \u003c\u003c TIME_LEVEL_SHIFT; 77 \u003e \u003e for (i=0;i\u003c3;i++) { 78 \u003e \u003e \u003e if ((time|(mask-1))==(current_time|(mask-1))) { 79 \u003e \u003e \u003e \u003e break; 80 \u003e \u003e \u003e } 81 \u003e \u003e \u003e mask \u003c\u003c= TIME_LEVEL_SHIFT; 82 \u003e \u003e } 83 84 \u003e \u003e link(\u0026T-\u003et[i][((time\u003e\u003e(TIME_NEAR_SHIFT + i*TIME_LEVEL_SHIFT)) \u0026 TIME_LEVEL_MASK)],node);\u003e // 根据过期时间的长短放到 t 中 85 \u003e } 86 } 分析： 将和 current_time 高 24 位相同的 time 放到 near[x] 中，x 为 time 的第 0 ~ 7 位表示的十进制值（如 time 的第 0 ~ 7 位为 00000010时，放到 near[2] 中） 将和 current_time 高 18 位相同的 time 放到 t[0][x]中，x 为 time 的第 8 ~ 14 位表示的十进制值（如 time 的第 8 ~ 14 位为 000010时，放到 t[0][2] 中） 将和 current_time 高 12 位相同的 time 放到 t[1][x]中，x 为 time 的第 15 ~ 21 位表示的十进制值（如 time 的第 15 ~ 21 位为 000010时，放到 t[1][2] 中） 将和 current_time 高 6 位相同的 time 放到 t[2][x]中，x 为 time 的第 22 ~ 27 位表示的十进制值（如 time 的第 22 ~ 27 位为 000010时，放到 t[2][2] 中） 将其余的 time 放到 t[3][x] 中，x 为 time 的第 28 ~ 32 位表示的十进制值（如 time 的第 28 ~ 32 位为 000010时，放到 t[3][2] 中） 定时器的执行 152 static inline void 153 timer_execute(struct timer *T) { 154 \u003e int idx = T-\u003etime \u0026 TIME_NEAR_MASK; 155 \u003e 156 \u003e while (T-\u003enear[idx].head.next) { 157 \u003e \u003e struct timer_node *current = link_clear(\u0026T-\u003enear[idx]); 158 \u003e \u003e SPIN_UNLOCK(T); 159 \u003e \u003e // dispatch_list don't need lock T 160 \u003e \u003e dispatch_list(current); 161 \u003e \u003e SPIN_LOCK(T); 162 \u003e } 163 } 因为最近的事件都在near上，只需要根据idx就能找到到期的事件 过期事件的再分配 101 static void 102 move_list(struct timer *T, int level, int idx) { 103 \u003e struct timer_node *current = link_clear(\u0026T-\u003et[level][idx]); 104 \u003e while (current) { 105 \u003e \u003e struct timer_node *temp=current-\u003enext; 106 \u003e \u003e add_node(T,current); 107 \u003e \u003e current=temp; 108 \u003e } 109 } 111 static void 112 timer_shift(struct timer *T) { 113 \u003e int mask = TIME_NEAR; 114 \u003e uint32_t ct = ++T-\u003etime; 115 \u003e if (ct == 0) { 116 \u003e \u003e move_list(T, ","date":"2023-11-14","objectID":"/posts/skynet/skynet_timer/:0:0","series":null,"tags":null,"title":"skynet的定时器","uri":"/posts/skynet/skynet_timer/#定时器的插入"},{"categories":["skynet相关"],"content":"\r简析 skyent 用 5 个链表数组来存储定时事件，分别是 near[256] 和 t[4]，其中 near 数组用来存放即将到期的定时事件，t[0] ~ t[3] 数组分别存放不同时间段的定时事件 插入时，定时器根据过期时间 expire 和当前时间 now 的相对时间，将事件放到对应的链表当中 tick的时候，定时器直接取 near 中的到期事件触发 每过一定时间，定时器将 t[0] ~ t[3] 中的事件重新分配到合适的位置 主要结构 39 \u003e struct timer { 40 \u003e struct link_list near[TIME_NEAR]; //到期时间较近定时器事件链表 41 \u003e struct link_list t[4][TIME_LEVEL]; //定时器事件链表 42 \u003e struct spinlock lock; //自旋锁 43 \u003e uint32_t time; //启动后tick的时间 44 \u003e uint32_t starttime; //启动时的系统时间 45 \u003e uint64_t current; //skynet自己计算出的实际时间 46 \u003e uint64_t current_point; //系统启动后的绝对时间 47 }; skynet定时器的基本思想就是把到期时间较近的事件放到near数组中，其余的放到t数值中，每次tick后就从near数组中找到到期的事件触发 tick函数 165 static void- 166 timer_update(struct timer *T) { 167 \u003e SPIN_LOCK(T); 168 169 \u003e // try to dispatch timeout 0 (rare condition) 170 \u003e timer_execute(T); 171 172 \u003e // shift time first, and then dispatch timer message 173 \u003e timer_shift(T); 174 175 \u003e timer_execute(T); 176 177 \u003e SPIN_UNLOCK(T); 178 } 245 void 246 skynet_updatetime(void) { 247 \u003e uint64_t cp = gettime(); 248 \u003e if(cp \u003c TI-\u003ecurrent_point) { 249 \u003e \u003e skynet_error(NULL, \"time diff error: change from %lld to %lld\", cp, TI-\u003ecurrent_point); 250 \u003e \u003e TI-\u003ecurrent_point = cp; 251 \u003e } else if (cp != TI-\u003ecurrent_point) { 252 \u003e \u003e uint32_t diff = (uint32_t)(cp - TI-\u003ecurrent_point); 253 \u003e \u003e TI-\u003ecurrent_point = cp; 254 \u003e \u003e TI-\u003ecurrent += diff; 255 \u003e \u003e int i; 256 \u003e \u003e for (i=0;i \u003e \u003e timer_update(TI); 258 \u003e \u003e } 259 \u003e } 260 } skynet的timer线程每2500微秒（即2.5毫秒）执行一次skynet_updatetime 主要流程 165 static void- 166 timer_update(struct timer *T) { 167 \u003e SPIN_LOCK(T); 168 169 \u003e // try to dispatch timeout 0 (rare condition) 170 \u003e timer_execute(T); // 触发定时器 171 172 \u003e // shift time first, and then dispatch timer message 173 \u003e timer_shift(T); // 过期时间再分配 174 175 \u003e timer_execute(T); // 触发定时器 176 177 \u003e SPIN_UNLOCK(T); 178 } 这里执行了两次timer_execute，按理来说，第一次timer_execute并不是必须的，因为注册过期时间为现在的事件是直接分发事件而不进入到定时器中。对于为啥执行两次的解释，作者希望timer 不需要关注 skynet 的用法实现，即不管skynet有没有处理time==0的情况，timer都不会因此出现bug 定时器的插入 67 static void 68 add_node(struct timer *T,struct timer_node *node) { 69 \u003e uint32_t time=node-\u003eexpire; // 过期时间 70 \u003e uint32_t current_time=T-\u003etime; // 当前时间 71 \u003e 72 \u003e if ((time|TIME_NEAR_MASK)==(current_time|TIME_NEAR_MASK)) { // 将较近的放到near数组中 73 \u003e \u003e link(\u0026T-\u003enear[time\u0026TIME_NEAR_MASK],node); 74 \u003e } else { 75 \u003e \u003e int i; 76 \u003e \u003e uint32_t mask=TIME_NEAR \u003c\u003c TIME_LEVEL_SHIFT; 77 \u003e \u003e for (i=0;i\u003c3;i++) { 78 \u003e \u003e \u003e if ((time|(mask-1))==(current_time|(mask-1))) { 79 \u003e \u003e \u003e \u003e break; 80 \u003e \u003e \u003e } 81 \u003e \u003e \u003e mask \u003c\u003c= TIME_LEVEL_SHIFT; 82 \u003e \u003e } 83 84 \u003e \u003e link(\u0026T-\u003et[i][((time\u003e\u003e(TIME_NEAR_SHIFT + i*TIME_LEVEL_SHIFT)) \u0026 TIME_LEVEL_MASK)],node);\u003e // 根据过期时间的长短放到 t 中 85 \u003e } 86 } 分析： 将和 current_time 高 24 位相同的 time 放到 near[x] 中，x 为 time 的第 0 ~ 7 位表示的十进制值（如 time 的第 0 ~ 7 位为 00000010时，放到 near[2] 中） 将和 current_time 高 18 位相同的 time 放到 t[0][x]中，x 为 time 的第 8 ~ 14 位表示的十进制值（如 time 的第 8 ~ 14 位为 000010时，放到 t[0][2] 中） 将和 current_time 高 12 位相同的 time 放到 t[1][x]中，x 为 time 的第 15 ~ 21 位表示的十进制值（如 time 的第 15 ~ 21 位为 000010时，放到 t[1][2] 中） 将和 current_time 高 6 位相同的 time 放到 t[2][x]中，x 为 time 的第 22 ~ 27 位表示的十进制值（如 time 的第 22 ~ 27 位为 000010时，放到 t[2][2] 中） 将其余的 time 放到 t[3][x] 中，x 为 time 的第 28 ~ 32 位表示的十进制值（如 time 的第 28 ~ 32 位为 000010时，放到 t[3][2] 中） 定时器的执行 152 static inline void 153 timer_execute(struct timer *T) { 154 \u003e int idx = T-\u003etime \u0026 TIME_NEAR_MASK; 155 \u003e 156 \u003e while (T-\u003enear[idx].head.next) { 157 \u003e \u003e struct timer_node *current = link_clear(\u0026T-\u003enear[idx]); 158 \u003e \u003e SPIN_UNLOCK(T); 159 \u003e \u003e // dispatch_list don't need lock T 160 \u003e \u003e dispatch_list(current); 161 \u003e \u003e SPIN_LOCK(T); 162 \u003e } 163 } 因为最近的事件都在near上，只需要根据idx就能找到到期的事件 过期事件的再分配 101 static void 102 move_list(struct timer *T, int level, int idx) { 103 \u003e struct timer_node *current = link_clear(\u0026T-\u003et[level][idx]); 104 \u003e while (current) { 105 \u003e \u003e struct timer_node *temp=current-\u003enext; 106 \u003e \u003e add_node(T,current); 107 \u003e \u003e current=temp; 108 \u003e } 109 } 111 static void 112 timer_shift(struct timer *T) { 113 \u003e int mask = TIME_NEAR; 114 \u003e uint32_t ct = ++T-\u003etime; 115 \u003e if (ct == 0) { 116 \u003e \u003e move_list(T, ","date":"2023-11-14","objectID":"/posts/skynet/skynet_timer/:0:0","series":null,"tags":null,"title":"skynet的定时器","uri":"/posts/skynet/skynet_timer/#定时器的执行"},{"categories":["skynet相关"],"content":"\r简析 skyent 用 5 个链表数组来存储定时事件，分别是 near[256] 和 t[4]，其中 near 数组用来存放即将到期的定时事件，t[0] ~ t[3] 数组分别存放不同时间段的定时事件 插入时，定时器根据过期时间 expire 和当前时间 now 的相对时间，将事件放到对应的链表当中 tick的时候，定时器直接取 near 中的到期事件触发 每过一定时间，定时器将 t[0] ~ t[3] 中的事件重新分配到合适的位置 主要结构 39 \u003e struct timer { 40 \u003e struct link_list near[TIME_NEAR]; //到期时间较近定时器事件链表 41 \u003e struct link_list t[4][TIME_LEVEL]; //定时器事件链表 42 \u003e struct spinlock lock; //自旋锁 43 \u003e uint32_t time; //启动后tick的时间 44 \u003e uint32_t starttime; //启动时的系统时间 45 \u003e uint64_t current; //skynet自己计算出的实际时间 46 \u003e uint64_t current_point; //系统启动后的绝对时间 47 }; skynet定时器的基本思想就是把到期时间较近的事件放到near数组中，其余的放到t数值中，每次tick后就从near数组中找到到期的事件触发 tick函数 165 static void- 166 timer_update(struct timer *T) { 167 \u003e SPIN_LOCK(T); 168 169 \u003e // try to dispatch timeout 0 (rare condition) 170 \u003e timer_execute(T); 171 172 \u003e // shift time first, and then dispatch timer message 173 \u003e timer_shift(T); 174 175 \u003e timer_execute(T); 176 177 \u003e SPIN_UNLOCK(T); 178 } 245 void 246 skynet_updatetime(void) { 247 \u003e uint64_t cp = gettime(); 248 \u003e if(cp \u003c TI-\u003ecurrent_point) { 249 \u003e \u003e skynet_error(NULL, \"time diff error: change from %lld to %lld\", cp, TI-\u003ecurrent_point); 250 \u003e \u003e TI-\u003ecurrent_point = cp; 251 \u003e } else if (cp != TI-\u003ecurrent_point) { 252 \u003e \u003e uint32_t diff = (uint32_t)(cp - TI-\u003ecurrent_point); 253 \u003e \u003e TI-\u003ecurrent_point = cp; 254 \u003e \u003e TI-\u003ecurrent += diff; 255 \u003e \u003e int i; 256 \u003e \u003e for (i=0;i \u003e \u003e timer_update(TI); 258 \u003e \u003e } 259 \u003e } 260 } skynet的timer线程每2500微秒（即2.5毫秒）执行一次skynet_updatetime 主要流程 165 static void- 166 timer_update(struct timer *T) { 167 \u003e SPIN_LOCK(T); 168 169 \u003e // try to dispatch timeout 0 (rare condition) 170 \u003e timer_execute(T); // 触发定时器 171 172 \u003e // shift time first, and then dispatch timer message 173 \u003e timer_shift(T); // 过期时间再分配 174 175 \u003e timer_execute(T); // 触发定时器 176 177 \u003e SPIN_UNLOCK(T); 178 } 这里执行了两次timer_execute，按理来说，第一次timer_execute并不是必须的，因为注册过期时间为现在的事件是直接分发事件而不进入到定时器中。对于为啥执行两次的解释，作者希望timer 不需要关注 skynet 的用法实现，即不管skynet有没有处理time==0的情况，timer都不会因此出现bug 定时器的插入 67 static void 68 add_node(struct timer *T,struct timer_node *node) { 69 \u003e uint32_t time=node-\u003eexpire; // 过期时间 70 \u003e uint32_t current_time=T-\u003etime; // 当前时间 71 \u003e 72 \u003e if ((time|TIME_NEAR_MASK)==(current_time|TIME_NEAR_MASK)) { // 将较近的放到near数组中 73 \u003e \u003e link(\u0026T-\u003enear[time\u0026TIME_NEAR_MASK],node); 74 \u003e } else { 75 \u003e \u003e int i; 76 \u003e \u003e uint32_t mask=TIME_NEAR \u003c\u003c TIME_LEVEL_SHIFT; 77 \u003e \u003e for (i=0;i\u003c3;i++) { 78 \u003e \u003e \u003e if ((time|(mask-1))==(current_time|(mask-1))) { 79 \u003e \u003e \u003e \u003e break; 80 \u003e \u003e \u003e } 81 \u003e \u003e \u003e mask \u003c\u003c= TIME_LEVEL_SHIFT; 82 \u003e \u003e } 83 84 \u003e \u003e link(\u0026T-\u003et[i][((time\u003e\u003e(TIME_NEAR_SHIFT + i*TIME_LEVEL_SHIFT)) \u0026 TIME_LEVEL_MASK)],node);\u003e // 根据过期时间的长短放到 t 中 85 \u003e } 86 } 分析： 将和 current_time 高 24 位相同的 time 放到 near[x] 中，x 为 time 的第 0 ~ 7 位表示的十进制值（如 time 的第 0 ~ 7 位为 00000010时，放到 near[2] 中） 将和 current_time 高 18 位相同的 time 放到 t[0][x]中，x 为 time 的第 8 ~ 14 位表示的十进制值（如 time 的第 8 ~ 14 位为 000010时，放到 t[0][2] 中） 将和 current_time 高 12 位相同的 time 放到 t[1][x]中，x 为 time 的第 15 ~ 21 位表示的十进制值（如 time 的第 15 ~ 21 位为 000010时，放到 t[1][2] 中） 将和 current_time 高 6 位相同的 time 放到 t[2][x]中，x 为 time 的第 22 ~ 27 位表示的十进制值（如 time 的第 22 ~ 27 位为 000010时，放到 t[2][2] 中） 将其余的 time 放到 t[3][x] 中，x 为 time 的第 28 ~ 32 位表示的十进制值（如 time 的第 28 ~ 32 位为 000010时，放到 t[3][2] 中） 定时器的执行 152 static inline void 153 timer_execute(struct timer *T) { 154 \u003e int idx = T-\u003etime \u0026 TIME_NEAR_MASK; 155 \u003e 156 \u003e while (T-\u003enear[idx].head.next) { 157 \u003e \u003e struct timer_node *current = link_clear(\u0026T-\u003enear[idx]); 158 \u003e \u003e SPIN_UNLOCK(T); 159 \u003e \u003e // dispatch_list don't need lock T 160 \u003e \u003e dispatch_list(current); 161 \u003e \u003e SPIN_LOCK(T); 162 \u003e } 163 } 因为最近的事件都在near上，只需要根据idx就能找到到期的事件 过期事件的再分配 101 static void 102 move_list(struct timer *T, int level, int idx) { 103 \u003e struct timer_node *current = link_clear(\u0026T-\u003et[level][idx]); 104 \u003e while (current) { 105 \u003e \u003e struct timer_node *temp=current-\u003enext; 106 \u003e \u003e add_node(T,current); 107 \u003e \u003e current=temp; 108 \u003e } 109 } 111 static void 112 timer_shift(struct timer *T) { 113 \u003e int mask = TIME_NEAR; 114 \u003e uint32_t ct = ++T-\u003etime; 115 \u003e if (ct == 0) { 116 \u003e \u003e move_list(T, ","date":"2023-11-14","objectID":"/posts/skynet/skynet_timer/:0:0","series":null,"tags":null,"title":"skynet的定时器","uri":"/posts/skynet/skynet_timer/#过期事件的再分配"},{"categories":["skynet相关"],"content":"\r简析 skynet 创建专门的 socket 线程用于处理 socket 相关的逻辑。skynet 将 socket 相关的操作提炼出 lua 接口供用于在 lua 层操作 socket lua 层 与 socket 层并不在同一个线程内，lua 层向 socket 线程发送消息是通过管道，而 socket 层向 lua 层发送消息是通过 skynet 的消息系统 ","date":"2023-11-01","objectID":"/posts/skynet/skynet_net/:0:1","series":null,"tags":null,"title":"skynet的网络","uri":"/posts/skynet/skynet_net/#简析"},{"categories":["skynet相关"],"content":"\rlua socket 接口 lua socket 接口代码在文件 lualib-src/lua-socket.c 中 // lualib-src/lua-socket.c 820 LUAMOD_API int 821 luaopen_skynet_socketdriver(lua_State *L) { 822 \u003e luaL_checkversion(L); 823 \u003e luaL_Reg l[] = { 824 \u003e \u003e { \"buffer\", lnewbuffer }, 825 \u003e \u003e { \"push\", lpushbuffer }, 826 \u003e \u003e { \"pop\", lpopbuffer }, 827 \u003e \u003e { \"drop\", ldrop }, 828 \u003e \u003e { \"readall\", lreadall }, 829 \u003e \u003e { \"clear\", lclearbuffer }, 830 \u003e \u003e { \"readline\", lreadline }, 831 \u003e \u003e { \"str2p\", lstr2p }, 832 \u003e \u003e { \"header\", lheader }, 833 \u003e \u003e { \"info\", linfo }, 834 835 \u003e \u003e { \"unpack\", lunpack }, 836 \u003e \u003e { NULL, NULL }, 837 \u003e }; 838 \u003e luaL_newlib(L,l); 839 \u003e luaL_Reg l2[] = { 840 \u003e \u003e { \"connect\", lconnect }, 841 \u003e \u003e { \"close\", lclose }, 842 \u003e \u003e { \"shutdown\", lshutdown }, 843 \u003e \u003e { \"listen\", llisten }, 844 \u003e \u003e { \"send\", lsend }, 845 \u003e \u003e { \"lsend\", lsendlow }, 846 \u003e \u003e { \"bind\", lbind }, 847 \u003e \u003e { \"start\", lstart }, 848 \u003e \u003e { \"pause\", lpause }, 849 \u003e \u003e { \"nodelay\", lnodelay }, 850 \u003e \u003e { \"udp\", ludp }, 851 \u003e \u003e { \"udp_connect\", ludp_connect }, 852 \u003e \u003e { \"udp_send\", ludp_send }, 853 \u003e \u003e { \"udp_address\", ludp_address }, 854 \u003e \u003e { \"resolve\", lresolve }, 855 \u003e \u003e { NULL, NULL }, 856 \u003e }; 857 \u003e lua_getfield(L, LUA_REGISTRYINDEX, \"skynet_context\"); 858 \u003e struct skynet_context *ctx = lua_touserdata(L,-1); 859 \u003e if (ctx == NULL) { 860 \u003e \u003e return luaL_error(L, \"Init skynet context first\"); 861 \u003e } 862 863 \u003e luaL_setfuncs(L,l2,1); 864 865 \u003e return 1; 866 } socket 操作为 l2 注册的 c 函数，这些函数最后都会调用 send_request 函数 // skynet-src/socket_server.c 1786 static void 1787 send_request(struct socket_server *ss, struct request_package *request, char type, int len) { 1788 request-\u003eheader[6] = (uint8_t)type; 1789 request-\u003eheader[7] = (uint8_t)len; 1790 const char * req = (const char *)request + offsetof(struct request_package, header[6]); 1791 for (;;) { 1792 ssize_t n = write(ss-\u003esendctrl_fd, req, len+2); 1793 if (n\u003c0) { 1794 if (errno != EINTR) { 1795 skynet_error(NULL, \"socket-server : send ctrl command error %s.\", strerror(errno)); 1796 } 1797 continue; 1798 } 1799 assert(n == len+2); 1800 return; 1801 } 1802 } send_request 的作用是把对应的操作及参数序列化并写入 socket sendctrl_fd 中 socket 线程那边通过监听 sendctrl_fd 获取对应 socket 操作类型及参数 ","date":"2023-11-01","objectID":"/posts/skynet/skynet_net/:0:2","series":null,"tags":null,"title":"skynet的网络","uri":"/posts/skynet/skynet_net/#lua-socket-接口"},{"categories":["skynet相关"],"content":"\rskyent 的 socket 线程 socket 线程主循环就是不断调用 socket_server_poll，主要做两件事 调用 socket_server_poll 处理 socket 调用 forward_message 把处理的结果打包发送给 lua 层 // skynet-src/skynet_socket.c 78 int- 79 skynet_socket_poll() { 80 \u003e struct socket_server *ss = SOCKET_SERVER; 81 \u003e assert(ss); 82 \u003e struct socket_message result; 83 \u003e int more = 1; 84 \u003e int type = socket_server_poll(ss, \u0026result, \u0026more); 85 \u003e switch (type) { 86 \u003e case SOCKET_EXIT: 87 \u003e \u003e return 0; 88 \u003e case SOCKET_DATA: 89 \u003e \u003e forward_message(SKYNET_SOCKET_TYPE_DATA, false, \u0026result); 90 \u003e \u003e break; 91 \u003e case SOCKET_CLOSE: 92 \u003e \u003e forward_message(SKYNET_SOCKET_TYPE_CLOSE, false, \u0026result); 93 \u003e \u003e break; 94 \u003e case SOCKET_OPEN: 95 \u003e \u003e forward_message(SKYNET_SOCKET_TYPE_CONNECT, true, \u0026result); 96 \u003e \u003e break; 97 \u003e case SOCKET_ERR: 98 \u003e \u003e forward_message(SKYNET_SOCKET_TYPE_ERROR, true, \u0026result); 99 \u003e \u003e break; 100 \u003e case SOCKET_ACCEPT: 101 \u003e \u003e forward_message(SKYNET_SOCKET_TYPE_ACCEPT, true, \u0026result); 102 \u003e \u003e break; 103 \u003e case SOCKET_UDP: 104 \u003e \u003e forward_message(SKYNET_SOCKET_TYPE_UDP, false, \u0026result); 105 \u003e \u003e break; 106 \u003e case SOCKET_WARNING: 107 \u003e \u003e forward_message(SKYNET_SOCKET_TYPE_WARNING, false, \u0026result); 108 \u003e \u003e break; 109 \u003e default: 110 \u003e \u003e skynet_error(NULL, \"Unknown socket message type %d.\",type); 111 \u003e \u003e return -1; 112 \u003e } 113 \u003e if (more) { 114 \u003e \u003e return -1; 115 \u003e } 116 \u003e return 1; 117 } socket_server_pool 主要做两件事情 调用 has_cmd 函数检查 sendctrl_fd 是否有事件，如果有就调用 ctrl_cmd 根据操作类型进行对应处理（如 “B” 对应 bind，“L” 对应 listen， “K” 对应 close 等） 调用 epoll_wait 获取监听 socket 的事件，并对应处理（ 1726 ~1771） 1671 int- 1672 socket_server_poll(struct socket_server *ss, struct socket_message * result, int * more) { 1673 \u003e for (;;) { 1674 \u003e \u003e if (ss-\u003echeckctrl) { 1675 \u003e \u003e \u003e if (has_cmd(ss)) { 1676 \u003e \u003e \u003e \u003e int type = ctrl_cmd(ss, result); 1677 \u003e \u003e \u003e \u003e if (type != -1) { 1678 \u003e \u003e \u003e \u003e \u003e clear_closed_event(ss, result, type); 1679 \u003e \u003e \u003e \u003e \u003e return type; 1680 \u003e \u003e \u003e \u003e } else 1681 \u003e \u003e \u003e \u003e \u003e continue; 1682 \u003e \u003e \u003e } else { 1683 \u003e \u003e \u003e \u003e ss-\u003echeckctrl = 0; 1684 \u003e \u003e \u003e } 1685 \u003e \u003e } 1686 \u003e \u003e if (ss-\u003eevent_index == ss-\u003eevent_n) { 1687 \u003e \u003e \u003e ss-\u003eevent_n = sp_wait(ss-\u003eevent_fd, ss-\u003eev, MAX_EVENT); 1688 \u003e \u003e \u003e ss-\u003echeckctrl = 1; 1689 \u003e \u003e \u003e if (more) { 1690 \u003e \u003e \u003e \u003e *more = 0; 1691 \u003e \u003e \u003e } 1692 \u003e \u003e \u003e ss-\u003eevent_index = 0; 1693 \u003e \u003e \u003e if (ss-\u003eevent_n \u003c= 0) { 1694 \u003e \u003e \u003e \u003e ss-\u003eevent_n = 0; 1695 \u003e \u003e \u003e \u003e int err = errno; 1696 \u003e \u003e \u003e \u003e if (err != EINTR) { 1697 \u003e \u003e \u003e \u003e \u003e skynet_error(NULL, \"socket-server: %s\", strerror(err)); 1698 \u003e \u003e \u003e \u003e } 1699 \u003e \u003e \u003e \u003e continue; 1700 \u003e \u003e \u003e } 1701 \u003e \u003e } ... 1726 \u003e \u003e default: 1727 \u003e \u003e \u003e if (e-\u003eread) { 1728 \u003e \u003e \u003e \u003e int type; 1729 \u003e \u003e \u003e \u003e if (s-\u003eprotocol == PROTOCOL_TCP) { 1730 \u003e \u003e \u003e \u003e \u003e type = forward_message_tcp(ss, s, \u0026l, result); 1731 \u003e \u003e \u003e \u003e \u003e if (type == SOCKET_MORE) { 1732 \u003e \u003e \u003e \u003e \u003e \u003e --ss-\u003eevent_index; 1733 \u003e \u003e \u003e \u003e \u003e \u003e return SOCKET_DATA; 1734 \u003e \u003e \u003e \u003e \u003e } 1735 \u003e \u003e \u003e \u003e } else { 1736 \u003e \u003e \u003e \u003e \u003e type = forward_message_udp(ss, s, \u0026l, result); 1737 \u003e \u003e \u003e \u003e \u003e if (type == SOCKET_UDP) { 1738 \u003e \u003e \u003e \u003e \u003e \u003e // try read again 1739 \u003e \u003e \u003e \u003e \u003e \u003e --ss-\u003eevent_index; 1740 \u003e \u003e \u003e \u003e \u003e \u003e return SOCKET_UDP; 1741 \u003e \u003e \u003e \u003e \u003e } 1742 \u003e \u003e \u003e \u003e } 1743 \u003e \u003e \u003e \u003e if (e-\u003ewrite \u0026\u0026 type != SOCKET_CLOSE \u0026\u0026 type != SOCKET_ERR) { 1744 \u003e \u003e \u003e \u003e \u003e // Try to dispatch write message next step if write flag set. 1745 \u003e \u003e \u003e \u003e \u003e e-\u003eread = false; 1746 \u003e \u003e \u003e \u003e \u003e --ss-\u003eevent_index; 1747 \u003e \u003e \u003e \u003e } 1748 \u003e \u003e \u003e \u003e if (type == -1) 1749 \u003e \u003e \u003e \u003e \u003e break;\u003e \u003e \u003e \u003e 1750 \u003e \u003e \u003e \u003e return type; 1751 \u003e \u003e \u003e } 1752 \u003e \u003e \u003e if (e-\u003ewrite) { 1753 \u003e \u003e \u003e \u003e int type = send_buffer(ss, s, \u0026l, result); 1754 \u003e \u003e \u003e \u003e if (type == -1) 1755 \u003e \u003e \u003e \u003e \u003e break; 1756 \u003e \u003e \u003e \u003e return type; 1757 \u003e \u003e \u003e } 1758 \u003e \u003e \u003e if (e-\u003eerror) { 1759 \u003e \u003e \u003e \u003e int error; 1760 \u003e \u003e \u003e \u003e socklen_t len = sizeof(error);-- 1761 \u003e \u003e \u003e \u003e int code = getsockopt(s-\u003efd, SOL_SOCKET, SO_ERROR, \u0026error, \u0026len);-- 1762 \u003e \u003e \u003e \u003e const char * err = NULL; 1763 \u003e \u003e \u003e \u003e if (code \u003c 0) { 1764 \u003e \u003e \u003e \u003e \u003e err = strerror(errno); 1765 \u003e \u003e \u003e \u003e } else if (error != 0) { 1766 \u003e \u003e \u003e \u003e \u003e err = strerror(error); 1767 \u003e \u003e \u003e \u003e } else { 1768 \u003e \u003e \u003e \u003e \u003e err = \"Unknown error\"; 1769 \u003e \u003e \u003e \u003e }","date":"2023-11-01","objectID":"/posts/skynet/skynet_net/:0:3","series":null,"tags":null,"title":"skynet的网络","uri":"/posts/skynet/skynet_net/#skyent-的-socket-线程"},{"categories":["skynet相关"],"content":"\rskynet 中 c 端绑定 lua 函数的做法\r简述 创建新的 userdata 创建一个新 thread 对象，把 callback function 至于这个 thread 中 把新 thread 绑定 userdata 的 uservalue 上（防止 gc） userdata 的 C 指针放到 C 端，用于 callback 调用 callback function 放到新 thead 中而不是注册表中是因为这样做比从注册表中读会 callback function 要高效 代码分析 122 static int 123 lcallback(lua_State *L) { 124 \u003e struct skynet_context * context = lua_touserdata(L, lua_upvalueindex(1)); 125 \u003e int forward = lua_toboolean(L, 2); // 获取第二个参数 126 \u003e luaL_checktype(L,1,LUA_TFUNCTION); // 检查第一个参数是否是 lua function 127 \u003e lua_settop(L,1); // 弹出除第一个参数外的所有参数，即 lua function 在 L 的栈底 128 \u003e struct callback_context * cb_ctx = (struct callback_context *)lua_newuserdatauv(L, sizeof(*cb_ctx), 2); // 创建 userdata callback_context 并压入到 L 的栈中 129 \u003e cb_ctx-\u003eL = lua_newthread(L); // 创建一个新线程并压入 L 的栈中 130 \u003e lua_pushcfunction(cb_ctx-\u003eL, traceback); // 将 c 函数 traceback 压入新线程的栈中 131 \u003e lua_setiuservalue(L, -2, 1); // 将新线程弹出并设置为 128 创建的 userdata 的 uservalue 132 \u003e lua_getfield(L, LUA_REGISTRYINDEX, \"callback_context\"); // 将注册表下标为 \"callback_context\" 的值压入 L 的栈中 133 \u003e lua_setiuservalue(L, -2, 2); // 将上面压入的值设置为 128 创建的 userdata 的 uservalue 134 \u003e lua_setfield(L, LUA_REGISTRYINDEX, \"callback_context\"); // 将 128 创建的 userdata 设置为注册表下标为 \"callback_context\" 的值 135 \u003e lua_xmove(L, cb_ctx-\u003eL, 1); // 将 userdata 从 L 中弹出并压入新线程中 136 137 \u003e skynet_callback(context, cb_ctx, (forward)?(_forward_pre):(_cb_pre)); // 将 _cb_pre/_forward_pre 设置为 context 的 callback，两种区别为是否自动释放 skynet msg 的空间 138 \u003e return 0; 139 } 第132~133行把旧的 userdata 设置为新 userdata 的 uservalue，是因为在重新设置 callback 时，如果直接替换掉旧的 userdata 时，这只旧 userdata 就处于可被 gc 的状态，如果此时触发 gc 行为，将会导致当前正在被执行的 coroutine 和 cb_ctx 被回收掉，从而导致crash。具体如下：skynet callback导致的crash问题 Lua binding 中正确的 callback ","date":"2023-09-19","objectID":"/posts/skynet/skynet_cb/:1:0","series":null,"tags":null,"title":"skynet的callback","uri":"/posts/skynet/skynet_cb/#skynet-中-c-端绑定-lua-函数的做法"},{"categories":["skynet相关"],"content":"\rskynet 中 c 端绑定 lua 函数的做法\r简述 创建新的 userdata 创建一个新 thread 对象，把 callback function 至于这个 thread 中 把新 thread 绑定 userdata 的 uservalue 上（防止 gc） userdata 的 C 指针放到 C 端，用于 callback 调用 callback function 放到新 thead 中而不是注册表中是因为这样做比从注册表中读会 callback function 要高效 代码分析 122 static int 123 lcallback(lua_State *L) { 124 \u003e struct skynet_context * context = lua_touserdata(L, lua_upvalueindex(1)); 125 \u003e int forward = lua_toboolean(L, 2); // 获取第二个参数 126 \u003e luaL_checktype(L,1,LUA_TFUNCTION); // 检查第一个参数是否是 lua function 127 \u003e lua_settop(L,1); // 弹出除第一个参数外的所有参数，即 lua function 在 L 的栈底 128 \u003e struct callback_context * cb_ctx = (struct callback_context *)lua_newuserdatauv(L, sizeof(*cb_ctx), 2); // 创建 userdata callback_context 并压入到 L 的栈中 129 \u003e cb_ctx-\u003eL = lua_newthread(L); // 创建一个新线程并压入 L 的栈中 130 \u003e lua_pushcfunction(cb_ctx-\u003eL, traceback); // 将 c 函数 traceback 压入新线程的栈中 131 \u003e lua_setiuservalue(L, -2, 1); // 将新线程弹出并设置为 128 创建的 userdata 的 uservalue 132 \u003e lua_getfield(L, LUA_REGISTRYINDEX, \"callback_context\"); // 将注册表下标为 \"callback_context\" 的值压入 L 的栈中 133 \u003e lua_setiuservalue(L, -2, 2); // 将上面压入的值设置为 128 创建的 userdata 的 uservalue 134 \u003e lua_setfield(L, LUA_REGISTRYINDEX, \"callback_context\"); // 将 128 创建的 userdata 设置为注册表下标为 \"callback_context\" 的值 135 \u003e lua_xmove(L, cb_ctx-\u003eL, 1); // 将 userdata 从 L 中弹出并压入新线程中 136 137 \u003e skynet_callback(context, cb_ctx, (forward)?(_forward_pre):(_cb_pre)); // 将 _cb_pre/_forward_pre 设置为 context 的 callback，两种区别为是否自动释放 skynet msg 的空间 138 \u003e return 0; 139 } 第132~133行把旧的 userdata 设置为新 userdata 的 uservalue，是因为在重新设置 callback 时，如果直接替换掉旧的 userdata 时，这只旧 userdata 就处于可被 gc 的状态，如果此时触发 gc 行为，将会导致当前正在被执行的 coroutine 和 cb_ctx 被回收掉，从而导致crash。具体如下：skynet callback导致的crash问题 Lua binding 中正确的 callback ","date":"2023-09-19","objectID":"/posts/skynet/skynet_cb/:1:0","series":null,"tags":null,"title":"skynet的callback","uri":"/posts/skynet/skynet_cb/#简述"},{"categories":["skynet相关"],"content":"\rskynet 中 c 端绑定 lua 函数的做法\r简述 创建新的 userdata 创建一个新 thread 对象，把 callback function 至于这个 thread 中 把新 thread 绑定 userdata 的 uservalue 上（防止 gc） userdata 的 C 指针放到 C 端，用于 callback 调用 callback function 放到新 thead 中而不是注册表中是因为这样做比从注册表中读会 callback function 要高效 代码分析 122 static int 123 lcallback(lua_State *L) { 124 \u003e struct skynet_context * context = lua_touserdata(L, lua_upvalueindex(1)); 125 \u003e int forward = lua_toboolean(L, 2); // 获取第二个参数 126 \u003e luaL_checktype(L,1,LUA_TFUNCTION); // 检查第一个参数是否是 lua function 127 \u003e lua_settop(L,1); // 弹出除第一个参数外的所有参数，即 lua function 在 L 的栈底 128 \u003e struct callback_context * cb_ctx = (struct callback_context *)lua_newuserdatauv(L, sizeof(*cb_ctx), 2); // 创建 userdata callback_context 并压入到 L 的栈中 129 \u003e cb_ctx-\u003eL = lua_newthread(L); // 创建一个新线程并压入 L 的栈中 130 \u003e lua_pushcfunction(cb_ctx-\u003eL, traceback); // 将 c 函数 traceback 压入新线程的栈中 131 \u003e lua_setiuservalue(L, -2, 1); // 将新线程弹出并设置为 128 创建的 userdata 的 uservalue 132 \u003e lua_getfield(L, LUA_REGISTRYINDEX, \"callback_context\"); // 将注册表下标为 \"callback_context\" 的值压入 L 的栈中 133 \u003e lua_setiuservalue(L, -2, 2); // 将上面压入的值设置为 128 创建的 userdata 的 uservalue 134 \u003e lua_setfield(L, LUA_REGISTRYINDEX, \"callback_context\"); // 将 128 创建的 userdata 设置为注册表下标为 \"callback_context\" 的值 135 \u003e lua_xmove(L, cb_ctx-\u003eL, 1); // 将 userdata 从 L 中弹出并压入新线程中 136 137 \u003e skynet_callback(context, cb_ctx, (forward)?(_forward_pre):(_cb_pre)); // 将 _cb_pre/_forward_pre 设置为 context 的 callback，两种区别为是否自动释放 skynet msg 的空间 138 \u003e return 0; 139 } 第132~133行把旧的 userdata 设置为新 userdata 的 uservalue，是因为在重新设置 callback 时，如果直接替换掉旧的 userdata 时，这只旧 userdata 就处于可被 gc 的状态，如果此时触发 gc 行为，将会导致当前正在被执行的 coroutine 和 cb_ctx 被回收掉，从而导致crash。具体如下：skynet callback导致的crash问题 Lua binding 中正确的 callback ","date":"2023-09-19","objectID":"/posts/skynet/skynet_cb/:1:0","series":null,"tags":null,"title":"skynet的callback","uri":"/posts/skynet/skynet_cb/#代码分析"},{"categories":["其他"],"content":" 段表 保存ELF文件各段的基本属性的结构，记录包括段名，段长度，在文件中的偏移量以及读写权限和段的其他属性 代码段 程序源代码编译后的机器指令存放位置，在文件中占用一定大小，该区域通常为只读 数据段（.data段） 保存已经初始化了的全局静态变量和局部静态变量，在文件中占用一定大小 .bss段 存放未初始化的全局变量和局部静态变量，在文件中不占大小（节省磁盘空间），程序运行前才会开辟空间，已经初始化的变量也可能因为编译器的优化而存放在该段（比如初始化值为0），该段大小（所有未初始化的变量总大小）记录在段表里，符号表里会记录各变量的位置 符号表 记录目标文件中所用到的所有符号（包括定义在目标文件中的全局符号，在本目标文件中引用的全局符号，段名，局部符号，行号信息等） 问题：ELF是什么？其大小与程序中全局变量的是否初始化有什么关系ELF为目标文件（linux下），未初始化的全局变量会保存在.bss段，而.bss段在ELF文件中并不占用大小。 ","date":"2021-11-15","objectID":"/posts/other/elf%E6%96%87%E4%BB%B6/:0:0","series":null,"tags":null,"title":"ELF文件","uri":"/posts/other/elf%E6%96%87%E4%BB%B6/#"},{"categories":["其他"],"content":" 段表 保存ELF文件各段的基本属性的结构，记录包括段名，段长度，在文件中的偏移量以及读写权限和段的其他属性 代码段 程序源代码编译后的机器指令存放位置，在文件中占用一定大小，该区域通常为只读 数据段（.data段） 保存已经初始化了的全局静态变量和局部静态变量，在文件中占用一定大小 .bss段 存放未初始化的全局变量和局部静态变量，在文件中不占大小（节省磁盘空间），程序运行前才会开辟空间，已经初始化的变量也可能因为编译器的优化而存放在该段（比如初始化值为0），该段大小（所有未初始化的变量总大小）记录在段表里，符号表里会记录各变量的位置 符号表 记录目标文件中所用到的所有符号（包括定义在目标文件中的全局符号，在本目标文件中引用的全局符号，段名，局部符号，行号信息等） 问题：ELF是什么？其大小与程序中全局变量的是否初始化有什么关系ELF为目标文件（linux下），未初始化的全局变量会保存在.bss段，而.bss段在ELF文件中并不占用大小。 ","date":"2021-11-15","objectID":"/posts/other/elf%E6%96%87%E4%BB%B6/:0:0","series":null,"tags":null,"title":"ELF文件","uri":"/posts/other/elf%E6%96%87%E4%BB%B6/#问题elf是什么其大小与程序中全局变量的是否初始化有什么关系"},{"categories":["其他"],"content":"\rGit配置 设置名称： git config –global user.name “名称” 设置邮箱： git config –global user.email “邮箱” 查看设置： git config –global –list ","date":"2021-10-27","objectID":"/posts/other/git%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/:0:0","series":null,"tags":null,"title":"Git基本操作","uri":"/posts/other/git%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/#git配置"},{"categories":["其他"],"content":"\rGit配置 设置名称： git config –global user.name “名称” 设置邮箱： git config –global user.email “邮箱” 查看设置： git config –global –list ","date":"2021-10-27","objectID":"/posts/other/git%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/:0:0","series":null,"tags":null,"title":"Git基本操作","uri":"/posts/other/git%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/#设置名称"},{"categories":["其他"],"content":"\rGit配置 设置名称： git config –global user.name “名称” 设置邮箱： git config –global user.email “邮箱” 查看设置： git config –global –list ","date":"2021-10-27","objectID":"/posts/other/git%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/:0:0","series":null,"tags":null,"title":"Git基本操作","uri":"/posts/other/git%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/#设置邮箱"},{"categories":["其他"],"content":"\rGit配置 设置名称： git config –global user.name “名称” 设置邮箱： git config –global user.email “邮箱” 查看设置： git config –global –list ","date":"2021-10-27","objectID":"/posts/other/git%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/:0:0","series":null,"tags":null,"title":"Git基本操作","uri":"/posts/other/git%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/#查看设置"},{"categories":["其他"],"content":"\rGit分支操作 新建分支： git branch 分支名 git checkout -b 分支名 切换分支： git checkout 分支名 删除分支： git branch -D 分支名 合并分支 git merge 分支名 ","date":"2021-10-27","objectID":"/posts/other/git%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/:0:0","series":null,"tags":null,"title":"Git基本操作","uri":"/posts/other/git%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/#git分支操作"},{"categories":["其他"],"content":"\rGit分支操作 新建分支： git branch 分支名 git checkout -b 分支名 切换分支： git checkout 分支名 删除分支： git branch -D 分支名 合并分支 git merge 分支名 ","date":"2021-10-27","objectID":"/posts/other/git%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/:0:0","series":null,"tags":null,"title":"Git基本操作","uri":"/posts/other/git%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/#新建分支"},{"categories":["其他"],"content":"\rGit分支操作 新建分支： git branch 分支名 git checkout -b 分支名 切换分支： git checkout 分支名 删除分支： git branch -D 分支名 合并分支 git merge 分支名 ","date":"2021-10-27","objectID":"/posts/other/git%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/:0:0","series":null,"tags":null,"title":"Git基本操作","uri":"/posts/other/git%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/#切换分支"},{"categories":["其他"],"content":"\rGit分支操作 新建分支： git branch 分支名 git checkout -b 分支名 切换分支： git checkout 分支名 删除分支： git branch -D 分支名 合并分支 git merge 分支名 ","date":"2021-10-27","objectID":"/posts/other/git%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/:0:0","series":null,"tags":null,"title":"Git基本操作","uri":"/posts/other/git%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/#删除分支"},{"categories":["其他"],"content":"\rGit分支操作 新建分支： git branch 分支名 git checkout -b 分支名 切换分支： git checkout 分支名 删除分支： git branch -D 分支名 合并分支 git merge 分支名 ","date":"2021-10-27","objectID":"/posts/other/git%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/:0:0","series":null,"tags":null,"title":"Git基本操作","uri":"/posts/other/git%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/#合并分支"},{"categories":["其他"],"content":"\rGit基本工作流程 添加文件到暂存区(staging area)： git add 文件名 将暂存区文件添加到仓库(local repository)中： git commit -m “提交信息” 上传远程代码(remote repository)并合并： git push -u origin 分支名 下载远程代码并合并 git pull ","date":"2021-10-27","objectID":"/posts/other/git%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/:0:0","series":null,"tags":null,"title":"Git基本操作","uri":"/posts/other/git%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/#git基本工作流程"},{"categories":["其他"],"content":"\rGit基本工作流程 添加文件到暂存区(staging area)： git add 文件名 将暂存区文件添加到仓库(local repository)中： git commit -m “提交信息” 上传远程代码(remote repository)并合并： git push -u origin 分支名 下载远程代码并合并 git pull ","date":"2021-10-27","objectID":"/posts/other/git%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/:0:0","series":null,"tags":null,"title":"Git基本操作","uri":"/posts/other/git%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/#添加文件到暂存区staging-area"},{"categories":["其他"],"content":"\rGit基本工作流程 添加文件到暂存区(staging area)： git add 文件名 将暂存区文件添加到仓库(local repository)中： git commit -m “提交信息” 上传远程代码(remote repository)并合并： git push -u origin 分支名 下载远程代码并合并 git pull ","date":"2021-10-27","objectID":"/posts/other/git%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/:0:0","series":null,"tags":null,"title":"Git基本操作","uri":"/posts/other/git%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/#将暂存区文件添加到仓库local-repository中"},{"categories":["其他"],"content":"\rGit基本工作流程 添加文件到暂存区(staging area)： git add 文件名 将暂存区文件添加到仓库(local repository)中： git commit -m “提交信息” 上传远程代码(remote repository)并合并： git push -u origin 分支名 下载远程代码并合并 git pull ","date":"2021-10-27","objectID":"/posts/other/git%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/:0:0","series":null,"tags":null,"title":"Git基本操作","uri":"/posts/other/git%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/#上传远程代码remote-repository并合并"},{"categories":["其他"],"content":"\rGit基本工作流程 添加文件到暂存区(staging area)： git add 文件名 将暂存区文件添加到仓库(local repository)中： git commit -m “提交信息” 上传远程代码(remote repository)并合并： git push -u origin 分支名 下载远程代码并合并 git pull ","date":"2021-10-27","objectID":"/posts/other/git%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/:0:0","series":null,"tags":null,"title":"Git基本操作","uri":"/posts/other/git%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/#下载远程代码并合并"},{"categories":["其他"],"content":"\rGit暂存区文件操作 使用暂存区指定文件覆盖工作区对应文件： git checkout 文件名 删除暂存区文件（工作区不做改变）： git rm –cached 文件名 回滚暂存区对应文件到当前版本： git reset HEAD 文件名 ","date":"2021-10-27","objectID":"/posts/other/git%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/:0:0","series":null,"tags":null,"title":"Git基本操作","uri":"/posts/other/git%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/#git暂存区文件操作"},{"categories":["其他"],"content":"\rGit暂存区文件操作 使用暂存区指定文件覆盖工作区对应文件： git checkout 文件名 删除暂存区文件（工作区不做改变）： git rm –cached 文件名 回滚暂存区对应文件到当前版本： git reset HEAD 文件名 ","date":"2021-10-27","objectID":"/posts/other/git%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/:0:0","series":null,"tags":null,"title":"Git基本操作","uri":"/posts/other/git%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/#使用暂存区指定文件覆盖工作区对应文件"},{"categories":["其他"],"content":"\rGit暂存区文件操作 使用暂存区指定文件覆盖工作区对应文件： git checkout 文件名 删除暂存区文件（工作区不做改变）： git rm –cached 文件名 回滚暂存区对应文件到当前版本： git reset HEAD 文件名 ","date":"2021-10-27","objectID":"/posts/other/git%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/:0:0","series":null,"tags":null,"title":"Git基本操作","uri":"/posts/other/git%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/#删除暂存区文件工作区不做改变"},{"categories":["其他"],"content":"\rGit暂存区文件操作 使用暂存区指定文件覆盖工作区对应文件： git checkout 文件名 删除暂存区文件（工作区不做改变）： git rm –cached 文件名 回滚暂存区对应文件到当前版本： git reset HEAD 文件名 ","date":"2021-10-27","objectID":"/posts/other/git%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/:0:0","series":null,"tags":null,"title":"Git基本操作","uri":"/posts/other/git%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/#回滚暂存区对应文件到当前版本"},{"categories":["其他"],"content":"\rGit远端交互 列出所有remote： git remote -v 添加远端： git remote add xxx 删除远端关联： git remote remove xxx ","date":"2021-10-27","objectID":"/posts/other/git%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/:0:0","series":null,"tags":null,"title":"Git基本操作","uri":"/posts/other/git%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/#git远端交互"},{"categories":["其他"],"content":"\rGit远端交互 列出所有remote： git remote -v 添加远端： git remote add xxx 删除远端关联： git remote remove xxx ","date":"2021-10-27","objectID":"/posts/other/git%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/:0:0","series":null,"tags":null,"title":"Git基本操作","uri":"/posts/other/git%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/#列出所有remote"},{"categories":["其他"],"content":"\rGit远端交互 列出所有remote： git remote -v 添加远端： git remote add xxx 删除远端关联： git remote remove xxx ","date":"2021-10-27","objectID":"/posts/other/git%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/:0:0","series":null,"tags":null,"title":"Git基本操作","uri":"/posts/other/git%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/#添加远端"},{"categories":["其他"],"content":"\rGit远端交互 列出所有remote： git remote -v 添加远端： git remote add xxx 删除远端关联： git remote remove xxx ","date":"2021-10-27","objectID":"/posts/other/git%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/:0:0","series":null,"tags":null,"title":"Git基本操作","uri":"/posts/other/git%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/#删除远端关联"},{"categories":["redis"],"content":"\rredis主从库模式 ","date":"2021-08-10","objectID":"/posts/redis/redis%E4%B8%BB%E4%BB%8E%E5%BA%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5/:0:0","series":null,"tags":null,"title":"Redis主从库数据同步","uri":"/posts/redis/redis%E4%B8%BB%E4%BB%8E%E5%BA%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5/#redis主从库模式"},{"categories":["redis"],"content":"\rredis高可靠性 高可靠性包含两层含义：数据尽量少丢失，服务尽量少中断 前者使用AOF和RDB可以保证 后者使用主从库模式保证 ","date":"2021-08-10","objectID":"/posts/redis/redis%E4%B8%BB%E4%BB%8E%E5%BA%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5/:1:0","series":null,"tags":null,"title":"Redis主从库数据同步","uri":"/posts/redis/redis%E4%B8%BB%E4%BB%8E%E5%BA%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5/#redis高可靠性"},{"categories":["redis"],"content":"\r读写分离 读操作：主库、从库都可以接收 写操作：⾸先到主库执⾏，然后，主库将写操作同步给从库 ","date":"2021-08-10","objectID":"/posts/redis/redis%E4%B8%BB%E4%BB%8E%E5%BA%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5/:2:0","series":null,"tags":null,"title":"Redis主从库数据同步","uri":"/posts/redis/redis%E4%B8%BB%E4%BB%8E%E5%BA%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5/#读写分离"},{"categories":["redis"],"content":"\r一致性问题 redis在主从数据库之间的复制是异步的，这意味着，主数据库执行完客户端请求的命令后会立即将命令发送给请求的客户端，并同步到从数据库，而不会等待从数据库接收到命令后再返回给客户端，因此该情况就会存在一定的数据不一致性风险，也就是redis不保证强一致性，而是通过从库策略追赶与主数据库的数据差异，以保障主从状态一致 ","date":"2021-08-10","objectID":"/posts/redis/redis%E4%B8%BB%E4%BB%8E%E5%BA%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5/:3:0","series":null,"tags":null,"title":"Redis主从库数据同步","uri":"/posts/redis/redis%E4%B8%BB%E4%BB%8E%E5%BA%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5/#一致性问题"},{"categories":["redis"],"content":"\r全量复制（主从库第一次同步） ","date":"2021-08-10","objectID":"/posts/redis/redis%E4%B8%BB%E4%BB%8E%E5%BA%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5/:0:0","series":null,"tags":null,"title":"Redis主从库数据同步","uri":"/posts/redis/redis%E4%B8%BB%E4%BB%8E%E5%BA%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5/#全量复制主从库第一次同步"},{"categories":["redis"],"content":"\r同步过程 从库通过replicaof命令，向主库发送psync命令（参数runID为主库id，offset为复制进度） 主库收到psync命令后，返回FULLRESYNC响应命令（参数为主库runID和主库⽬前的复制进度 offset），主从库开始同步 主库执⾏bgsave命令，⽣成RDB⽂件，接着将⽂件发给从库，从库接收文件后清空当前数据库，加载RDB文件 主库同步RDB文件并不会阻塞，会将期间的写请求记录在专⻔的replication buffer 主库完成RDB文件发送后，会把replication buffer中的修改操作发给从库，从库执行这些操作后就完成了数据同步 ","date":"2021-08-10","objectID":"/posts/redis/redis%E4%B8%BB%E4%BB%8E%E5%BA%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5/:1:0","series":null,"tags":null,"title":"Redis主从库数据同步","uri":"/posts/redis/redis%E4%B8%BB%E4%BB%8E%E5%BA%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5/#同步过程"},{"categories":["redis"],"content":"\r主从级联模式（“主-从-从”模式） 多个从库连接到同一主库，会导致主库压力增大（主要是fork子进程会阻塞主线程，传输RDB文件会占用主库网络带宽），此时可以使用主从级联模式 ","date":"2021-08-10","objectID":"/posts/redis/redis%E4%B8%BB%E4%BB%8E%E5%BA%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5/:2:0","series":null,"tags":null,"title":"Redis主从库数据同步","uri":"/posts/redis/redis%E4%B8%BB%E4%BB%8E%E5%BA%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5/#主从级联模式主-从-从模式"},{"categories":["redis"],"content":"\rRDB文件是否落盘 Redis在全量复制时，既支持先生成RDB文件，再把RDB文件传给从库，也支持在主库上直接通过socket把数据传给从库，这称为无盘复制 ","date":"2021-08-10","objectID":"/posts/redis/redis%E4%B8%BB%E4%BB%8E%E5%BA%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5/:3:0","series":null,"tags":null,"title":"Redis主从库数据同步","uri":"/posts/redis/redis%E4%B8%BB%E4%BB%8E%E5%BA%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5/#rdb文件是否落盘"},{"categories":["redis"],"content":"\r基于长连接的命令传播​ 一旦主从库完成全量复制，他们之间会维护一个网络连接，主库通过该连接把接受到的命令操作同步给从库，避免频繁建立连接的开销 ","date":"2021-08-10","objectID":"/posts/redis/redis%E4%B8%BB%E4%BB%8E%E5%BA%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5/:0:0","series":null,"tags":null,"title":"Redis主从库数据同步","uri":"/posts/redis/redis%E4%B8%BB%E4%BB%8E%E5%BA%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5/#基于长连接的命令传播"},{"categories":["redis"],"content":"\r增量复制 ","date":"2021-08-10","objectID":"/posts/redis/redis%E4%B8%BB%E4%BB%8E%E5%BA%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5/:0:0","series":null,"tags":null,"title":"Redis主从库数据同步","uri":"/posts/redis/redis%E4%B8%BB%E4%BB%8E%E5%BA%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5/#增量复制"},{"categories":["redis"],"content":"\r使用场景 网络断连或阻塞会导致主从库无法进行命令传播，而重新进行一次全量复制开销过大，此时主从库使用增量复制同步数据 ","date":"2021-08-10","objectID":"/posts/redis/redis%E4%B8%BB%E4%BB%8E%E5%BA%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5/:1:0","series":null,"tags":null,"title":"Redis主从库数据同步","uri":"/posts/redis/redis%E4%B8%BB%E4%BB%8E%E5%BA%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5/#使用场景"},{"categories":["redis"],"content":"\r同步过程 主从库断开后，主库会把期间收到的写操作命令写入replication buff和repl_backlog_buffer缓冲区中 repl_backlog_buffer是一个环形缓冲区，slave_repl_offset为从库下标，master_repl_offset为主库下标 主从库连接恢复后，从库首先发送psync命令，并把自己当前的slave_repl_offset发给主库 主库把处于master_repl_offset和slave_repl_offset之间的命令操作同步给从库完成同步 ","date":"2021-08-10","objectID":"/posts/redis/redis%E4%B8%BB%E4%BB%8E%E5%BA%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5/:2:0","series":null,"tags":null,"title":"Redis主从库数据同步","uri":"/posts/redis/redis%E4%B8%BB%E4%BB%8E%E5%BA%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5/#同步过程-1"},{"categories":["redis"],"content":"\rrepl_backlog_buffer缓冲区 由于repl_back_log_buffer是一个环形缓冲区，当主从库读取速度差异过大有可能导致从库未读的操作被主库的新写操作覆盖，造成主从库数据不一致 缓冲空间⼤⼩ = 主库写⼊命令速度 * 操作⼤⼩ - 主从库间 ⽹络传输命令速度 * 操作⼤⼩，实际应用中考虑其他突发情况，通常把这个缓冲空间扩大一倍 可以考虑使用切片集群分担单个主库的请求压力 repl_backlog_buffer是所有从库共享的 ","date":"2021-08-10","objectID":"/posts/redis/redis%E4%B8%BB%E4%BB%8E%E5%BA%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5/:3:0","series":null,"tags":null,"title":"Redis主从库数据同步","uri":"/posts/redis/redis%E4%B8%BB%E4%BB%8E%E5%BA%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5/#repl_backlog_buffer缓冲区"},{"categories":["redis"],"content":"\r断线重连后同步选择 由于repl_backlog_buffer存在覆盖的情况，重连后主库会根据从库的slave_repl_offset选择全量复制还是增量复制 ","date":"2021-08-10","objectID":"/posts/redis/redis%E4%B8%BB%E4%BB%8E%E5%BA%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5/:4:0","series":null,"tags":null,"title":"Redis主从库数据同步","uri":"/posts/redis/redis%E4%B8%BB%E4%BB%8E%E5%BA%93%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5/#断线重连后同步选择"},{"categories":["redis"],"content":"\rAOF日志 ","date":"2021-07-26","objectID":"/posts/redis/redis%E6%8C%81%E4%B9%85%E5%8C%96/:0:0","series":null,"tags":null,"title":"Redis持久化","uri":"/posts/redis/redis%E6%8C%81%E4%B9%85%E5%8C%96/#aof日志"},{"categories":["redis"],"content":"\rAOF日志实现 AOF日志记录的是redis收到的操作命令，以文本形式保存，由主线程写回 AOF日志采用先执⾏命令再记⽇志的方式（为避免额外的检查开销，不阻塞当前写操作） 用AOF方法进行故障恢复时，需要逐一把操作日志都执行一遍 ","date":"2021-07-26","objectID":"/posts/redis/redis%E6%8C%81%E4%B9%85%E5%8C%96/:1:0","series":null,"tags":null,"title":"Redis持久化","uri":"/posts/redis/redis%E6%8C%81%E4%B9%85%E5%8C%96/#aof日志实现"},{"categories":["redis"],"content":"\r三种回写策略 Always，同步写回：每次执行写操作后立马同步将日志写回磁盘 Everysec，每秒写回：先把日志写到AOF文件的内存缓冲区，每隔一秒将缓冲区的内容写回磁盘 No，操作系统控制写回：先把日志写到AOF文件的内存缓冲区，有操作系统决定写回时机 ","date":"2021-07-26","objectID":"/posts/redis/redis%E6%8C%81%E4%B9%85%E5%8C%96/:2:0","series":null,"tags":null,"title":"Redis持久化","uri":"/posts/redis/redis%E6%8C%81%E4%B9%85%E5%8C%96/#三种回写策略"},{"categories":["redis"],"content":"\r三种回写策略的优劣 写回方式 写回时机 优点 缺点 Always 同步写回 可靠度高，数据基本不会丢失 每次写操作都要落盘，性能影响较大 Everysec 每秒写回 性能适中 宕机时丢失1s内的数据 No 系统控制写回 性能好 宕机会丢失较多数据 ","date":"2021-07-26","objectID":"/posts/redis/redis%E6%8C%81%E4%B9%85%E5%8C%96/:3:0","series":null,"tags":null,"title":"Redis持久化","uri":"/posts/redis/redis%E6%8C%81%E4%B9%85%E5%8C%96/#三种回写策略的优劣"},{"categories":["redis"],"content":"\rAOF重写机制 主线程fork重写子进程，子进程将新的内容写到临时文件中 重写并不阻塞主线程，主线程对于新的写入操作，一边将他们累积到内存缓冲中，一边追加到现有的AOF文件中（避免重写时宕机引起数据丢失） 子进程完成重写后发送信号给父进程，父进程接收信号后将内存缓存中的所有内容追加到新的AOF文件中（阻塞） 最后redis原子地用新文件替换旧文件，之后的命令将直接追加到新的AOF文件末尾 ","date":"2021-07-26","objectID":"/posts/redis/redis%E6%8C%81%E4%B9%85%E5%8C%96/:4:0","series":null,"tags":null,"title":"Redis持久化","uri":"/posts/redis/redis%E6%8C%81%E4%B9%85%E5%8C%96/#aof重写机制"},{"categories":["redis"],"content":"\r内存快照 ","date":"2021-07-26","objectID":"/posts/redis/redis%E6%8C%81%E4%B9%85%E5%8C%96/:0:0","series":null,"tags":null,"title":"Redis持久化","uri":"/posts/redis/redis%E6%8C%81%E4%B9%85%E5%8C%96/#内存快照"},{"categories":["redis"],"content":"\rRedis提供了两个命令来⽣成RDB⽂件，分别是save和bgsave save：在主线程中执行，会导致阻塞 bgsave：创建子线程专门用于写入RDB文件，避免主线程阻塞，是redis默认配置 ","date":"2021-07-26","objectID":"/posts/redis/redis%E6%8C%81%E4%B9%85%E5%8C%96/:1:0","series":null,"tags":null,"title":"Redis持久化","uri":"/posts/redis/redis%E6%8C%81%E4%B9%85%E5%8C%96/#redis提供了两个命令来成rdb件分别是save和bgsave"},{"categories":["redis"],"content":"\r快照原理（利用copy-on-write原理） 主线程fork⽣成bgsave⼦进程，两者共享同一物理内存，子线程将redis内存中的数据全量记录到磁盘文件中，该快照文件成为RDB文件 主线程对数据进行写操作时，会将该部分内存复制一份副本（写时复制），主线程可直接修改原来的数据而不用担心子进程的数据污染问题 ","date":"2021-07-26","objectID":"/posts/redis/redis%E6%8C%81%E4%B9%85%E5%8C%96/:2:0","series":null,"tags":null,"title":"Redis持久化","uri":"/posts/redis/redis%E6%8C%81%E4%B9%85%E5%8C%96/#快照原理利用copy-on-write原理"},{"categories":["redis"],"content":"\r快照优劣 优：可以快速恢复数据库（避免了AOF需要顺序，逐一执行命令带来的低效性能的问题） 劣：频繁的快照会造成性能的消耗，主要是将全量数据写入磁盘的消耗和主线程进行fork操作的消耗 ","date":"2021-07-26","objectID":"/posts/redis/redis%E6%8C%81%E4%B9%85%E5%8C%96/:3:0","series":null,"tags":null,"title":"Redis持久化","uri":"/posts/redis/redis%E6%8C%81%E4%B9%85%E5%8C%96/#快照优劣"},{"categories":["redis"],"content":"\r总结 数据不能丢失时，内存快照和AOF的混合使⽤是⼀个很好的选择 如果允许分钟级别的数据丢失，可以只使⽤RDB 如果只⽤AOF，优先使⽤everysec的配置选项，因为它在可靠性和性能之间取了⼀个平衡 ","date":"2021-07-26","objectID":"/posts/redis/redis%E6%8C%81%E4%B9%85%E5%8C%96/:0:0","series":null,"tags":null,"title":"Redis持久化","uri":"/posts/redis/redis%E6%8C%81%E4%B9%85%E5%8C%96/#总结"},{"categories":["其他"],"content":"\rCenos8安装json-c库 先把文件从https://github.com/json-c/json-c克隆到本地目录 进入json-c目录分别执行cmake ./，sudo make install（cmake要先下好） 安装完成后查看一下/usr/local/include目录下是否有json-c目录（没有就是安装的时候没给权限导致复制失败） 查看/usr/local/lib目录或者/usr/local/lib64目录下是否有json-c的动态库文件（没有的话应该时安装有问题） 添加动态路的路径：sudo vim /etc/ld.so.conf添加/usr/local/lib或者/usr/local/lib64（根据上一步的目录） 用 sudo ldconfig更新文件/etc/ld.so.conf的修改生效 使用时要包含json.h头文件，编译时需要加上-ljson-c（我看网上的都是-json的，这个应该是要看动态库的名称确定吧） 安装时遇到的问题 没有安装cmake工具导致执行cmake命令时出现问题（cmake的版本也要确定一下） 使用-json-c编译导致编译失败 make install时没给权限导致install失败（安装时弹了一个error，没仔细看出来） 没有添加动态库的加载路径，导致执行文件时会报错找不到动态库文件，用ldd查看一下是libjson-c.so.5文件没找到，加了路径记得要sudo ldconfig更新一下不然还是不生效的 ","date":"2021-01-26","objectID":"/posts/other/centos8%E5%AE%89%E8%A3%85json-c%E5%BA%93/:0:0","series":null,"tags":null,"title":"CentOS8安装json C库","uri":"/posts/other/centos8%E5%AE%89%E8%A3%85json-c%E5%BA%93/#"},{"categories":["其他"],"content":"\rCenos8安装json-c库 先把文件从https://github.com/json-c/json-c克隆到本地目录 进入json-c目录分别执行cmake ./，sudo make install（cmake要先下好） 安装完成后查看一下/usr/local/include目录下是否有json-c目录（没有就是安装的时候没给权限导致复制失败） 查看/usr/local/lib目录或者/usr/local/lib64目录下是否有json-c的动态库文件（没有的话应该时安装有问题） 添加动态路的路径：sudo vim /etc/ld.so.conf添加/usr/local/lib或者/usr/local/lib64（根据上一步的目录） 用 sudo ldconfig更新文件/etc/ld.so.conf的修改生效 使用时要包含json.h头文件，编译时需要加上-ljson-c（我看网上的都是-json的，这个应该是要看动态库的名称确定吧） 安装时遇到的问题 没有安装cmake工具导致执行cmake命令时出现问题（cmake的版本也要确定一下） 使用-json-c编译导致编译失败 make install时没给权限导致install失败（安装时弹了一个error，没仔细看出来） 没有添加动态库的加载路径，导致执行文件时会报错找不到动态库文件，用ldd查看一下是libjson-c.so.5文件没找到，加了路径记得要sudo ldconfig更新一下不然还是不生效的 ","date":"2021-01-26","objectID":"/posts/other/centos8%E5%AE%89%E8%A3%85json-c%E5%BA%93/:0:0","series":null,"tags":null,"title":"CentOS8安装json C库","uri":"/posts/other/centos8%E5%AE%89%E8%A3%85json-c%E5%BA%93/#cenos8安装json-c库"},{"categories":["其他"],"content":" 命令 含义/用法 start 开始调试,停在第一行代码处 (gdb) start l(list) 查看源代码 (gdb) l \u003cnumber/function\u003e b(breakpoint) 设置断点 (gdb) b \u003c行号/函数名\u003eb … if i == 3：条件断点 info info breakpoints：显示所有断点 info stack：查看堆栈信息 info args：查看当前参数值 info frame：输出栈帧的使用情况 info locals：查看局部变量 s(step) 执行一行源程序代码，如果此行代码中有函数调用，则进入该函数 (gdb) s n(next) 执行一行源程序代码，此行代码中的函数调用也一并执行 (gdb) n r(run) 运行被调试的程序 (gdb) r c(continue) 继续执行被调试程序，直至下一个断点或程序结束 (gdb) c finish 函数结束 p(print) 显示指定变量的值 (gdb) p \u003c变量名\u003e set args name=v 可指定运行时参数。(gdb) set args 10 20 show args 查看运行时参数 q(quit) 退出GDB调试环境 (gdb) q bt(backtrack) 查看函数堆栈 (gdb) bt f(frame) 切换当前栈 (gdb) f \u003c栈序号\u003e u(until) 结束当前循环 (gdb) until set variable 给变量赋值 jump 在源代码的另一段开始运行 delete 删除断点 (gdb) delete \u003c断点号\u003e watch 变量变化时暂停运行 awatch 变量被访问、改变是暂停执行 rwatch 变量被访问时暂停执行 disp(display) 跟踪查看某个变量,每次停下来都显示它的值 k(kill) 终止正在调试的程序 ","date":"2021-01-15","objectID":"/posts/other/gdb%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/:0:0","series":null,"tags":null,"title":"GDB基本操作","uri":"/posts/other/gdb%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/#"},{"categories":["其他"],"content":"\r一、阻塞IO（blocking I/O） io两个阶段： 等待数据准备 将数据从内核拷贝到进程中 调用recvfrom后调用进程/线程阻塞直到内核把数据准备好并把数据拷贝到用户内存后，内核返回结果，调用进程/线程的阻塞状态解除，继续运行（阻塞io） 实际上，除非特别指定，几乎所有的IO接口 ( 包括socket接口 ) 都是阻塞型的 ","date":"2020-09-03","objectID":"/posts/other/io%E6%A8%A1%E5%9E%8B/:0:0","series":null,"tags":null,"title":"Io模型","uri":"/posts/other/io%E6%A8%A1%E5%9E%8B/#一阻塞ioblocking-io"},{"categories":["其他"],"content":"\r二、非阻塞IO（noblocking I/O） 调用recvfrom后直接返回结果，若内核数据并没有准备好则返回error。 在非阻塞式IO中，用户进程其实是需要不断的主动询问kernel数据准备好了没有（这种方法会导致cpu占用率大幅上升） 与阻塞io的主要差异是调用接口后立即返回调用结果而不是阻塞调用方直到内核返回结果。 Linux下，可以通过设置socket使其变为non-blocking ","date":"2020-09-03","objectID":"/posts/other/io%E6%A8%A1%E5%9E%8B/:0:0","series":null,"tags":null,"title":"Io模型","uri":"/posts/other/io%E6%A8%A1%E5%9E%8B/#二非阻塞ionoblocking-io"},{"categories":["其他"],"content":"\r三、多路复用IO（IO multiplexing） select/epoll的好处就在于单个process就可以同时处理多个网络连接的IO。它的基本原理就是select/epoll会不断的轮询所负责的所有socket，当某个socket有数据到达了，就通知用户进程 当用户进程调用了select，process会阻塞在select上而不是单个io操作上，而同时，kernel会“监视”所有select负责的socket，当任何一个socket的数据准备好了，select就会返回。这个时候用户进程再调用read操作，将数据从kernel拷贝到用户进程。 ","date":"2020-09-03","objectID":"/posts/other/io%E6%A8%A1%E5%9E%8B/:0:0","series":null,"tags":null,"title":"Io模型","uri":"/posts/other/io%E6%A8%A1%E5%9E%8B/#三多路复用ioio-multiplexing"},{"categories":["其他"],"content":"\r四、异步IO（Asynchronous I/O） 用户进程发起read操作之后，立刻就可以开始去做其它的事。而另一方面，从kernel的角度，当它受到一个asynchronous read之后，首先它会立刻返回，所以不会对用户进程产生任何block。然后，kernel会等待数据准备完成，然后将数据拷贝到用户内存，当这一切都完成之后，kernel会给用户进程发送一个signal，告诉它read操作完成了。 真正的异步io（全过程并没有被阻塞，其他四种可能阻塞在数据准备阶段或者数据复制阶段）。 ","date":"2020-09-03","objectID":"/posts/other/io%E6%A8%A1%E5%9E%8B/:0:0","series":null,"tags":null,"title":"Io模型","uri":"/posts/other/io%E6%A8%A1%E5%9E%8B/#四异步ioasynchronous-io"},{"categories":["其他"],"content":"\r信号驱动IO（signal driven I/O， SIGIO） 首先我们允许套接口进行信号驱动I/O,并安装一个信号处理函数，进程继续运行并不阻塞。当数据准备好时，进程会收到一个SIGIO信号，可以在信号处理函数中调用I/O操作函数处理数据。当数据报准备好读取时，内核就为该进程产生一个SIGIO信号。我们随后既可以在信号处理函数中调用recvfrom读取数据报，并通知主循环数据已准备好待处理，也可以立即通知主循环，让它来读取数据报。无论如何处理SIGIO信号，这种模型的优势在于等待数据报到达(第一阶段)期间，进程可以继续执行，不被阻塞。免去了select的阻塞与轮询，当有活跃套接字时，由注册的handler处理。 阻塞在数据复制阶段。 ","date":"2020-09-03","objectID":"/posts/other/io%E6%A8%A1%E5%9E%8B/:0:0","series":null,"tags":null,"title":"Io模型","uri":"/posts/other/io%E6%A8%A1%E5%9E%8B/#信号驱动iosignal-driven-io-sigio"},{"categories":["其他"],"content":"\r管道(内核管理的一个缓冲区) 匿名管道：通常用于父子进程的通信(通过fork复制父进程的信息，关闭父进程的读（或写）通道和子进程的写（或读）通道从而形成单向通信的通道) 有名管道：通常用于无亲缘关系进程间的通信 四种读写情况 读端不读（fd[0]未关闭），写端一直写 ：直到管道写满数据后，再次write会导致阻塞，直到管道有空位置才写去数据并返回 写端不写（fd[1]未关闭），但是读端一直读（写端不关闭）：管道为空时阻塞读端，直到管道有数据时才读数据并返回（是否阻塞取决于写端计数器是否大于0….） 读端一直读，且fd[0]保持打开，而写端写了一部分数据不写了，并且关闭fd[1]：读取管道所有数据后返回0 读端读了一部分数据，不读了且关闭fd[0]，写端一直在写且f[1]还保持打开状态：一旦读端关闭而进程write时会接受信号SIGPIPE，通常导致进程异常终止 局限性 只支持单向数据流 缓冲区有限，管道只存在于主存中 所传送的是无格式字节流 效率低下（写数据需要读端读取后才返回） 需要注意同步问题？(自带有同步机制) 有名管道：长期存于系统中，使用不当容易出错 ","date":"2020-07-03","objectID":"/posts/other/%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E8%AE%AF/:0:0","series":null,"tags":null,"title":"进程间通讯","uri":"/posts/other/%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E8%AE%AF/#管道内核管理的一个缓冲区"},{"categories":["其他"],"content":"\r消息队列 实际上就是信息的链表，存放在内核中并由消息队列标识符标识。消息队列克服了信号传递信息少、管道只能承载无格式字节流以及缓冲区大小受限等缺点。 信息的传递经过四次信息复制：写端-\u003e内核 内核-\u003e读端(消息队列中信息的复制需要额外消耗CPU的时间．不适宜于信息量大或操作频繁的场合) 消息队列克服了信号传递信息少,管道只能承载无格式字节流以及缓冲区大小受限等特点 可以实现任意进程间的通信，并通过系统调用函数来实现消息发送和接收之间的同步，无需考虑同步问题，方便 ","date":"2020-07-03","objectID":"/posts/other/%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E8%AE%AF/:0:0","series":null,"tags":null,"title":"进程间通讯","uri":"/posts/other/%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E8%AE%AF/#消息队列"},{"categories":["其他"],"content":"\r共享内存 内存共享机制：将进程的虚拟地址空间映射到相同的物理内存中 该做法会引起进程的内存竞争问题 共享内存是最快的IPC(进程间通信)方式 ","date":"2020-07-03","objectID":"/posts/other/%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E8%AE%AF/:0:0","series":null,"tags":null,"title":"进程间通讯","uri":"/posts/other/%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E8%AE%AF/#共享内存"},{"categories":["其他"],"content":"\r信号量 用于实现进程间的互斥同步问题 ","date":"2020-07-03","objectID":"/posts/other/%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E8%AE%AF/:0:0","series":null,"tags":null,"title":"进程间通讯","uri":"/posts/other/%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E8%AE%AF/#信号量"},{"categories":["其他"],"content":"\rsocket 不同主机间进程通信的主要方法 ","date":"2020-07-03","objectID":"/posts/other/%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E8%AE%AF/:0:0","series":null,"tags":null,"title":"进程间通讯","uri":"/posts/other/%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E8%AE%AF/#socket"}]